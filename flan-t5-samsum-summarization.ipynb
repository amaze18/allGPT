{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune FLAN-T5 for chat & dialogue summarization\n",
    "\n",
    "In this blog, you will learn how to fine-tune [google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl) for chat & dialogue summarization using Hugging Face Transformers. If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \n",
    "\n",
    "In this example we will use the [samsum](https://huggingface.co/datasets/samsum) dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup Development Environment](#1-setup-development-environment)\n",
    "2. [Load and prepare samsum dataset](#2-load-and-prepare-samsum-dataset)\n",
    "3. [Fine-tune and evaluate FLAN-T5](#3-fine-tune-and-evaluate-flan-t5)\n",
    "4. [Run Inference and summarize ChatGPT dialogues](#4-run-inference-and-summarize-chatgpt-dialogues)\n",
    "\n",
    "Before we can start, make sure you have a [Hugging Face Account](https://huggingface.co/join) to save artifacts and experiments. \n",
    "\n",
    "## Quick intro: FLAN-T5, just a better T5\n",
    "\n",
    "FLAN-T5 released with the [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) paper is an enhanced version of T5 that has been finetuned in a mixture of tasks. The paper explores instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. The paper discovers that overall instruction finetuning is a general method for improving the performance and usability of pretrained language models. \n",
    "\n",
    "![flan-t5](../assets/flan-t5.png)\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2210.11416\n",
    "* Official repo: https://github.com/google-research/t5x\n",
    "\n",
    "--- \n",
    "\n",
    "Now we know what FLAN-T5 is, let's get started. 🚀\n",
    "\n",
    "_Note: This tutorial was created and run on a g4dn.xlarge AWS EC2 Instance including a NVIDIA T4._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "#! pip install -U ipykernel\n",
    "#! pip install transformers datasets  torch accelerate langchain sentence_transformers chromadb runhouse tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install git-fls for pushing model and logs to the hugging face hub\n",
    "#!sudo apt-get install git-lfs --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account, you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "import click\n",
    "from typing import List\n",
    "from langchain.document_loaders import TextLoader, PDFMinerLoader, CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.docstore.document import Document\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom document loaders\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if 'text/html content not found in email' in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
    "\n",
    "        return doc\n",
    "\n",
    "\n",
    "# Map file extensions to document loaders and their arguments\n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    # \".docx\": (Docx2txtLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".eml\": (MyElmLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "\n",
    "def load_single_document(file_path: str) -> Document:\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()[0]\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
    "\n",
    "\n",
    "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n",
    "        )\n",
    "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
    "            for i, doc in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
    "                results.append(doc)\n",
    "                pbar.update()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare samsum dataset\n",
    "\n",
    "we will use the [samsum](https://huggingface.co/datasets/samsum) dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['source_documents/bhagwan.txt', 'source_documents/anupam.txt']\n",
      "['source_documents/Free Open Source Alternative to ChatGPT — GPT4All by Wei-Meng Lee May, 2023 Level Up Coding.html', 'source_documents/Automated Planning Tool makes work order allocation more efficient - Amazon Science.html', 'source_documents/Anupam Purwar - NSF PG Scholarship Programme.html', 'source_documents/Anupam Purwar - IEEE Xplore Author Profile.html', 'source_documents/Anupam Purwar - Amazon Science.html', 'source_documents/_Anupam Purwar_ - _Google Scholar_.html']\n",
      "6\n",
      "['source_documents/arimax-model-for-forecasting-maintenance-work-amfm-a-multi-stage-seasonal-arimax-model-for-work-order-time-series-forecasting.pdf', 'source_documents/hom.pdf', 'source_documents/automated-planning-tool-apt-a-mised-interger-non-linear-programming-problem-solver-for-workorder-scheduling.pdf']\n",
      "3\n",
      "[Document(page_content='Bhagwan Chowdhry is a Professor of Finance at the Indian School of Business and Research Professor at UCLA Anderson where he has held an appointment since 1988. He is the Executive Director, Digital Identity Research Initiative (DIRI), and Faculty Director, I-Venture @ ISB. \\n\\nProfessor Chowdhry has also taught at the University of Chicago, University of Illinois at Chicago, and the Hong Kong University of Science and Technology. He received his Ph.D. from the University of Chicago Booth School of Business . He has an M.B.A. in Finance from the University of Iowa and a B.Tech. in Mechanical Engineering from Indian Institute of Technology, Kanpur.\\n\\nHis research interests, on which he has published several papers in finance and economics journals, are in International Finance, Corporate Finance, Impact Investing, and FinTech. He has also organized and taught Executive Education programs in Los Angeles, Singapore, Hong Kong, Mumbai, and Hyderabad. Professor Chowdhry is an Advisor to several FinTech start-ups. \\n\\nProfessor Chowdhry has proposed Financial Access at Birth (FAB) initiative in which every child born in the world is given an initial deposit of $100 in an online bank account to guarantee that everyone in the world will have access to financial services in a few decades. \\n', metadata={'source': 'source_documents/bhagwan.txt'}), Document(page_content='Anupam Purwar is author with 150+ citations | i10 index=5 | h index 7. Wide industrial experience of aerospace, defence, materials, biomedical, project and operations management. At ISB, I am investigating interactions between operational risks and financial risks to drive the valuation of technology backed firms.\\n\\n\\nAnupam is currently working as a Research Scientist with Amazon developing tech products for Amazon’s global network. He specializes in solving problems related to Natural Language Processing and Optimization. In his previous role at Amazon, he was credited with developing Railways as the third mode of transport which fetched him two awards from Global VP. Prior to this, Anupam worked as a Research Scientist at Indian Institute of Science (IISc). At IISc, he was part of multi-institutional effort which included IITs and ISRO to develop ultra-high-temperature materials and novel structures, which resulted in 20+ peer reviewed publications with Anupam as the lead author. He graduated among top 10% of the class from Indian School of Business and BITS Pilani. He is passionate about contributing towards education of kids from underprivileged backgrounds. He is an environment conservationist and takes pride in cycling every day for work as well as personal commute.\\n\\n', metadata={'source': 'source_documents/anupam.txt'}), Document(page_content='Member-only story\\n\\nFree Open Source Alternative to ChatGPT — GPT4All\\n\\nBuild a conversational chatbot using the free open source GPT4All\\n\\nWei-Meng Lee·Follow\\n\\nPublished inLevel Up Coding·9 min read·May 18\\n\\nListenShare\\n\\n2\\n\\n-\\n\\nListen\\n\\nShare\\n\\nJoshua Hoehne on\\n\\nUnsplash\\n\\nBy now you should already been very familiar with ChatGPT (or at least have heard of its prowess). While ChatGPT is very powerful and useful, it has several drawbacks that may prevent some people from using it:\\n\\nIt cannot be run offline, which also raises the next concern.\\n\\nIt poses privacy risks as the data you supplied will be sent to OpenAI. This is a big no-no for enterprises.\\n\\nIntegrating ChatGPT into your applications requires an API key from OpenAI, which adopts a pay-per-use model.\\n\\nFortunately, the wait is over for a solution that addresses the above concerns — GPT4All.\\n\\nGPT4All (https://gpt4all.io/index.html) is an open-source project containing a number of pre-trained Large Language Models (LLMs) that you can use to run locally using consumer grade CPUs. GPT4All contains a number of models that ranges from 3GB to 8GB. What’s more exciting? It is free!\\n\\nWhile the performance of GPT4All may not be on par with the current ChatGPT, with contributions from the open source community it has significant potentials for further development and enhancements. It may eventually be able to compete on the same level with commercial models like ChatGPT from OpenAI.\\n\\nIn this article, I will take you on a whirlwind tour of how to use GPT4All using Python, and how you can use it to build a chatbot.\\n\\nLet’s go!\\n\\nInstalling GPT4All\\n\\nTo use GPT4All programmatically in Python, you need to install it using the pip command:\\n\\nFor this article I will be using Jupyter Notebook.\\n\\n!pip install gpt4all\\n\\nListing all supported Models\\n\\nGPT4All supports a number of pre-trained models. To list all the models available, use the list_models() function:\\n\\nfrom gpt4all import GPT4AllGPT4All.list_models()\\n\\nYou will get the following result (I have highlighted the model names in bold):\\n\\nLoading a Specific Model\\n\\nBased on the models that you have seen listed in the previous section, you can now go ahead and load the model that you want. For this example, I will use the ggml-gpt4all-j-v1.3-groovy model:\\n\\ngpt = GPT4All(\"ggml-gpt4all-l13b-snoozy.bin\")\\n\\nPersonally I have tried two models — ggml-gpt4all-j-v1.3-groovy.bin and ggml-gpt4all-l13b-snoozy.bin. Based on some of the testing, I find that the ggml-gpt4all-l13b-snoozy.bin is much more accurate. It is a 8.14GB model.\\n\\nThe first time you run this, it will download the model and store it locally on your computer in the following directory: ~/.cache/gpt4all/.\\n\\nllama.cpp: loading model from /Users/weimenglee/.cache/gpt4all/ggml-gpt4all-l13b-snoozy.binllama_model_load_internal: format     = ggjt v1 (latest)llama_model_load_internal: n_vocab    = 32000llama_model_load_internal: n_ctx      = 2048llama_model_load_internal: n_embd     = 5120llama_model_load_internal: n_mult     = 256llama_model_load_internal: n_head     = 40llama_model_load_internal: n_layer    = 40llama_model_load_internal: n_rot      = 128llama_model_load_internal: ftype      = 2 (mostly Q4_0)llama_model_load_internal: n_ff       = 13824llama_model_load_internal: n_parts    = 1llama_model_load_internal: model size = 13Bllama_model_load_internal: ggml ctx size =  73.73 KBllama_model_load_internal: mem required  = 9807.47 MB (+ 1608.00 MB per state)llama_init_from_file: kv self size  = 1600.00 MB\\n\\nAsking a Question\\n\\nTo ask a question, you call the chat_completion() function by passing it a list of dictionaries, like this:\\n\\n[  {    \"role\": \"user\",     \"content\": \"What is the national flower of Canada\"  }]\\n\\nThe role key indicates who is asking the question (user) or responding (assistant) and the content key contains the question. To hold a conversation with GPT4All, you need to pass it the question as well as its previous responses (you will see how this works shortly). This is similar to how ChatGPT works:\\n\\nInterfacing ChatGPT with PythonBuild conversational chatbots using the OpenAI APIlevelup.gitconnected.com\\n\\nThe following code snippets sends the question to GPT4All:\\n\\nmessages = [{\"role\": \"user\", \"content\": \"What is the national flower of Canada\"}]gpt.chat_completion(messages)\\n\\nYou will see the following output in Jupyter Notebook (I have bolded the response):\\n\\n### Instruction:             The prompt below is a question to answer, a task to complete, or a conversation             to respond to; decide which and write an appropriate response.            ### Prompt: What is the national flower of Canada?### Response:The national flower of Canada is the Canadian Maple Leaf.\\n\\nAt the same time, the response returned by GTP4All will also be printed out (formatted for clarity):\\n\\n{  \\'model\\': \\'ggml-gpt4all-l13b-snoozy\\',  \\'usage\\': {    \\'prompt_tokens\\': 264,     \\'completion_tokens\\': 57,     \\'total_tokens\\': 321  },  \\'choices\\': [    {      \\'message\\': {        \\'role\\': \\'assistant\\',        \\'content\\': \\'The national flower of Canada is the Canadian Maple Leaf.\\'      }    }  ]}\\n\\nAs you can see from the output above, the response you want is found in the keys highlighted in bold.\\n\\nIntegrating with Gradio\\n\\nTo make it easier for users to interact with the GTP4All, you should ideally bind it to a web-based UI. For this I am going to use Gradio, a Python package that binds your Python function to a web-based UI.\\n\\nSharing your Machine Learning or Deep Learning Projects with Users with GradioCreate a user-friendly web-based UI to let users interact with your trained modelslevelup.gitconnected.com\\n\\nTo do that, I first define a function named chat(), which will call GTP4All with the question asked by the user. I then bind this function to Gradio:\\n\\nimport gradio as gr# for storing the prompts and responsesmessages = []def chat(message):    global messages    # append the prompt    messages.append({\"role\": \"user\", \"content\": message})        # send to GPT4All    response = gpt.chat_completion(messages)        # extract the content of the response    content = response[\\'choices\\'][0][\\'message\\'][\\'content\\']        # append the response by GPT4All    messages.append({\"role\":\"assistant\", \"content\": content})        # return the content to the user    return content# bind it to gradiogr.Interface(fn = chat,              inputs = \"text\",              outputs = \"text\").launch()\\n\\nObserve that in the code above, each response by the GPT4All is appended to the messages list. This is to ensure that GPT4All is able to hold a conversation with the user.\\n\\nWhen you run the above code snippet, you will see the following UI:\\n\\nType a question on the left text box and click Submit. The question will be passed to GPT4All and after a while, you will see the response:\\n\\nTo ask a follow-up question, click Clear and ask the next question:\\n\\nRealize that GPT4All is aware of the context of the question and can follow-up with the conversation. This is because you have appended the previous responses from GPT4All in the follow-up call.\\n\\nCreating a Chatbot using Gradio\\n\\nIf you are reading up until this point, you would have realized that having to clear the message every time you want to ask a follow-up question is troublesome. A much more intuitive UI would be to make it behave more like a chat application. In fact, you can build a chatbot-like UI using Gradio.\\n\\nThe following code snippet shows how to wrap the GTP4All call with a chatbot-like UI using Gradio:\\n\\nimport gradio as grimport randomimport timemessages = []with gr.Blocks() as mychatbot:  # Blocks is a low-level API that allows                                 # you to create custom web applications    chatbot = gr.Chatbot()      # displays a chatbot    question = gr.Textbox()     # for user to ask a question    clear = gr.Button(\"Clear Conversation\")  # Clear button        # function to clear the conversation    def clear_messages():        global messages        messages = []    # reset the messages list            def chat(message, chat_history):        global messages        messages.append({\"role\": \"user\", \"content\": message})        response = gpt.chat_completion(messages)        content = response[\\'choices\\'][0][\\'message\\'][\\'content\\']        messages.append({\"role\":\"assistant\", \"content\": content})                chat_history.append((message, content))        return \"\", chat_history    # wire up the event handler for Submit button (when user press Enter)    question.submit(fn = chat,                     inputs = [question, chatbot],                     outputs = [question, chatbot])    # wire up the event handler for the Clear Conversation button    clear.click(fn = clear_messages,                 inputs = None,                 outputs = chatbot,                 queue = False)mychatbot.launch()\\n\\nNote that this time round the chat() function accepts two arguments — the question to ask, and an instance of ChatBot.\\n\\nWhen you run the above code snippet, you will see the following:\\n\\nType a question into the textbox and press Enter. The response will be appended to the UI:\\n\\nYou can follow-up with another question:\\n\\nIf you like reading my articles and that it helped your career/study, please consider signing up as a Medium member. It is $5 a month, and it gives you unlimited access to all the articles (including mine) on Medium. If you sign up using the following link, I will earn a small commission (at no additional cost to you). Your support means that I will be able to devote more time on writing articles like this.\\n\\nJoin Medium with my referral link - Wei-Meng LeeRead every story from Wei-Meng Lee (and thousands of other writers on Medium). Your membership fee directly supports…weimenglee.medium.com\\n\\nSummary\\n\\nIn this article, I have discussed how you can use GPT4All, the free open-source project that you can run locally using some of the pre-trained LLMs. The nice feature about GTP4All is that it does not need Internet to work, and that it runs locally, thereby allaying fears of your data being sent to the cloud. In future articles, I will talk more about GPT4All. Stay tuned!\\n\\nLevel Up Coding\\n\\nThanks for being a part of our community! Before you go:\\n\\n👏 Clap for the story and follow the author 👉\\n\\n📰 View more content in the Level Up Coding publication\\n\\n💰 Free coding interview course ⇒ View Course\\n\\n🔔 Follow us: Twitter | LinkedIn | Newsletter\\n\\n🚀👉 Join the Level Up talent collective and find an amazing job\\n\\nWei-Meng Lee\\n\\nin\\n\\nLevel Up Coding\\n\\nTraining Your Own LLM using privateGPTLearn how to train your own language model without exposing your private data to the provider\\n\\n8 min read·May 19\\n\\n6\\n\\n-\\n\\nNitin Sharma\\n\\nin\\n\\nLevel Up Coding\\n\\nAutoGPT is Taking Over the Internet: Here Are the Incredible Use Cases That Will Blow Your MindFrom acting as an intern to automating your work on Discord.\\n\\n6 min read·Apr 25\\n\\n31\\n\\n-\\n\\n31\\n\\nArslan Ahmad\\n\\nin\\n\\nLevel Up Coding\\n\\n12 Microservices Patterns I Wish I Knew Before the System Design InterviewMastering the Art of Scalable and Resilient Systems with Essential Microservices Design Patterns\\n\\n13 min read·May 16\\n\\n9\\n\\n-\\n\\nWei-Meng Lee\\n\\nin\\n\\nLevel Up Coding\\n\\nConnecting ChatGPT with Your Own Data using LlamaIndexLearn how to create your own chatbot for your business\\n\\n6 min read·3 days ago\\n\\n-\\n\\nWei-Meng Lee\\n\\nin\\n\\nLevel Up Coding\\n\\nTraining Your Own LLM using privateGPTLearn how to train your own language model without exposing your private data to the provider\\n\\n8 min read·May 19\\n\\n6\\n\\n-\\n\\nGabe Araujo, M.Sc.\\n\\nin\\n\\nLevel Up Coding\\n\\n🐼Introducing PandasAI: The Generative AI Python Library 🐼Pandas AI is an additional Python library that enhances Pandas, the widely-used data analysis and manipulation tool, by incorporating…\\n\\n9 min read·May 16\\n\\n4\\n\\n-\\n\\nThomas A Dorfer\\n\\nin\\n\\nTowards Data Science\\n\\nTen Years of AI in ReviewFrom image classification to chatbot therapy\\n\\n15 min read·4 days ago\\n\\n9\\n\\n-\\n\\nLeonie Monigatti\\n\\nin\\n\\nTowards Data Science\\n\\nGetting Started with LangChain: A Beginner’s Guide to Building LLM-Powered ApplicationsA LangChain tutorial to build anything with large language models in Python\\n\\n12 min read·Apr 25\\n\\n16\\n\\n-\\n\\n16\\n\\nGuodong (Troy) Zhao\\n\\nin\\n\\nBootcamp\\n\\nA comprehensive and hands-on guide to autonomous agents with GPTUnderstand autonomous agents with LLM, from conceptual guide to hands-on tutorial\\n\\n15 min read·May 2\\n\\n4\\n\\n-\\n\\nClayton Moulynox\\n\\nin\\n\\nThe Generator\\n\\nI Made an AI News Editor-in-Chief Without Knowing How to CodeNow I get fresh story angles on current topics and headlines\\n\\n15 min read·May 17\\n\\n8\\n\\n-', metadata={'source': 'source_documents/Free Open Source Alternative to ChatGPT — GPT4All by Wei-Meng Lee May, 2023 Level Up Coding.html'}), Document(page_content=\"Operations research and optimization\\n\\nAutomated Planning Tool makes work order allocation more efficient\\n\\n“Branch-and-bound” method rules out nonoptimal solutions to mixed-integer nonlinear-programming problems.\\n\\nBy\\n\\nAnupam Purwar\\n\\nMarch 28, 2023\\n\\nShare\\n\\nShare\\n\\nCopy link\\n\\nEmail\\n\\nTwitter\\n\\nLinkedIn\\n\\nFacebook\\n\\nLine\\n\\nReddit\\n\\nQZone\\n\\nSina Weibo\\n\\nWeChat\\n\\nWhatsApp\\n\\n分享到微信\\n\\nConference\\n\\nEURO 2022\\n\\nRelated publications\\n\\nAutomated planning tool (APT): A mixed integer non linear programming problem solver for workorder scheduling\\n\\nAutomated \\nplanning tools are programs that combine artificial intelligence and \\ndesigned algorithms to plan and schedule tasks, resources, and \\nactivities. They are used in a variety of industries, including \\nmanufacturing, supply chain management, healthcare, and transportation, \\nwhere they can help improve efficiency and reduce costs by automating \\nrepetitive and time-consuming planning tasks.\\n\\nWork order \\nallocation is a critical task in many of these industries; it is a type \\nof resource allocation problem, where the goal is to schedule a set of \\nwork orders on a set of resources (machines, personnel, etc.) in an \\nefficient and effective manner. It involves allocating resources, such \\nas labor, equipment, and materials, to specific tasks in a way that \\nmeets various constraints and objectives.\\n\\nAt the 2022 meeting of the Association of European Operational Research Societies (EURO), I presented a paper titled “Automated Planning Tool (APT): A mixed integer non-linear programming problem solver for workorder scheduling”, which describes a new approach to solving work-order-allocation problems.\\n\\nMy\\n method, called the Automated Planning Tool (APT), uses a mixed-integer \\nnonlinear-programming (MINLP) solver to handle the complex and nonlinear\\n nature of work-order-allocation problems. MINLP is an optimization \\ntechnique that combines variables constrained to integer values with \\ncontinuous nonlinear functions.\\n\\nThe APT algorithm is based on the branch-and-bound \\nmethod, which divides an optimization problem into smaller subproblems \\nand then uses a bounding function to estimate the solution for each \\nsubproblem. The bounding function provides an upper or lower bound on \\nthe optimal solution, which can be used to prune the search space by \\neliminating subproblems that are known not to contain the optimal \\nsolution.\\n\\nRelated content\\n\\nJonathan Toner’s hunt for hard questions took him from Antarctica to Amazon\\n\\nHow the former astrobiology professor is charting new territory as a scientist for Amazon Flex.\\n\\nBecause it involves a large number of constraints (such\\n as task dependencies, deadlines, and resource constraints), work order \\nallocation is an NP-complete problem, meaning that for even a reasonable\\n numbers of variables, it can become impractically time consuming. It \\nbecomes even more complex and challenging when it involves nonlinear \\nconstraints, such as resource utilization and maintenance costs.\\n\\nAutomated\\n planning tools must thus find approximate solutions to optimization \\nproblems or exploit problem structure in order to reduce the \\ncomputational complexity. Some techniques used in automated planning \\ntools include\\n\\nPERT (program evaluation and review technique)\\n\\ncritical-path method (CPM) for Gantt charts\\n\\nlinear programming (LP)\\n\\ninteger programming (IP)\\n\\nconstraint programming (CP)\\n\\nartificial-intelligence planning (AIP).\\n\\nHowever,\\n traditional optimization techniques, such as linear programming, are \\nnot very well suited to solving these types of problems.\\n\\nComplexity\\n\\nAPT’s\\n complexity is closely tied to the constraints that must be taken into \\naccount when creating a schedule. These might include the time window \\nwithin which a task must be completed, the availability and \\nqualifications of technicians, or the physical layout of a facility.\\n\\nRelated content\\n\\nSolving some of the largest, most complex operations problems\\n\\nHow Amazon’s Supply Chain Optimization Technologies team has evolved over time to meet a challenge of staggering complexity.\\n\\nSuch constraints can make the problem more difficult to\\n solve, as they may limit the number of possible solutions or make it \\nharder to find an optimal schedule. The complexity of the problem also \\ndepends on the number of work orders, the number of resources available,\\n and the need to satisfy the integrality constraint for binary decision \\nvariables. It's important to consider these constraints both while \\nformulating the problem as an MINLP problem and while designing the \\noptimization algorithm to solve the problem.\\n\\nAPT\\n\\nIn\\n the case of APT, the algorithm is implemented as a tree search, where \\nthe root node represents the original problem, and the children of each \\nnode represent the subproblems generated by branching. The search \\ncontinues until all nodes have been explored or a feasible solution has \\nbeen found.\\n\\nWe have used the Xpress solver to solve this problem \\nwith multiple constraints and a mix of integer/continuous decision \\nvariables. Our studies show that APT is able to find optimal solutions \\nfor work-order-allocation problems with nonlinear objective functions in\\n a relatively short amount of time.\\n\\nAWS-powered implementation\\n\\nAPT\\n is written using Python and uses Amazon Web Services (AWS) \\ninfrastructure for data storage as well as computation. AWS’s ECS \\nFargate allowed us to scale APT in an affordable way with no upper cap \\non the number of users. ECS Fargate is a technology provided by AWS for \\nrunning Docker containers on the AWS Elastic Container Service (ECS) \\nwithout the need for managing the underlying infrastructure. Fargate \\neliminates the need for users to provision and manage the EC2 instances \\nthat their containers run on, allowing them to focus on developing and \\ndeploying their applications. It also enables automatic scaling of \\ncontainerized applications and integrates with other AWS services, such \\nas load balancers and security groups.\\n\\nThe Automated Planning Tool\\n (APT) is a powerful and efficient method for solving \\nwork-order-allocation problems. Its ability to handle nonlinear \\nobjectives and its use of a branch-and-bound method make it a valuable \\ntool for scheduling and resource allocation.\\n\\nResearch areas\\n\\nOperations research and optimization\\n\\nTags\\n\\nMultivariate optimization\\n\\nConference\\n\\nEURO 2022\\n\\nRelated publications\\n\\nAutomated planning tool (APT): A mixed integer non linear programming problem solver for workorder scheduling\\n\\nAbout the Author\\n\\nAnupam Purwar\\n\\nAnupam Purwar is a research scientist in Amazon's \\nGlobal Reliability and Maintenance Engineering organization.\", metadata={'source': 'source_documents/Automated Planning Tool makes work order allocation more efficient - Amazon Science.html'}), Document(page_content='Anupam Purwar\\n\\nadmin\\n\\nAnupam is currently working as a Research Scientist with Amazon \\ndeveloping tech products for Amazon’s global network. He specializes in \\nsolving problems related to Natural Language Processing and \\nOptimization. In his previous role at Amazon, he was credited with \\ndeveloping Railways as the third mode of transport which fetched him two\\n awards from Global VP. Prior to this, Anupam worked as a Research \\nScientist at Indian Institute of Science (IISc). At IISc, he was part of\\n multi-institutional effort which included IITs and ISRO to develop \\nultra-high-temperature materials and novel structures, which resulted in\\n 20+ peer reviewed publications with Anupam as the lead author. He \\ngraduated among top 10% of the class from Indian School of Business and \\nBITS Pilani. He is passionate about contributing towards education of \\nkids from underprivileged backgrounds. He is an environment \\nconservationist and takes pride in cycling every day for work as well as\\n personal commute.\\n\\nCategories:', metadata={'source': 'source_documents/Anupam Purwar - NSF PG Scholarship Programme.html'}), Document(page_content=\"Author details\\n\\n< Back\\n\\nAnupam Purwar\\n\\nPublications\\n\\n Citations \\n\\n19\\n\\nPublications by Year\\n\\n2014\\n\\n2017\\n\\nCo-Authors:\\n\\nVinod Kumar ChaubeyMani Sankar DasguptaSneh DeepDivya Joshi Show All Co-Authors (4)\\n\\nAnupam Purwar\\n\\nAffiliation\\n\\nNational Laboratory of Radar Signal Processing\\n\\nXidian University\\n\\nXi’an, China\\n\\nPublication Topics\\n\\nceramics,\\n\\nfinite element analysis,\\n\\ngas turbines,\\n\\nspace vehicles,\\n\\ntemperature measurement,\\n\\ntemperature sensors,\\n\\nthermocouples,\\n\\nzirconium compounds,\\n\\nAWGN,\\n\\nGlobal Positioning System,\\n\\nadaptive control,\\n\\ncondition monitoring,\\n\\ncontrol system synthesis,\\n\\nelectric resistance,\\n\\nenergy conservation,\\n\\nerror statistics,\\n\\njamming,\\n\\nlamps,\\n\\nlighting control,\\n\\nneurocontrollers,\\n\\npotentiometers,\\n\\nstepping motors,\\n\\nthyristor circuits,\\n\\ntime-varying systems,\\n\\ntriodes\\n\\nShow More\\n\\nPublications\\n\\nCitations\\n\\n19\\n\\nPublications by Year\\n\\n2014\\n\\n2017\\n\\nCo-Authors:\\n\\nVinod Kumar ChaubeyMani Sankar DasguptaSneh DeepDivya JoshiShow All Co-Authors (4)\\n\\nThis Author's Publications\\n\\n\\n\\nExport\\n\\nSearch History\\n\\nShowing 1-5 of 5 results\\n\\nSortNewestSortRelevance Newest Oldest Most Cited By Papers Most Cited By Patents Most Popular Publication Title A-Z Publication Title Z-A\\n\\nFilter Results\\n\\nShow\\n\\nAll Results\\n\\nOpen Access Only\\n\\nYearSingle YearRange2014201720142017From ToApply\\n\\nAuthorAnupam Purwar(5)Divya Joshi(3)Mani Sankar Dasgupta(2)Sneh Deep(1)Vinod Kumar Chaubey(1)Apply\\n\\nAffiliationDepartment of Electrical & Electronics Engineering, Birla Institute of Technology and Science, Pilani, India(2)Department of Aerospace Engineering, Indian Institute of Science, Bangalore, India(2)Department of Mechanical Engineering, Birla Institute of Technology and Science, Pilani, India(2)National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China(1)Aerospace Department, Indian Institute of Science, Bangalore, India(1)Show More…Apply\\n\\nPublication Title2014 Annual IEEE India Conference (INDICON)(1)2015 Annual IEEE India Conference (INDICON)(1)2016 IEEE Annual India Conference (INDICON)(1)2017 14th IEEE India Council International Conference (INDICON)(1)2017 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)(1)Apply\\n\\nPublisherIEEE(5)Apply\\n\\nConference LocationBangalore, India(1)Bengaluru, India(1)New Delhi, India(1)Pune, India(1)Roorkee, India(1)Apply\\n\\nPublication Topicsceramics(2)finite element analysis(2)gas turbines(2)space vehicles(2)temperature measurement(2)Show More…Apply\\n\\nSelect All on Page\\n\\nSort By\\n\\nNewest\\n\\nSort By\\n\\nResults\\n\\nA multipurpose sensor for heat flux and temperature measurement: Design and Computational Analysis\\n\\nAnupam Purwar\\n\\n2017 14th IEEE India Council International Conference (INDICON)\\n\\nYear: 2017\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\n Cited by: \\n\\nPapers (1)\\n\\nAbstract\\n\\nHTML\\n\\n\\n\\n\\n\\nThe\\n development of multi-purpose sensors for sensing heat flux and \\ntemperature has become critical for operation of high temperature \\nstructures viz. gas turbines, furnaces and hypersonic space vehicles. \\nConventional thermocouples and heat flux sensors have limitations owing \\nto the maximum operational temperature of metallic alloys. In this \\nperspective, it is imperative to explore new material syst...\\n\\nShow More\\n\\nA multipurpose sensor for heat flux and temperature measurement: Design and Computational Analysis\\n\\nAnupam Purwar\\n\\n2017 14th IEEE India Council International Conference (INDICON)\\n\\nYear: 2017\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\nA novel thermocouple for ultra high temperature applications: Design and computational analysis\\n\\nAnupam Purwar;Sneh Deep\\n\\n2017 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)\\n\\nYear: 2017\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\n Cited by: \\n\\nPapers (2)\\n\\nAbstract\\n\\nHTML\\n\\n\\n\\n\\n\\nThe\\n development of high temperature sensors for accurately measuring \\ntemperature has become critical for structural health monitoring of high\\n temperature structures viz. gas turbines, furnaces and hypersonic space\\n vehicles. Conventional thermocouples have limitations owing to the \\nmaximum operational temperature of metallic alloys. In this perspective,\\n it is imperative to explore new material syste...\\n\\nShow More\\n\\nA novel thermocouple for ultra high temperature applications: Design and computational analysis\\n\\nAnupam Purwar;Sneh Deep\\n\\n2017 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)\\n\\nYear: 2017\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\nGPS signal jamming and anti-jamming strategy — A theoretical analysis\\n\\nAnupam Purwar;Divya Joshi;Vinod Kumar Chaubey\\n\\n2016 IEEE Annual India Conference (INDICON)\\n\\nYear: 2016\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\n Cited by: \\n\\nPapers (11)\\n\\nAbstract\\n\\nHTML\\n\\n\\n\\n\\n\\nThe\\n weak strength of GPS signal makes it very vulnerable to jamming. This \\ncalls for appropriate modeling and implementation of GPS signal source \\nand jammers that are used to jam GPS signal. The present work describes a\\n simple and efficient way to simulate GPS signal using MATLAB® with \\nfocus on investigation of GPS signal jamming and devising an \\nanti-jamming technique to counter different jamming s...\\n\\nShow More\\n\\nGPS signal jamming and anti-jamming strategy — A theoretical analysis\\n\\nAnupam Purwar;Divya Joshi;Vinod Kumar Chaubey\\n\\n2016 IEEE Annual India Conference (INDICON)\\n\\nYear: 2016\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\nSmart control of electric lamp using artificial intelligence based controller\\n\\nAnupam Purwar;Divya Joshi;Mani Sankar Dasgupta\\n\\n2015 Annual IEEE India Conference (INDICON)\\n\\nYear: 2015\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\n Cited by: \\n\\nPapers (3)\\n\\nAbstract\\n\\nHTML\\n\\n\\n\\n\\n\\nThere\\n is need and scope of controlling the power consumed by conventional \\nelectric lamps in presence of some natural light. An artificial \\nintelligence based control system has been developed to control a lamp \\ndimmer circuit with bidirectional triode thyristor. The light present in\\n the room is sensed and voltage supplied to the lamp is controlled by \\nvarying the time constant of the circuit through ...\\n\\nShow More\\n\\nSmart control of electric lamp using artificial intelligence based controller\\n\\nAnupam Purwar;Divya Joshi;Mani Sankar Dasgupta\\n\\n2015 Annual IEEE India Conference (INDICON)\\n\\nYear: 2015\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\nHuman detection based on explicit skin colour space thresholding and minor motion detection\\n\\nDivya Joshi;Anupam Purwar;Mani Sankar Dasgupta\\n\\n2014 Annual IEEE India Conference (INDICON)\\n\\nYear: 2014\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\\n\\n Cited by: \\n\\nPapers (2)\\n\\nAbstract\\n\\nHTML\\n\\n\\n\\n\\n\\nIn\\n this paper, a novel approach for human presence detection from digital \\nimage is presented. This approach is based on combined clue from skin \\ncolour and minor motion detection. For skin colour detection, explicit \\nthresholding of CbCr colour channels is used and for minor motion \\ndetection morphological differences of multiple consecutive images has \\nbeen used. MATLAB® is used for processing. Relat...\\n\\nShow More\\n\\nHuman detection based on explicit skin colour space thresholding and minor motion detection\\n\\nDivya Joshi;Anupam Purwar;Mani Sankar Dasgupta\\n\\n2014 Annual IEEE India Conference (INDICON)\\n\\nYear: 2014\\n\\n| Conference Paper\\n\\n| Publisher: IEEE\", metadata={'source': 'source_documents/Anupam Purwar - IEEE Xplore Author Profile.html'}), Document(page_content='Author\\n\\nAnupam Purwar\\n\\nResearch Scientist', metadata={'source': 'source_documents/Anupam Purwar - Amazon Science.html'}), Document(page_content='Loading...\\n\\nThe system can\\'t perform the operation now. Try again later.\\n\\nCitations per year\\n\\nDuplicate citations\\n\\nThe following articles are merged in Scholar. Their \\n\\ncombined citations are counted only for the first article.\\n\\nMerged citations\\n\\nThis \"Cited by\" count includes citations to the following articles in Scholar. The ones marked * may be different from the article in the profile.\\n\\nAdd co-authorsCo-authors\\n\\nFollow\\n\\nNew articles by this author\\n\\nNew citations to this author\\n\\nNew articles related to this author\\'s research\\n\\nDone\\n\\nMy profile\\n\\nMy library\\n\\nMetrics\\n\\nAlerts\\n\\nSettings\\n\\ntestmailpush2@gmail.com\\n\\ntestmailpush2@gmail.com\\n\\nMy Account\\n\\nAdd account\\n\\nSign out\\n\\nGet my own profile\\n\\nCited byView all\\n\\nCitations\\n\\n203\\n\\n176\\n\\nh-index\\n\\ni10-index\\n\\n42\\n\\n21\\n\\n2014\\n\\n2015\\n\\n2016\\n\\n2017\\n\\n2018\\n\\n2019\\n\\n2020\\n\\n2021\\n\\n2022\\n\\n2023\\n\\n23\\n\\n23\\n\\n42\\n\\n38\\n\\n37\\n\\n27\\n\\nCo-authors\\n\\n#gsc_rsb-1PPASakAAAAJ-img{width:28px;height:32px;}.gs_el_ta #gsc_rsb-1PPASakAAAAJ-img,.gs_el_ph #gsc_rsb-1PPASakAAAAJ-img{width:49px;height:56px;}Bikramjit BasuProfessor, Materials Research Center, Indian Institute of Science, Bangalore, INDIA; OtherVerified email at iisc.ac.in\\n\\n#gsc_rsb-6qLHtdAAAAAJ-img{width:32px;height:32px;}.gs_el_ta #gsc_rsb-6qLHtdAAAAAJ-img,.gs_el_ph #gsc_rsb-6qLHtdAAAAAJ-img{width:56px;height:56px;}D Roy MahapatraIndian Institute of ScienceVerified email at iisc.ac.in\\n\\n#gsc_rsb-_PHeV3gAAAAJ-img{width:25px;height:32px;}.gs_el_ta #gsc_rsb-_PHeV3gAAAAJ-img,.gs_el_ph #gsc_rsb-_PHeV3gAAAAJ-img{width:44px;height:56px;}M.S DasguptaProfessor Mechanical EngineeringVerified email at pilani.bits-pilani.ac.in\\n\\n#gsc_rsb-plS0F34AAAAJ-img{width:29px;height:32px;}.gs_el_ta #gsc_rsb-plS0F34AAAAJ-img,.gs_el_ph #gsc_rsb-plS0F34AAAAJ-img{width:51px;height:56px;}Ravikumar KUniversity of Pittsburgh, Chemical and Petroleum EnginneringVerified email at pitt.edu\\n\\n#gsc_rsb-gbBQ4x0AAAAJ-img{width:31px;height:32px;}.gs_el_ta #gsc_rsb-gbBQ4x0AAAAJ-img,.gs_el_ph #gsc_rsb-gbBQ4x0AAAAJ-img{width:54px;height:56px;}Sneh DeepPh D, Indian Institute of ScienceVerified email at iisc.ac.in\\n\\n#gsc_rsb-VvD5STUAAAAJ-img{width:32px;height:21px;}.gs_el_ta #gsc_rsb-VvD5STUAAAAJ-img,.gs_el_ph #gsc_rsb-VvD5STUAAAAJ-img{width:56px;height:37px;}Amaresh ChakrabartiProfessor Indian Institute Science Bangalore IndiaVerified email at iisc.ac.in\\n\\n#gsc_rsb-yqlb20wAAAAJ-img{width:32px;height:24px;}.gs_el_ta #gsc_rsb-yqlb20wAAAAJ-img,.gs_el_ph #gsc_rsb-yqlb20wAAAAJ-img{width:56px;height:42px;}Oren SadotBen Gurion University of the NegevVerified email at bgu.ac.il\\n\\n#gsc_rsb-ZAcWZYEAAAAJ-img{width:32px;height:32px;}.gs_el_ta #gsc_rsb-ZAcWZYEAAAAJ-img,.gs_el_ph #gsc_rsb-ZAcWZYEAAAAJ-img{width:56px;height:56px;}Ben-Dorben gurion universityVerified email at bgu.ac.il\\n\\n#gsc_rsb-DpkqTwcAAAAJ-img{width:32px;height:30px;}.gs_el_ta #gsc_rsb-DpkqTwcAAAAJ-img,.gs_el_ph #gsc_rsb-DpkqTwcAAAAJ-img{width:56px;height:53px;}Swaroop Anand HangalIndian Institute of Technology BombayVerified email at iitbombay.org\\n\\n#gsc_rsb-VI8PjkEAAAAJ-img{width:32px;height:32px;}.gs_el_ta #gsc_rsb-VI8PjkEAAAAJ-img,.gs_el_ph #gsc_rsb-VI8PjkEAAAAJ-img{width:56px;height:56px;}venkateswaran ThiruvenkatamScientist, Vikram Sarabhai Space Centre/ISRO\\n\\n#gsc_rsb-hr2IZacAAAAJ-img{width:27px;height:32px;}.gs_el_ta #gsc_rsb-hr2IZacAAAAJ-img,.gs_el_ph #gsc_rsb-hr2IZacAAAAJ-img{width:48px;height:56px;}Ariharan S.Scientist/Engineer-SD, Vikram Sarabhai Space Centre, ISRO, Thiruvananthapuram, India\\n\\n#gsc_rsb-k2QxS5UAAAAJ-img{width:31px;height:32px;}.gs_el_ta #gsc_rsb-k2QxS5UAAAAJ-img,.gs_el_ph #gsc_rsb-k2QxS5UAAAAJ-img{width:54px;height:56px;}Vinod ChaubeyEEE Department, BITS PilaniVerified email at pilani.bits-pilani.ac.in\\n\\n#gsc_rsb-NmrxWAEAAAAJ-img{width:27px;height:32px;}.gs_el_ta #gsc_rsb-NmrxWAEAAAAJ-img,.gs_el_ph #gsc_rsb-NmrxWAEAAAAJ-img{width:48px;height:56px;}Vikrant BhakarSustainability Technologist, Irish Manufacturing ResearchVerified email at imr.ie\\n\\n#gsc_rsb-IP24HdQAAAAJ-img{width:32px;height:32px;}.gs_el_ta #gsc_rsb-IP24HdQAAAAJ-img,.gs_el_ph #gsc_rsb-IP24HdQAAAAJ-img{width:56px;height:56px;}Christoph HerrmannProfessor for Sustainable Manufacturing and Life Cycle Engineering, Technische Universität BraunschweigVerified email at tu-braunschweig.de\\n\\n#gsc_rsb-5UXTiBEAAAAJ-img{width:32px;height:32px;}.gs_el_ta #gsc_rsb-5UXTiBEAAAAJ-img,.gs_el_ph #gsc_rsb-5UXTiBEAAAAJ-img{width:56px;height:56px;}B Venkata Manoj KumarProfessor, Metallurgical and Materials Engineering, IIT RoorkeeVerified email at iitr.ac.in\\n\\nFollow\\n\\nAnupam Purwar\\n\\nResearcher, \\n\\nIndian Institute of Science, Bangalore\\n\\nVerified email at mrc.iisc.ernet.in - \\n\\nHomepage\\n\\nMaterials Development\\n\\nFEM\\n\\nStructural design\\n\\nSustainable technologies\\n\\nMachine Learning\\n\\nArticles\\n\\nCited by\\n\\nCo-authors\\n\\nTitle\\n\\nSort\\n\\nSort by citations\\n\\nSort by year\\n\\nSort by title\\n\\nCited byCited by\\n\\nYear\\n\\nThermo‐structural design of ZrB2–SiC‐based thermal protection system for hypersonic space vehiclesA Purwar, B BasuJournal of the American Ceramic Society 100 (4), 1618-1633, 2017\\n\\n51\\n\\n2017\\n\\nDevelopment of ZrB2–SiC–Ti by multi stage spark plasma sintering at 1600°CP Anupam, R Krishnamurthy, A S., KG Nagarajan, B BikramjitJournal of the Ceramic Society of Japan 124 (No. 4 (April)), 393-402, 2016\\n\\n41\\n\\n2016\\n\\nExperimental and computational analysis of thermo‐oxidative‐structural stability of ZrB2‐SiC‐Ti during Arc‐jet testingA Purwar, T Venkateshwaran, B BasuJournal of the American Ceramic Society 100 (10), 2017\\n\\n31\\n\\n2017\\n\\nGPS Signal Jamming and Anti-jamming Strategy - A Theoretical AnalysisA Purwar, D Joshi, VK ChaubeyINDICON 2016, 2016\\n\\n19\\n\\n2016\\n\\nDesign of Thermal Barrier Coating System for Scramjet using Coupled Thermo- Structural AnalysisA Purwar, DR MahapatraTransactions of the Indian Ceramic Society 75 (4), 242-249, 2016\\n\\n10\\n\\n2016\\n\\nExperimental and computational analysis for thermo-erosive stability assessment of ZrB2-SiC based multiphase compositesA Purwar, S Ariharan, B BasuInternational Journal of Refractory Metals and Hard Materials 84, 104972, 2019\\n\\n2019\\n\\nThermo-Structural Design of Strut Based Flame Holder for Scramjet CombustorA PurwarInternational Conference on Research into Design 65 (1), 105-115, 2017\\n\\n2017\\n\\nA novel thermocouple for ultra high temperature applications: Design and computational analysisA Purwar, S Deep2017 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia\\xa0…, 2017\\n\\n2017\\n\\nHuman detection based on explicit skin colour space thresholding and minor motion detectionD Joshi, A Purwar, MS Dasgupta2014 Annual IEEE India Conference (INDICON), 1-6, 2014\\n\\n2014\\n\\nResearch into Design for Communities, Volume 2A Chakrabarti, D ChakrabartiSpringer, 2017\\n\\n2017\\n\\nSmart control of electric lamp using artificial intelligence based controllerA Purwar, D Joshi, MS Dasgupta2015 Annual IEEE India Conference (INDICON), 1-5, 2015\\n\\n2015\\n\\nThermo-structural design of hypersonic vehicle sharp leading edges for thermo-erosive stability using finite element modellingA Purwar31st International Symposium on Shock Waves 2: Applications 31, 1027-1034, 2019\\n\\n2019\\n\\nA methodology for coupled thermal-structural analysis and structural design of scramjet combustorA Purwar, DR Mahapatra, N Thakor, KS Priyamvada, R Mukherjee30th International Symposium on Shock Waves 1: ISSW30-Volume 1, 245-250, 2017\\n\\n2017\\n\\nThermal Analysis of Scramjet Combustor Panel with Active Cooling Using Cellular MaterialsR Mukherjee, NK Gopinath, V Vignesh, A Purwar, DR Mahapatra30th International Symposium on Shock Waves 1: ISSW30-Volume 1, 239-244, 2017\\n\\n2017\\n\\nAutomated Planning Tool (APT): A mixed integer non-linear programming problem solver for Workorder schedulingP AnupamAmazon Science, 2022\\n\\n2022\\n\\nEffect of Different Root Canal Filling Materials in Endo-perio Lesions: Design and Computational AnalysisP Anupam, PragyaDesign for Tomorrow, Springer, Print ISBN: 978-981-16-0083-8 3, 39-49, 2021\\n\\n2021\\n\\nA multipurpose sensor for heat flux and temperature measurement: Design and Computational AnalysisA Purwar14th IEEE India Council International Conference (INDICON), 2017\\n\\n2017\\n\\n30th International Symposium on Shock Waves 1: ISSW30-Volume 1G Ben-Dor, O Sadot, O IgraSpringer, 2017\\n\\n2017\\n\\nHuman Detection Based on Explicit Skin Colour Space Thresholding and Minor MotionD Joshi, A Purwar, MS DasguptaIEEE INDICOM, 2014\\n\\n2014\\n\\nARIMAX model for forecasting maintenance work (AMFM): A multi-stage seasonal ARIMAX model for work order time series forecastingP Anupam, M ReimherrConference on Digital Economy, 2023\\n\\n2023\\n\\nThe system can\\'t perform the operation now. Try again later.\\n\\nArticles 1–20\\n\\nShow more\\n\\nHelp\\n\\nPrivacy\\n\\nTerms', metadata={'source': 'source_documents/_Anupam Purwar_ - _Google Scholar_.html'}), Document(page_content='ARIMAX Model for Forecasting Maintenance Work (AMFM): A Multi-Stage\\nSeasonal ARIMAX Model for Workorder Time Series Forecasting\\n\\nAnupam Purwar\\n\\nMatthew Reimherr\\n\\nAmazon Science\\n\\nAbstract\\n\\nE-commerce business depends on smooth day\\nto day functioning of it’s warehouses/facilities.\\nThe functioning of these facilities depend on the\\nhealth of material handling equipment. To keep\\nthese equipment healthy, these facilities employ\\nthe help of maintenance engineers who perform\\npredictive/breakdown maintenance work. To en-\\nsure an effective maintenance operation necessi-\\ntates efficient planning of maintenance work. For\\nefficient planning of future maintenance work,\\none needs to have good estimates of the fu-\\nture maintenance work. We created time se-\\nries of maintenance work (breakdown and mis-\\ncellaneous) in terms of demand hours for every\\nday/week. Next, we built several models and\\nevaluated these models on the basis of forecast-\\ning accuracy metrics viz. Mean Absolute per-\\ncent error (MAPE) and Root mean squared er-\\nror (RMSE) to determine which modelling tech-\\nnique is most suitable. Seasonal ARIMA with\\nexogenous variable (SARIMAX) was found to be\\nthe most suitable approach with additional hyper-\\nparameters like training dataset length and train-\\ning data window start/end. This paper discusses\\nthe details of this SARIMAX approach and the\\nprocedure used to identify the best facility spe-\\ncific SARIMAX model. The proposed solution\\nprovides forecasts using SARIMAX framework\\nwith an out of sample MAPE less than 30 per-\\ncent and RMSPE less than 20.\\n\\n1\\n\\nIntroduction\\n\\nTime series forecasting has been integral part of data sci-\\nence and statistics with the earliest applications in the area\\n\\nCODE 2023, Copyright Author (s). Do not distribute.\\n\\nof production planning and control, weather and stock mar-\\nket forecasting [1, 2]. Time series analysis and forecasting\\nas a field dates back to the early 1930s [3]. Forecasting\\nhas been used for pretesting market launch of new prod-\\nucts [4, 5, 6, 7, 8]. Benefits of integrated business planning,\\nforecasting, and process management has been demon-\\nstrated by Toor et. al. [9]. Application of demand Fore-\\ncasting for e-commerce Platforms has also been attempted\\nin past [10, 11]. However, forecasting maintenance engi-\\nneering work for e-commerce facilities using time series\\ndata has not been reported yet.\\n\\nScheduling and forecasting maintenance work is a critical\\nproblem in the world of maintenance engineering, but it\\nis mostly done at an equipment level to predict equipment\\nbreakdown [12, 13, 14]. Historically, it has been solved us-\\ning machine learning as well as classical time series fore-\\ncasting techniques for facility and asset maintenance by\\nvarious industrial software providers [15, 16, 17, 18]. Most\\nindustrial forecasting solutions are available as add on fea-\\ntures in an Enterprise Asset Management Software and of-\\nfer limited customization based on the business realities of\\nthe end user. Many of these solutions are tailored more to-\\nwards needs of traditional manufacturing, aerospace and\\npower-plant maintenance. Maintenance work prediction\\nfor electronics systems has been carried using SARIMA\\nSimilarly, predictive maintenance of production\\n[14].\\nequipment has been reported using neural network autore-\\ngression and ARIMA [19, 20]. However, the varied na-\\nture/type and complexity of equipment in e-commerce fa-\\ncilities, with every facility being unique, necessitates the\\ndevelopment of forecasting models which capture the fa-\\ncility specific business realities. For example, one facil-\\nity may may differ from another in terms of its operational\\nhours, available maintenance hours, type, nature and num-\\nber of equipment as well as the days on which a facility\\nis shut down owing to holidays. Hence, one needs to de-\\nvelop facility specific forecasting models to accurately pre-\\ndict maintenance activities in e-commerce facilities. Be-\\nsides, these facilities keep changing with time due to dy-\\nnamic nature of e-commerce business, which necessitates\\ndeveloping an approach which can help adapt to the new re-\\nality. Also, there is a lack of availability of standard meth-\\nods to estimate/predict the number of hours required for\\n\\n\\x0cAMFM: SARIMAX based Forecasting Framework\\n\\nmaintenance activities [21]. Forecasting model for main-\\ntenance and repair costs of buildings has been explored\\n[22], also approaches for maintenance forecasting man-\\nagement have been explored [23]. However, scientific lit-\\nerature reporting development of forecasting maintenance\\nwork in the context of e-commerce facilities remains lim-\\nited to internal consumption of operators of these facili-\\nties or does not exist. In this context, we are reporting a\\nforecasting approach which we devised to suit the facility\\nspecific as well as dynamic (time dependent) nature of the\\ne-commerce facilities. The findings from our investigation\\nto forecast the maintenance work are discussed in detail by\\nbringing out the comparison between different forecasting\\ntechniques. The proposed approach uses a grid of hyper-\\nparameters which include exogenous variables which influ-\\nence the workorder generation process as well as hyperpa-\\nrameters that guide selection of the most appropriate train-\\ning data.\\n\\nPresent day e-commerce facility process large volumes of\\ncustomer orders everyday, which is only possible because\\nof the high reliability of equipment supporting these fa-\\ncilities. The maintenance work pertaining to these equip-\\nment is digitally recorded as a workorder. The health\\nof these equipment is taken care by facility maintenance\\nteams by executing and planning/scheduling workorders\\n(WOs) for thousands of equipment of different types ev-\\nery day. This maintenance work is divided into three major\\ncomponents: (1) scheduled and planned preventive main-\\ntenance, including planned repairs and shutdowns, and (2)\\nemergency/breakdown maintenance and (3) miscellaneous.\\nThe first one is deterministic in nature, whereas the latter\\ntwo depend on the probabilistic failure pattern and facility\\nspecific business reality. The latter two contribute to uncer-\\ntainty in maintenance forecasting and capacity planning. In\\nthis work, we have gathered breakdown and miscellaneous\\nmaintenance WO time series as these are the ones that bring\\nuncertainty in planning [24]. Each workorder (WO) re-\\nquires a well defined number of maintenance hours to com-\\nplete the maintenance of that equipment. This workorder\\nhours demand across a span of time has been gathered and\\nmodelled as time series in the present work to forecast the\\ndemand of workorder hours for next 3 weeks. This fore-\\ncast is essential in determining the number of maintenance\\nhours required and available operational hours (Available\\nOperational Hours = Total Hours - Maintenance Hours).\\nAvailable operational hours in turn help the facility decide\\nthe shipment handling capacity as well as number of main-\\ntenance engineers required and configuration of day/night\\nshifts.\\nIn the absence of this workorder hours forecast,\\nthese decisions remain ad-hoc and tempo-spatially specific\\nin nature. As important facility specifications like ship-\\nment handling capacity and required number of mainte-\\nnance engineers could not be updated periodically depend-\\ning on changes in demand workorder hours. This leads to\\ninstances where demand for maintenance work is greater\\n\\nthan available maintenance hours leading to delay in com-\\npletion of workorders as well as leftover shipments which\\nfacility is unable to process. To bridge these gaps, one\\nneeds to pro-actively know the demand of workorders for\\neach facility and accordingly plan [25].\\n\\nAccurate maintenance forecasting has the potential to in-\\ncrease operational availability and reduce maintenance-\\nrelated downtime [26]. WO forecasting also helps e-\\ncommerce business to pro-actively know the volume of\\nmaintenance work at facility level and then project the same\\nfor a cluster of co-located facilities.This in turn can help fa-\\ncilitate cluster level work aggregation for maintenance staff\\nacross the co-located facilities and realize associated ef-\\nficiencies. This opens up the opportunity to move away\\nfrom facility specific planning to network wide/centralized\\nplanning. Centralized planning using data driven insights\\ncan help create scenario specific plans by pro-actively ad-\\njusting maintenance hours availability across planning pe-\\nriod for a group of sites ahead of time. An excess demand\\nof maintenance WO hours can be fulfilled by extra main-\\ntenance hours available with another facility in the same\\ncluster. Also, it makes workorder scheduling more accu-\\nrate/ robust bringing uniformity in planning across multiple\\nfacilities in the network. Hence, we collected time series\\ndata of workorders for multiple facilities and developed a\\ndata driven solution which uses past 52 weeks data for fore-\\ncasting the demand of workorders across these facilities for\\nthe following 3 weeks. This solution, namely an ARIMAX\\nModel for Forecasting Maintenance Work (AMFM), pro-\\nvides the e-commerce facilities a visibility into future and\\nplan as well prioritize high criticality maintenance work\\nahead of time.\\n\\n2 Notation List\\n\\nACF: Autocorrelation Function\\n\\nAIC: Akaike Information Criterion\\n\\nBIC: Bayesian Information Criterion\\n\\nMISC Miscellaneous Maintenance in Hours\\n\\nPACF Partial Autocorrelation Function\\n\\nWO Maintenance Work Order\\n\\nMAPE Mean Absolute Percent Error\\n\\nRMSPE Root Mean Squared Percent Error\\n\\n3 Problem Statement\\n\\nThe primary problem we are solving through this work is to\\nforecast workorders which contribute to the uncertainty in\\nplanning, namely the breakdown or reactive WOs as well\\nas three other types of WOs called training, project and\\n\\n\\x0cAnupam Purwar, Matthew Reimherr\\n\\nadmin (broadly classified as miscellaneous). Through ac-\\ntive forecasting of Demand across these 4 workorder types,\\nwe envisage to drive data informed decisions to influence\\neach facility’s plan. The purpose of building the forecasting\\nmodel is to ensure every facility has an accurate forecast for\\nWO hours at a weekly frequency. To achieve this, we need\\nto meet following objectives:\\n\\n• Forecasting model needs to provide consistent fore-\\ncast of WO hours (aggregate of different WO types)\\nat a facility level.\\n\\n• Approach needs to be accurate enough to have good\\nout of sample accuracy as well as generalizable across\\nmultiple facilities\\n\\n• Model shall be parsimonious and results explainable\\n\\nto facility planning teams\\n\\nTo meet the above objectives, we need to critically examine\\ntime series corresponding to each WO type as well as at\\nan aggregate level and identify important features which\\ninfluence the behaviour or movement of these time series.\\n\\n4 Model Development\\n\\nIn the present work, we have developed a forecasting model\\ninspired from the use case of maintenance work planning\\ndone in e-commerce facilities. The problem is to build fa-\\ncility specific forecasting models for multiple time series\\npertaining to different nature of workorders. The complex-\\nity of this effort is governed by the nature of facility spe-\\ncific business realities, eg. days/weeks on which there is\\nsudden increase or drop in maintenance activities. There\\nare both known unkowns and unkown unkowns which are\\nto be identified and accounted for while selecting the most\\nappropriate approach for forecasting. Besides, we are lim-\\nited by the length of historic time series data which can\\nbe used to train the forecasting model. We cannot use\\ndata which is older than 1 year because the facilities un-\\ndergo many changes every 4 quarters in terms of equipment\\nand maintenance schedules due to the ever evolving nature\\nof e-commerce facilities. Last but not the least, the fore-\\ncast needs to be auto published every week by consuming\\nstreaming time series data. The forecast model also needs\\nto be adaptively learn from any change in the business real-\\nity which governs the generation of WOs and accordingly\\nadjusting the forecast model. We have documented the\\nmodel development into multiple sections viz. data prepa-\\nration, model creation, model testing and deployment (Sec-\\ntion 4 and Section 5). Results are discussed in Section 6,\\nfollowed by conclusions in Section 7.\\n\\n4.1 Data Preparation\\n\\nWe picked time series data of different WO types from a\\ndozen facilities and for each time series performed Aug-\\nmented Dicky Fuller (ADF) 1 test to evaluate the station-\\nary/ non-stationary nature of the time series. We also cal-\\nculated rolling statistics (rolling mean and standard devia-\\ntion). This revealed that some of the time series (Reactive-r,\\nProject-p, Admin-a, Training-t, Billable- b= r+p+t) are sta-\\ntionary whereas others are non-stationary in nature, refer\\nFigure 1. The non-stationary time series need to be made\\nstationary by suitable differencing.\\n\\nFigure 1: Results for site 1: Rolling Mean and Standard\\ndeviation for different Time series\\n\\nFigure 2: Results for site 1: Additive decomposition for\\nTime series = ’b’\\n\\n1Augmented Dickey Fuller test (ADF Test) is a common sta-\\ntistical test used to test whether a given Time series is stationary\\nor not.\\n\\n\\x0cAMFM: SARIMAX based Forecasting Framework\\n\\nunreliable for training a forecasting model which is sup-\\nposed to make daily prediction. Also, working backwards\\nfrom the business requirement of a weekly forecast, it was\\ndecided to aggregate time series to transform daily data into\\nweekly. Next we devised, a more advanced 2 stage SARI-\\nMAX has been developed to discover the best SARIMAX\\nmodel for each site and time series. We have developed a\\ntool around this model called ARIMAX Model for Fore-\\ncasting Maintenance Work (AMFM). US holiday data has\\nbeen used as exogenous variable used in the SARIMAX\\napproach owing to the fact that during holiday period, the\\nlevel of maintenance activity changes for the facilities. Be-\\nsides, the selection of historic time series dataset is limited\\nto last 365 days as the facility specific reality like the num-\\nber of equipments and no of technicians (maximum techni-\\ncian hours available) also changes as a function of time. We\\nhave added another layer of filtering over the historic time\\nseries dataset where we chop off some data at the begin-\\nning (Beg Date) and ending (End Date) by evaluating the\\nmodel accuracy metrics, as shown in Figure 4. This helps\\nus remove time series data which may be outlier in terms of\\nchanges happening at the facility due to an unplanned facil-\\nity shutdown or a high criticality maintenance event. Thus,\\nfiltering historic time series data filters out anomalies which\\nmay impact the robustness of the forecasting model.\\n\\n4.3\\n\\n2-stage SARIMAX based model\\n\\nIn stage 1, we create multiple S-ARIMAX models by vary-\\ning the length of historic dataset and calculate their AIC\\nvalues. The length of the historic dataset is varied by\\nchanging the beginning date (Beg Date) and end date (End\\nDate), refer Figure 4. From this historic dataset, we se-\\nlect the training dataset using the hold out ratio 1. Next,\\nwe train the S-ARIMAX model using this training dataset.\\nFor training each S-ARIMAX model, we pick different pa-\\nrameters from a grid of (p, d, q)- (P, D, Q) values and pick\\nthe top 10 S-ARIMAX models with the lowest AIC values\\nand statistically significant parameter estimates (p < 0.05),\\nas shown in Table 2. For the top 10 models picked from\\nStage 1, we forecast values by breaking the time series ac-\\ntual data (historic data) into train and test using hold out\\nratio 2. Then, the forecast values for these 10 models are\\nscanned again to check for forecast values which cross the\\nmaximum technician availability hours (max hrs) for the\\nfacility. If the forecast values exceed the maximum tech-\\nnician available hours (max hrs) for the facility, then it is\\nscaled to max hrs. The number of times scaling is done\\nis counted and recorded as scaling fraction.\\nIn parallel,\\nwe also calculate MAPE/RMSPE for the test data (out of\\nsample values based on hold out ratio 2). Now, we have\\ncalculated 3 metrics viz. MAPE, RMSPE and count of\\nsmoothing/scaling for each of the 10 models for a given\\nfacility and time series combination (refer Fig.5). We iden-\\ntify the best model as the one which has the lowest MAPE\\n\\nFigure 3: Results for site 1: Multiplicative decomposition\\nfor Time series = ’b’\\n\\n4.2 Selection of Forecasting method\\n\\nDuring preliminary modelling, we experimented with mul-\\ntiple approaches like TBATS, Simple Exponential Smooth-\\ning (SES), Auto Regressive Integrated Moving Average\\n(ARIMA), Auto Regressive Integrated Moving Average\\nwith Exogenous variable (ARIMAX), Seasonal Auto Re-\\ngressive Integrated Moving Average with Exogenous vari-\\nable (SARIMAX) for daily as well weekly aggregated time\\nseries data. To date, these forecasting models have seen lit-\\ntle use in the area of maintenance engineering and authors\\ncould not find use of these models in forecasting main-\\ntenance work for e-commerce facilities. We performed\\nweekly aggregation of the daily WO time series data, how-\\never even after weekly aggregation some of these time se-\\nries are stationary and some non-stationary. Among all the\\napproaches investigated, SARIMAX resulted in better fore-\\ncasting performance in terms of MAPE/RMSPE, as shown\\nin Table 1.\\n\\nSARIMAX model has been used for short-term load\\nforecasting [27], decomposition method with SARIMAX\\nmodel has been demonstrated to give low MAPE [28].\\nSARIMAX modeling has been used for electricity genera-\\ntion forecasting of grid-connected Photovoltaic (PV) plants\\n[29] as well as for forecasting emergency department (ED)\\nhourly occupancy [30]. Experimenting with both daily and\\nweekly aggregated data, we found that daily data does not\\nallow selection of a generalized forecasting method for all\\nthe 4-time series across different facilities. A deep dive\\nwith the facilities around the actual time series data for dif-\\nferent WO types uncovered that WO hours are some times\\nbooked/logged into the system on just 1 day of the week\\nand the day of the week when they get logged may also\\nvary from one facility to another. This makes the daily data\\n\\n\\x0cAnupam Purwar, Matthew Reimherr\\n\\nTable 1: Comparison of different Forecasting Techniques\\n\\nForecasting Techniques\\n\\nBillable (b=p+r+t) Admin (a) Billable: Reactive (r) Billable: Project (p) Billable: Training (t)\\n\\nMetric(X=US Holidays) MAPE\\n\\nDeep AR*\\nTBATS\\nHolts winter\\nSES\\nSARIMAX**\\nARIMAX\\nMedian/Mean\\nMethod of proportions\\n\\n38.73\\n52.11\\n40.32\\n39.25\\n37.71\\n42.25\\n56.8\\n-\\n\\nMAPE\\n\\n143.2\\n24.86\\n149.77\\n115.36\\n24.5\\n56.26\\n-\\n-\\n\\nMAPE\\n\\n44.35 (p50)\\n-\\n-\\n-\\n15.22\\n42.5\\n65,55\\n94\\n\\nMAPE\\n\\n71 (p10)\\n-\\n-\\n-\\n30.5\\n57.5\\n76, 121\\n149\\n\\nMAPE\\n\\n96 (p10)\\n-\\n-\\n-\\n241\\n241\\n> 1e3\\n> 1e7\\n\\nTable 2: SARIMAX Model Parameters\\n\\nDep. Variable:\\nModel:\\nDate:\\nTime:\\nSample:\\n\\nCovariance Type:\\n\\ny\\nSARIMAX(3, 1, 3)x(2, 1, [1, 2], 7)\\nMon, 22 Aug 2022\\n10:10:35\\n0\\n- 44\\nopg\\n\\nNo. Observations:\\nLog Likelihood\\nAIC\\nBIC\\nHQIC\\n\\n44\\n-82.010\\n188.021\\n207.023\\n194.653\\n\\nholidays\\nar.L1\\nar.L2\\nar.L3\\nma.L1\\nma.L2\\nma.L3\\nar.S.L7\\nar.S.L14\\nma.S.L7\\nma.S.L14\\nsigma2\\n\\nstd err\\n\\n1e-06\\n3.25e-11\\n5.31e-10\\n2.02e-09\\n2.75e-08\\n9.47e-10\\n9.48e-09\\n6.42e-09\\n1.9e-08\\n4.12e-09\\n4.31e-10\\n5.03e-07\\n\\nz\\n\\nP> |z|\\n\\n-7.73e+07\\n-6.58e+09\\n4e+08\\n4.96e+08\\n1.06e+08\\n3.09e+09\\n1.06e+08\\n3.11e+08\\n-5.25e+07\\n4.85e+08\\n2.32e+09\\n2.3e+09\\n\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n\\nLjung-Box (L1) (Q):\\nProb(Q):\\nHeteroskedasticity (H):\\nProb(H) (two-sided):\\n\\nJarque-Bera (JB):\\nProb(JB):\\nSkew:\\n\\n6.47\\n0.01\\n0.00\\n0.00 Kurtosis:\\n\\n735.60\\n0.00\\n3.70\\n23.87\\n\\n\\x0cAMFM: SARIMAX based Forecasting Framework\\n\\nand has a count of smoothing/scaling fraction not exceed-\\ning 20 percent for the given time series and facility combi-\\nnation (5). The proposed approach varies multiple model\\nhyper parameters (historic data selection window, hold out\\nratio for stage 1, SARIMA model order and hold out ratio\\nfor stage 2 ) to identify the best forecasting model based on\\nMAPE/RMSPE as well as scaling fraction/count. All these\\nhyperparameters are summarized in Table 3.\\n\\nFigure 4: Window Hyper Parameter: Selection of Training\\nData\\n\\n5 Deployment Infrastructure\\n\\nCurrently, AMFM model has a data pipe which fetches real\\ntime time series data of Workorder hours for each type.\\nNext, this time series data is post-processed by the Fore-\\ncasting model hosted in an EC2 instance, as shown in Fig-\\nure 5). All modules of AMFM are written in Python and\\nutilizes AWS infrastructure for data storage/handling (refer\\nFigure 6). It’s solver module uses the statsmodel library\\nwhich is an open source library for regression and forecast-\\ning problems [31]. AMFM users can access/visualize the\\nforecast values for each time series as well as the aggregate\\nby accessing the AWS Quicksight dashboard where the re-\\nsults are published every week. On the output side, the\\nQuicksight dashboard provides several metrics including\\nthe forecast accuracy and actual vs. forecast values week\\non week.\\n\\n6 Results\\n\\n6.1 Results: Forecasting Model in Production\\n\\nThe WO forecasting model has been taken into production\\nwith the data engineering and cloud compute infrastructure\\nbuilt using the AWS Services like S3 buckets, Redshift for\\ndata storage and EC2 for cloud compute. We have also\\nsetup a mechanism to monitor the forecasts going forward,\\n\\nFigure 5: Multiple Forecasting Models developed using 2-\\nStage SARIMAX\\n\\nusing AWS Quicksight Dashboard. The forecast model\\nruns once every week and publishes the forecast values to\\nS3 buckets and Redshift tables. Currently, forecasts are\\nmade for all the facilities on-boarded and the forecast data\\nIn long\\nis published to an AWS Quicksight Dashboard.\\nrun, a user interface would also be created around the fore-\\ncast model which will be hosted in an auto scaling compute\\ncluster with a code package for the end users to selectively\\nrun the model for their facility and consume forecast as flat\\n.csv files.\\n\\n6.2 Results: Accuracy Metrics\\n\\nThe proposed approach creates multiple models for each\\nfacility and then selects the best model in terms of MAPE,\\nRMSPE and smoothing fraction. Table ?? and Table 4 sum-\\nmarize the accuracy metrics for the best model along with\\nthe model number. We can see that the accuracy metrics\\nvary for different facilities as well as the SARIMAX model\\norder. The accuracy statistics vary from one facility to an-\\nother which in turn is dependent on the operating nature of\\neach facility. This also highlights the importance of having\\n\\n\\x0cAnupam Purwar, Matthew Reimherr\\n\\nTable 3: Grid of Hyper Parameters: Forecasting Model Selection\\n\\nHyperparameter\\n\\nValue 1 Value 2 Value 3\\n\\nValue 4\\n\\nStages of optimization\\n\\np, d, q and s-P, D, Q p-value<0.05, Stage1-AIC, Stage2-MAPE\\n\\nHold out ratio 2\\nHold out ratio 1\\nBeg\\nEnd\\n\\nScaling count\\n\\n0.7\\n0.7\\n365\\n0\\n\\n0.8\\n0.8\\n355\\n5\\n\\n0.85\\n0.85\\n345\\n10\\n\\n0.9\\n0.9\\n335\\n15\\n\\n15%\\n\\nPost stage 1/2\\n\\nStage 1/2\\n\\nStage 2\\nStage 1\\nStage 1/2\\nStage 1/2\\n\\nFigure 6: Forecast Model built and Deploy using AWS\\nStack\\n\\ndifferent models for each facility. Besides, the selection\\nwindow specified in terms of Beg Date and End Date (re-\\nfer Figure 4) which results in the best accuracy also varies\\nfrom one facility to another. This demonstrates the im-\\nportance of iteratively selecting the best historic time win-\\ndow for training to come out with the most robust forecast\\nmodel. We carried out experiments by aggregating actuals\\nof different WO types viz. p, t, r and used it to forecast\\nbillable workorder hours per week. The accuracy metrics\\nviz. MAPE/RMSPE demonstrate lower values within ac-\\nceptable limits as well more consistency, refer Table 4.\\n\\n7 Conclusion\\n\\nThis paper developed a forecasting framework to predict\\nrequired maintenance hours using a family of models cus-\\ntomized for each facility and time series type. Devel-\\nopment of the model involved understanding the worko-\\nrder creation process of e-commerce facility maintenance\\nteams besides experimenting with different forecasting\\ntechniques and then performing a comparative analysis.\\nSome important facts about the AMFM model are as fol-\\nlows:\\n\\n1. SARIMAX turned out to be the best model for these\\ntime series for mid and short term forecasting. The\\nresult of this approaches is also easily explainable, but\\nfor long term high accuracy prediction of WO hours,\\nwe need to gather more features/exogenous variables.\\n\\nin customer demand) ramp up dates as another exoge-\\nnous variable model accuracy can be further explored\\nto investigate if it could help account for spikes in de-\\nmand of WO hours\\n\\n3. The 2-stage SARIMAX approach, particularly the se-\\nlection of historic time series data using a sliding win-\\ndow, helps select the training data by filtering out\\nanomalous data points which do not follow the trend\\n\\n4. Classical forecasting techniques like SARIMA aug-\\nmented by business specific realities can help create\\nforecasting models customized for each facility in a\\nnetwork of e-commerce facilities even with limited\\nhistoric time series data.\\n\\n5. Facility specific periodic planning of maintenance en-\\ngineering work can be made more accurate by deploy-\\ning workorder forecasting models across e-commerce\\nfacilities.\\n\\nIn the future, we plan to do a careful outlier detection exer-\\ncise to build more robust models which provide repeatable\\nhigh accuracy forecasts rather than models biased by out-\\nliers. This is why MAPE can be a better indicator of a\\nforecast model’s accuracy compared to RMSPE.\\n\\n8 Acknowledgments\\n\\nThe authors would like to thank the RME leadership for\\nsupporting the publication of salient results from this re-\\nsearch work. Last but not the least, I would also like to\\nthank everyone who provided feedback for continuous im-\\nprovement of this research.\\n\\nReferences\\n\\nCowles, A. (1933), “Can stock market forecasters fore-\\n\\ncast?” Econometrica, 1, 309–324.\\n\\n2. Forecast accuracy has been improved by including US\\nholidays as an exogenous variable. On a similar line,\\nthe impact of including pre-peak (days before surge\\n\\nCraft, E.D. (1998), “The value of weather information\\nservices for nineteenth century Great Lakes shipping,”\\nAmerican Economic Review, 88, 1059–1076\\n\\n\\x0cAMFM: SARIMAX based Forecasting Framework\\n\\nTable 4: Accuracy metrics for Billable (b) Workorders Forecast, b = p+r+t, Run Date=02/08/22\\n\\nOrder\\n\\nSes. order RMSPE MAPE Beg Date End Date AIC Model no\\n\\nSm. frac.\\n\\nFacility Model count\\n\\n(0, 1, 1)\\n(0, 1, 0)\\n(0, 1, 2)\\n(0, 1, 1)\\n(0, 1, 0)\\n\\n(0, 1, 1, 7)\\n(0, 1, 0, 7)\\n(0, 1, 0, 7)\\n(2, 1, 3, 7)\\n(1, 1, 0, 7)\\n\\n22.1\\n8.5\\n29.1\\n72.0\\n25.2\\n\\n30.2\\n6.9\\n19.4\\n14.4\\n17.7\\n\\n07/08/21\\n16/09/21\\n27/08/21\\n07/08/21\\n17/08/21\\n\\n03/07/22\\n03/07/22\\n13/07/22\\n03/07/22\\n02/08/22\\n\\n352.0\\n324.7\\n394.1\\n435.0\\n341.5\\n\\n53\\n348\\n128\\n43\\n57\\n\\n0.0\\n0.0\\n0.0\\n0.0\\n0.2\\n\\n1\\n2\\n3\\n4\\n5\\n\\n315\\n349\\n209\\n150\\n201\\n\\nOgburn, W. F. (1934), “Studies in prediction and the dis-\\n\\ntortion of reality,” Social Forces, 13, 224–229.\\n\\nShocker, A. D. W. G. Hall (1986), “Pretest market mod-\\nels: A critical evaluation,” Journal of Product Innovation\\nManagement, 3, 86–107.\\n\\nSilk, A.J. G. L. Urban (1978), “Pre-test market eval-\\nuation of new product goods: A model and measure-\\nment methodology,” Journal of Marketing Research, 15,\\n171–191.\\n\\nTheil, H. R. F. Kosobud (1968), “How informative are con-\\nsumer buying intention surveys?” Review of Economics\\nand Statistics, 50, 207–232.\\n\\nTobin, J. (1959), “On the predictive value of consumer in-\\ntentions and attitudes,” Review of Economics and Statis-\\ntics, 41, 1–11.\\n\\nUrban, G. L., B. D. Weinberg J. R. Hauser (1996), “Pre-\\nmarket forecasting of really-new products,” Journal of\\nMarketing, 60, 47–60.\\n\\nPal Singh Toor, T. and Dhir, T. (2011), ”Benefits of inte-\\ngrated business planning, forecasting, and process man-\\nagement”, Business Strategy Series, Vol. 12 No. 6, pp.\\n275-288. https://doi.org/10.1108/17515631111185914\\n\\nA. Jain, V. Karthikeyan, S. B, S. BR, S. K and B. S, ”De-\\nmand Forecasting for E-Commerce Platforms,” 2020\\nIEEE International Conference for Innovation in Tech-\\nnology (INOCON), 2020, pp. 1-4, doi: 10.1109/IN-\\nOCON50539.2020.9298395.\\n\\nHong, S.P., Gang, Hoe-Il,\\nLee, Dong-Il. (2000).\\nMarket Forecasting for B2C B2B eCommerce. Elec-\\ntronics and Telecommunications Trends, 15(6), 103.\\nhttps://doi.org/10.22648/ETRI.2000.J.150612\\n\\nSong, Weixing, Wu, Jingjing, Kang, Jianshe and Zhang,\\nJun. ”Research on maintenance spare parts require-\\nment prediction based on LSTM recurrent neural net-\\nwork” Open Physics, vol. 19, no. 1, 2021, pp. 618-627.\\nhttps://doi.org/10.1515/phys-2021-0072\\n\\nSheu, D.D., Kuo, J.Y. A model for preventive maintenance\\noperations and forecasting. J Intell Manuf 17, 441–451\\n(2006). https://doi.org/10.1007/s10845-005-0017-6\\n\\nI. A. Salman, ”Forecasting models for maintenance work\\nload with seasonal components,” Annual Symposium\\nReliability and Maintainability, 2004 - RAMS, 2004, pp.\\n514-520\\n\\nJD Edwards EnterpriseOne Applications,\\n\\nOracle\\nhttps://docs.oracle.com/cd/E16582_\\n01/index.htm\\n\\nDynamics 365 Supply Chain Management, https://\\ndocs.microsoft.com/en-us/dynamics365/\\nsupply-chain/asset-management/\\nwork-orders/maintenance-forecasts\\n\\nMaximo for Aviation, https://www.ibm.com/\\ndocs/en/maximo-for-aviation/7.6.1?\\ntopic=SS5RRF_7.6.1/com.ibm.acm.doc/\\nplusamntfo/c_maint_forecast_appl.html\\n\\nDude\\n\\nIntelligence\\n\\nfor Maintenance,\\n\\nhttps:\\n\\n//help.dudesolutions.com/Content/\\nDocumentation/Dude%20Intelligence/\\nEducation/Maintenance-Forecast.htm\\n\\nPatr´ıcia Ramos, Jos´e Manuel Soares Oliveira, Paula\\nSilva Predictive maintenance of production equipment\\nbased on neural network autoregression and ARIMA,\\nhttps://core.ac.uk/download/pdf/143396566.pdf\\n\\nZhao, J., Xu, L. and Liu, L. (2007) “Equipment Fault\\nForecasting Based on ARMA Model”, Proceedings of\\nthe IEEE Conference on International Conference on\\nMechatronics and Automation, August 5 - 8, 2007,\\nHarbin, China.\\n\\nKhalid, W., Albrechtsen, S.H., Sigsgaard, K.V., Mortensen,\\nN.H., Hansen, K.B. and Soleymani, I. (2021), ”Predict-\\ning maintenance work hours in maintenance planning”,\\nJournal of Quality in Maintenance Engineering, Vol. 27\\nNo. 2, pp. 366-384, https://doi.org/10.1108/JQME-06-\\n2019-0058\\n\\nLiu, Yiqun (2006) A forecasting model for maintenance\\nand repair costs for office buildings. Masters thesis,\\nConcordia University.\\n\\nBarbu, Catalin Alexandru; Andreica, Marin; Popescu, Ion-\\nPetru, Modern Approaches for Maintenance Forecasting\\n\\n\\x0cAnupam Purwar, Matthew Reimherr\\n\\nManagement, Calitatea, suppl. Quality-Access to Suc-\\ncess: Acces la Success; Bucharest Vol. 19, Iss. S1, (Mar\\n2018): 53-58\\n\\nAl-Fares, H., Duffuaa, S. (2009). Maintenance Forecasting\\nand Capacity Planning. In: Ben-Daya, M., Duffuaa, S.,\\nRaouf, A., Knezevic, J., Ait-Kadi, D. (eds) Handbook of\\nMaintenance Management and Engineering. Springer,\\nLondon\\n\\nAnupam Purwar, Automated planning tool (APT): A\\nmixed integer non linear programming problem solver\\nfor workorder scheduling, Amazon Science, 2022\\n\\nSave Time and Money with Accurate Maintenance\\nhttps://blogs.sw.siemens.com/service-\\nForecasting,\\nlifecycle-management/2021/12/09\\n/save-time-and-\\nmoney-with-accurate-maintenance-forecasting\\n\\nAgostino Tarsitano, Ilaria L. Amerise, Short-term load\\nforecasting using a two-stage sarimax model, Energy,\\nVolume 133, 2017, Pages 108-114, ISSN 0360-5442,\\nhttps://doi.org/10.1016/j.energy.2017.05.126.\\n\\nNontapa, C., Kesamoon, C., Kaewhawong, N., Intrapai-\\nboon, P. (2020). A New Time Series Forecasting Us-\\ning Decomposition Method with SARIMAX Model.\\nIn: Yang, H., Pasupa, K., Leung, A.CS., Kwok, J.T.,\\nChan, J.H., King, I. (eds) Neural Information Pro-\\ncessing. ICONIP 2020. Communications in Computer\\nand Information Science, vol 1333. Springer, Cham.\\nhttps://doi.org/10.1007/978-3-030-63823-884\\n\\nS. I. Vagropoulos, G. I. Chouliaras, E. G. Kardakos, C.\\nK. Simoglou and A. G. Bakirtzis, ”Comparison of SARI-\\nMAX, SARIMA, modified SARIMA and ANN-based\\nmodels for short-term PV generation forecasting,” 2016\\nIEEE International Energy Conference (ENERGYCON),\\n2016, pp. 1-6, doi: 10.1109/ENERGYCON.2016.7514029.\\n\\nQian Cheng, Nilay Tanik Argon, Christopher Scott Evans,\\nYufeng Liu, Timothy F. Platts-Mills, Serhan Ziya, Fore-\\ncasting emergency department hourly occupancy using\\ntime series analysis, The American Journal of Emergency\\nMedicine, Volume 48, 2021, Pages 177-182, ISSN 0735-\\n6757, https://doi.org/10.1016/j.ajem.2021.04.075.\\n\\nStatsmodels Library, https://www.statsmodels.\\norg/devel/gettingstarted.html\\n\\n\\x0c', metadata={'source': 'source_documents/arimax-model-for-forecasting-maintenance-work-amfm-a-multi-stage-seasonal-arimax-model-for-work-order-time-series-forecasting.pdf'}), Document(page_content='Machine Learning \\n& Data Science \\nBlueprints  \\nfor Finance\\n\\nFrom Building Trading Strategies to  \\nRobo-Advisors Using Python\\n\\nHariom Tatsat, Sahil Puri \\n & Brad Lookabaugh\\n\\n\\x0c\\x0cMachine Learning and Data\\nScience Blueprints for Finance\\nFrom Building Trading Strategies to\\nRobo-Advisors Using Python\\n\\nHariom Tatsat, Sahil Puri, and Brad Lookabaugh\\n\\nBeijing\\nBeijing\\n\\nBoston\\nBoston\\n\\nFarnham Sebastopol\\nFarnham Sebastopol\\n\\nTokyo\\nTokyo\\n\\n\\x0cMachine Learning and Data Science Blueprints for Finance\\nby Hariom Tatsat, Sahil Puri, and Brad Lookabaugh\\n\\nCopyright © 2021 Hariom Tatsat, Sahil Puri, and Brad Lookabaugh. All rights reserved.\\n\\nPrinted in the United States of America.\\n\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\n\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso  available  for  most  titles  (http://oreilly.com).  For  more  information,  contact  our  corporate/institu‐\\ntional sales department: 800-998-9938 or corporate@oreilly.com.\\n\\nIndexer: WordCo Indexing Services, Inc.\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\n\\nAcquisitions Editor: Michelle Smith\\nDevelopment Editor: Jeff Bleiel\\nProduction Editor: Christopher Faucher\\nCopyeditor: Piper Editorial, LLC\\nProofreader: Arthur Johnson\\n\\nOctober 2020:\\n\\n First Edition\\n\\nRevision History for the First Edition\\n2020-09-29:  First Release\\n\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492073055 for release details.\\n\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Machine Learning and Data Science\\nBlueprints for Finance, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\n\\nThe  views  expressed  in  this  work  are  those  of  the  authors,  and  do  not  represent  the  publisher’s  views.\\nWhile  the  publisher  and  the  authors  have  used  good  faith  efforts  to  ensure  that  the  information  and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk.  If  any  code  samples  or  other  technology  this  work  contains  or  describes  is  subject  to  open  source\\nlicenses  or  the  intellectual  property  rights  of  others,  it  is  your  responsibility  to  ensure  that  your  use\\nthereof complies with such licenses and/or rights.\\n\\n978-1-492-07305-5\\n\\n[LSCH]\\n\\n\\x0cTable of Contents\\n\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   ix\\n\\nPart I. \\n\\nThe Framework\\n\\n1. Machine Learning in Finance: The Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   1\\nCurrent and Future Machine Learning Applications in Finance                              2\\nAlgorithmic Trading                                                                                                    2\\nPortfolio Management and Robo-Advisors                                                              2\\nFraud Detection                                                                                                            3\\nLoans/Credit Card/Insurance Underwriting                                                            3\\nAutomation and Chatbots                                                                                           3\\nRisk Management                                                                                                         4\\nAsset Price Prediction                                                                                                  4\\nDerivative Pricing                                                                                                         4\\nSentiment Analysis                                                                                                       5\\nTrade Settlement                                                                                                           5\\nMoney Laundering                                                                                                       5\\nMachine Learning, Deep Learning, Artificial Intelligence, and Data Science         5\\nMachine Learning Types                                                                                                 7\\nSupervised                                                                                                                      7\\nUnsupervised                                                                                                                 8\\nReinforcement Learning                                                                                              9\\nNatural Language Processing                                                                                       10\\nChapter Summary                                                                                                          11\\n\\niii\\n\\n\\x0c2. Developing a Machine Learning Model in Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n\\n 13\\nWhy Python?                                                                                                                   13\\nPython Packages for Machine Learning                                                                     14\\nPython and Package Installation                                                                              15\\nSteps for Model Development in Python Ecosystem                                                15\\nModel Development Blueprint                                                                                 16\\nChapter Summary                                                                                                          29\\n\\n3. Artificial Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   31\\nANNs: Architecture, Training, and Hyperparameters                                             32\\nArchitecture                                                                                                                 32\\nTraining                                                                                                                        34\\nHyperparameters                                                                                                        36\\nCreating an Artificial Neural Network Model in Python                                         40\\nInstalling Keras and Machine Learning Packages                                                 40\\nRunning an ANN Model Faster: GPU and Cloud Services                                  43\\nChapter Summary                                                                                                          45\\n\\nPart II. \\n\\nSupervised Learning\\n\\n4. Supervised Learning: Models and Concepts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n\\n 49\\nSupervised Learning Models: An Overview                                                               51\\nLinear Regression (Ordinary Least Squares)                                                          52\\nRegularized Regression                                                                                              55\\nLogistic Regression                                                                                                     57\\nSupport Vector Machine                                                                                           58\\nK-Nearest Neighbors                                                                                                  60\\nLinear Discriminant Analysis                                                                                   62\\nClassification and Regression Trees                                                                         63\\nEnsemble Models                                                                                                        65\\nANN-Based Models                                                                                                   71\\nModel Performance                                                                                                       73\\nOverfitting and Underfitting                                                                                    73\\nCross Validation                                                                                                         74\\nEvaluation Metrics                                                                                                      75\\nModel Selection                                                                                                              79\\nFactors for Model Selection                                                                                      79\\nModel Trade-off                                                                                                          81\\nChapter Summary                                                                                                          82\\n\\niv \\n\\n| \\n\\nTable of Contents\\n\\n\\x0c5. Supervised Learning: Regression (Including Time Series Models). . . . . . . . . . . . . . . . .   83\\nTime Series Models                                                                                                        86\\nTime Series Breakdown                                                                                             87\\nAutocorrelation and Stationarity                                                                             88\\nTraditional Time Series Models (Including the ARIMA Model)                        90\\nDeep Learning Approach to Time Series Modeling                                              92\\nModifying Time Series Data for Supervised Learning Models                            95\\nCase Study 1: Stock Price Prediction                                                                           95\\nBlueprint for Using Supervised Learning Models to Predict a Stock Price       97\\nCase Study 2: Derivative Pricing                                                                                114\\n\\nBlueprint for Developing a Machine Learning Model for Derivative\\n\\nPricing                                                                                                                     115\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors                                   125\\n\\nBlueprint for Modeling Investor Risk Tolerance and Enabling a Machine\\n\\nLearning–Based Robo-Advisor                                                                           127\\nCase Study 4: Yield Curve Prediction                                                                       141\\n\\nBlueprint for Using Supervised Learning Models to Predict the Yield\\n\\nCurve                                                                                                                       142\\nChapter Summary                                                                                                        149\\nExercises                                                                                                                        150\\n\\n6. Supervised Learning: Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   151\\nCase Study 1: Fraud Detection                                                                                   153\\n\\nBlueprint for Using Classification Models to Determine Whether a\\n\\nTransaction Is Fraudulent                                                                                   153\\nCase Study 2: Loan Default Probability                                                                    166\\n\\nBlueprint for Creating a Machine Learning Model for Predicting Loan\\n\\nDefault Probability                                                                                                167\\nCase Study 3: Bitcoin Trading Strategy                                                                     179\\n\\nBlueprint for Using Classification-Based Models to Predict Whether to\\n\\nBuy or Sell in the Bitcoin Market                                                                       180\\nChapter Summary                                                                                                        190\\nExercises                                                                                                                        191\\n\\nPart III.  Unsupervised Learning\\n\\n7. Unsupervised Learning: Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . .   195\\nDimensionality Reduction Techniques                                                                    197\\nPrincipal Component Analysis                                                                               198\\nKernel Principal Component Analysis                                                                  201\\n\\nTable of Contents \\n\\n| \\n\\nv\\n\\n\\x0ct-distributed Stochastic Neighbor Embedding                                                    202\\nCase Study 1: Portfolio Management: Finding an Eigen Portfolio                       202\\nBlueprint for Using Dimensionality Reduction for Asset Allocation               203\\nCase Study 2: Yield Curve Construction and Interest Rate Modeling                 217\\nBlueprint for Using Dimensionality Reduction to Generate a Yield Curve    218\\nCase Study 3: Bitcoin Trading: Enhancing Speed and Accuracy                          227\\n\\nBlueprint for Using Dimensionality Reduction to Enhance a Trading\\n\\nStrategy                                                                                                                   228\\nChapter Summary                                                                                                        236\\nExercises                                                                                                                        236\\n\\n8. Unsupervised Learning: Clustering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  237\\nClustering Techniques                                                                                                 239\\nk-means Clustering                                                                                                  239\\nHierarchical Clustering                                                                                            240\\nAffinity Propagation Clustering                                                                             242\\nCase Study 1: Clustering for Pairs Trading                                                              243\\nBlueprint for Using Clustering to Select Pairs                                                     244\\nCase Study 2: Portfolio Management: Clustering Investors                                  259\\nBlueprint for Using Clustering for Grouping Investors                                     260\\nCase Study 3: Hierarchical Risk Parity                                                                      267\\nBlueprint for Using Clustering to Implement Hierarchical Risk Parity           268\\nChapter Summary                                                                                                        277\\nExercises                                                                                                                        277\\n\\nPart IV.  Reinforcement Learning and Natural Language Processing\\n\\n9. Reinforcement Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n\\n 281\\nReinforcement Learning—Theory and Concepts                                                   283\\nRL Components                                                                                                        284\\nRL Modeling Framework                                                                                        288\\nReinforcement Learning Models                                                                            293\\nKey Challenges in Reinforcement Learning                                                         298\\nCase Study 1: Reinforcement Learning–Based Trading Strategy                          298\\nBlueprint for Creating a Reinforcement Learning–Based Trading Strategy   300\\nCase Study 2: Derivatives Hedging                                                                            316\\n\\nBlueprint for Implementing a Reinforcement Learning–Based Hedging\\n\\nStrategy                                                                                                                   317\\nCase Study 3: Portfolio Allocation                                                                             334\\n\\nvi \\n\\n| \\n\\nTable of Contents\\n\\n\\x0cBlueprint for Creating a Reinforcement Learning–Based Algorithm for\\n\\nPortfolio Allocation                                                                                              335\\nChapter Summary                                                                                                        344\\nExercises                                                                                                                        345\\n\\n10. Natural Language Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   347\\nNatural Language Processing: Python Packages                                                     349\\nNLTK                                                                                                                          349\\nTextBlob                                                                                                                     349\\nspaCy                                                                                                                          350\\nNatural Language Processing: Theory and Concepts                                             350\\n1. Preprocessing                                                                                                        351\\n2. Feature Representation                                                                                        356\\n3. Inference                                                                                                                360\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies               362\\nBlueprint for Building a Trading Strategy Based on Sentiment Analysis        363\\nCase Study 2: Chatbot Digital Assistant                                                                   383\\nBlueprint for Creating a Custom Chatbot Using NLP                                        385\\nCase Study 3: Document Summarization                                                                 393\\nBlueprint for Using NLP for Document Summarization                                   394\\nChapter Summary                                                                                                        400\\nExercises                                                                                                                        400\\n\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   401\\n\\nTable of Contents \\n\\n| \\n\\nvii\\n\\n\\x0c\\x0cPreface\\n\\nThe value of machine learning (ML) in finance is becoming more apparent each day.\\nMachine learning is expected to become crucial to the functioning of financial mar‐\\nkets. Analysts, portfolio managers, traders, and chief investment officers should all be\\nfamiliar  with  ML  techniques.  For  banks  and  other  financial  institutions  striving  to\\nimprove financial analysis, streamline processes, and increase security, ML is becom‐\\ning the technology of choice. The use of ML in institutions is an increasing trend, and\\nits  potential  for  improving  various  systems  can  be  observed  in  trading  strategies,\\npricing, and risk management.\\n\\nAlthough  machine  learning  is  making  significant  inroads  across  all  verticals  of  the\\nfinancial services industry, there is a gap between the ideas and the implementation\\nof  machine  learning  algorithms.  A  plethora  of  material  is  available  on  the  web  in\\nthese areas, yet very little is organized. Additionally, most of the literature is limited\\nto trading algorithms only. Machine Learning and Data Science Blueprints for Finance\\nfills  this  void  and  provides  a  machine  learning  toolbox  customized  for  the  financal\\nmarket  that  allows  the  readers  to  be  part  of  the  machine  learning  revolution.  This\\nbook is not limited to investing or trading strategies; it focuses on leveraging the art\\nand craft of building ML-driven algorithms that are crucial in the finance industry.\\n\\nImplementing machine learning models in finance is easier than commonly believed.\\nThere is also a misconception that big data is needed for building machine learning\\nmodels. The case studies in this book span almost all areas of machine learning and\\naim to handle such misconceptions. This book not only will cover the theory and case\\nstudies  related  to  using  ML  in  trading  strategies  but  also  will  delve  deep  into  other\\ncritical  “need-to-know”  concepts  such  as  portfolio  management,  derivative  pricing,\\nfraud  detection,  corporate  credit  ratings,  robo-advisor  development,  and  chatbot\\ndevelopment. It will address real-life problems faced by practitioners and provide sci‐\\nentifically sound solutions supported by code and examples.\\n\\nPreface \\n\\n| \\n\\nix\\n\\n\\x0cThe Python codebase for this book on GitHub will be useful and serve as a starting\\npoint  for  industry  practitioners  working  on  their  projects.  The  examples  and  case\\nstudies  shown  in  the  book  demonstrate  techniques  that  can  easily  be  applied  to  a\\nwide range of datasets. The futuristic case studies, such as reinforcement learning for\\ntrading, building a robo-advisor, and using machine learning for instrument pricing,\\ninspire readers to think outside the box and motivate them to make the best of the\\nmodels and data available.\\n\\nWho This Book Is For\\nThe format of the book and the list of topics covered make it suitable for professio‐\\nnals  working  in  hedge  funds,  investment  and  retail  banks,  and  fintech  firms.  They\\nmay have titles such as data scientist, data engineer, quantitative researcher, machine\\nlearning  architect,  or  software  engineer.  Additionally,  the  book  will  be  useful  for\\nthose professionals working in support functions, such as compliance and risk.\\n\\nWhether a quantitative trader in a hedge fund is looking for ideas in using reinforce‐\\nment learning for trading cryptocurrency or an investment bank quant is looking for\\nmachine learning–based techniques to improve the calibration speed of pricing mod‐\\nels,  this  book  will  add  value.  The  theory,  concepts,  and  codebase  mentioned  in  the\\nbook will be extremely useful at every step of the model development lifecycle, from\\nidea generation to model implementation. Readers can use the shared codebase and\\ntest  the  proposed  solutions  themselves,  allowing  for  a  hands-on  reader  experience.\\nThe  readers  should  have  a  basic  knowledge  of  statistics,  machine  learning,  and\\nPython.\\n\\nHow This Book Is Organized\\nThis book provides a comprehensive introduction to how machine learning and data\\nscience can be used to design models across different areas in finance. It is organized\\ninto four parts.\\n\\nPart I: The Framework\\nThe first part provides an overview of machine learning in finance and the building\\nblocks of machine learning implementation. These chapters serve as the foundation\\nfor the case studies covering different machine learning types presented in the rest of\\nthe book.\\n\\nx \\n\\n|  Preface\\n\\n\\x0cThe chapters under the first part are as follows:\\n\\nChapter 1, Machine Learning in Finance: The Landscape\\n\\nThis chapter provides an overview of applications of machine learning in finance\\nand provides a brief overview of several types of machine learning.\\n\\nChapter 2, Developing a Machine Learning Model in Python\\n\\nThis chapter looks at the Python-based ecosystem for machine learning. It also\\ncover  the  steps  for  machine  learning  model  development  in  the  Python  frame‐\\nwork.\\n\\nChapter 3, Artificial Neural Networks\\n\\nGiven that an artificial neural network (ANN) is a primary algorithm used across\\nall types of machine learning, this chapter looks at the details of ANNs, followed\\nby a detailed implementation of an ANN model using Python libraries.\\n\\nPart II: Supervised Learning\\nThe  second  part  covers  fundamental  supervised  learning  algorithms  and  illustrates\\nspecific applications and case studies.\\n\\nThe chapters under the second part are as follows:\\n\\nChapter 4, Supervised Learning: Models and Concepts\\n\\nThis  chapter  provides  an  introduction  to  supervised  learning  techniques  (both\\nclassification  and  regression).  Given  that  a  lot  of  models  are  common  between\\nclassification  and  regression,  the  details  of  those  models  are  presented  together\\nalong  with  other  concepts  such  as  model  selection  and  evaluation  metrics  for\\nclassification and regression.\\n\\nChapter 5, Supervised Learning: Regression (Including Time Series Models)\\n\\nSupervised  learning-based  regression  models  are  the  most  commonly  used\\nmachine learning models in finance. This chapter covers the models from basic\\nlinear regression to advance deep learning. The case studies covered in this sec‐\\ntion include models for stock price prediction, derivatives pricing, and portfolio\\nmanagement.\\n\\nChapter 6, Supervised Learning: Classification\\n\\nClassification is a subcategory of supervised learning in which the goal is to pre‐\\ndict the categorical class labels of new instances, based on past observations. This\\nsection  discusses  several  case  studies  based  on  classification–based  techniques,\\nsuch as logistic regression, support vector machines, and random forests.\\n\\nPreface \\n\\n| \\n\\nxi\\n\\n\\x0cPart III: Unsupervised Learning\\nThe  third  part  covers  the  fundamental  unsupervised  learning  algorithms  and  offers\\napplications and case studies.\\n\\nThe chapters under the third part are as follows:\\n\\nChapter 7, Unsupervised Learning: Dimensionality Reduction\\n\\nThis chapter describes the essential techniques to reduce the number of features\\nin a dataset while retaining most of their useful and discriminatory information.\\nIt also discusses the standard approach to dimensionality reduction via principal\\ncomponent  analysis  and  covers  case  studies  in  portfolio  management,  trading\\nstrategy, and yield curve construction.\\n\\nChapter 8, Unsupervised Learning: Clustering\\n\\nThis chapter covers the algorithms and techniques related to clustering and iden‐\\ntifying groups of objects that share a degree of similarity. The case studies utiliz‐\\ning clustering in trading strategies and portfolio management are covered in this\\nchapter.\\n\\nPart IV: Reinforcement Learning and Natural Language Processing\\nThe  fourth  part  covers  the  reinforcement  learning  and  natural  language  processing\\n(NLP) techniques.\\n\\nThe chapters under the fourth part are as follows:\\n\\nChapter 9, Reinforcement Learning\\n\\nThis chapter covers concepts and case studies on reinforcement learning, which\\nhave great potential for application in the finance industry. Reinforcement learn‐\\ning’s main idea of “maximizing the rewards” aligns perfectly with the core moti‐\\nvation of several areas within finance. Case studies related to trading strategies,\\nportfolio optimization, and derivatives hedging are covered in this chapter.\\n\\nChapter 10, Natural Language Processing\\n\\nThis  chapter  describes  the  techniques  in  natural  language  processing  and  dis‐\\ncusses  the  essential  steps  to  transform  textual  data  into  meaningful  representa‐\\ntions  across  several  areas  in  finance.  Case  studies  related  to  sentiment  analysis,\\nchatbots, and document interpretation are covered.\\n\\nxii \\n\\n|  Preface\\n\\n\\x0cConventions Used in This Book\\nThe following typographical conventions are used in this book:\\n\\nItalic\\n\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\n\\nConstant width\\n\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments  such  as  variable  or  function  names,  databases,  data  types,  environment\\nvariables, statements, and keywords.\\n\\nThis element signifies a tip or suggestion.\\n\\nThis element signifies a general note.\\n\\nThis element indicates a warning or caution.\\n\\nThis element indicates a blueprint.\\n\\nUsing Code Presented in the Book\\nAll  code  in  this  book  (case  studies  and  master  template)  is  available  at  the  GitHub\\ndirectory: https://github.com/tatsath/fin-ml. The code is hosted on a cloud platform,\\nso  every  case  study  can  be  run  without  installing  a  package  on  a  local  machine  by\\nclicking https://mybinder.org/v2/gh/tatsath/fin-ml/master.\\n\\nThis book is here to help you get your job done. In general, if example code is offered,\\nyou may use it in your programs and documentation. You do not need to contact us\\nfor permission unless you’re reproducing a significant portion of the code. For exam‐\\nple,  writing  a  program  that  uses  several  chunks  of  code  from  this  book  does  not\\n\\nPreface \\n\\n| \\n\\nxiii\\n\\n\\x0crequire permission. Selling or distributing examples from O’Reilly books does require\\npermission. Answering a question by citing this book and quoting example code does\\nnot  require  permission.  Incorporating  a  significant  amount  of  example  code  from\\nthis book into your product’s documentation does require permission.\\n\\nWe  appreciate,  but  generally  do  not  require,  attribution.  An  attribution  usually\\nincludes the title, author, publisher, and ISBN. For example: Machine Learning and\\nData  Science  Blueprints  for  Finance  by  Hariom  Tatsat,  Sahil  Puri,  and  Brad  Looka‐\\nbaugh (O’Reilly, 2021), 978-1-492-07305-5.\\n\\nIf  you  feel  your  use  of  code  examples  falls  outside  fair  use  or  the  permission  given\\nabove, feel free to contact us at permissions@oreilly.com.\\n\\nPython Libraries\\nThe book uses Python 3.7. Installing the Conda package manager is recommended in\\norder  to  create  a  Conda  environment  to  install  the  requisite  libraries.  Installation\\ninstructions are available on the GitHub repo’s README file.\\n\\nO’Reilly Online Learning\\n\\nFor more than 40 years, O’Reilly Media has provided technol‐\\nogy  and  business  training,  knowledge,  and  insight  to  help\\ncompanies succeed.\\n\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform  gives  you  on-demand  access  to  live  training  courses,  in-depth  learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\\n\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\n\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\n\\nxiv \\n\\n|  Preface\\n\\n\\x0cWe have a web page for this book, where we list errata, examples, and any additional\\ninformation.  You  can  access  this  page  at  https://oreil.ly/ML-and-data-science-\\nblueprints.\\n\\nEmail  bookquestions@oreilly.com  to  comment  or  ask  technical  questions  about  this\\nbook.\\n\\nFor news and information about our books and courses, visit http://oreilly.com.\\n\\nFind us on Facebook: http://facebook.com/oreilly\\n\\nFollow us on Twitter: http://twitter.com/oreillymedia\\n\\nWatch us on YouTube: http://youtube.com/oreillymedia\\n\\nAcknowledgments\\nWe want to thank all those who helped to make this book a reality. Special thanks to\\nJeff Bleiel for honest, insightful feedback, and for guiding us through the entire pro‐\\ncess.  We  are  incredibly  grateful  to  Juan  Manuel  Contreras,  Chakri  Cherukuri,  and\\nGregory  Bronner,  who  took  time  out  of  their  busy  lives  to  review  our  book  in  so\\nmuch detail. The book benefited from their valuable feedback and suggestions. Many\\nthanks as well to O’Reilly’s fantastic staff, in particular Michelle Smith for believing in\\nthis project and helping us define its scope.\\n\\nSpecial Thanks from Hariom\\nI would like to thank my wife, Prachi, and my parents for their love and support. Spe‐\\ncial thanks to my father for encouraging me in all of my pursuits and being a contin‐\\nuous source of inspiration.\\n\\nSpecial Thanks from Sahil\\nThanks to my family, who always encouraged and supported me in all endeavors.\\n\\nSpecial Thanks from Brad\\nThank you to my wife, Megan, for her endless love and support.\\n\\nPreface \\n\\n| \\n\\nxv\\n\\n\\x0c\\x0cPART I\\nThe Framework\\n\\n\\x0c\\x0cCHAPTER 1\\nMachine Learning in Finance:\\nThe Landscape\\n\\nMachine learning promises to shake up large swathes of finance\\n\\n—The Economist (2017)\\n\\nThere is a new wave of machine learning and data science in finance, and the related\\napplications will transform the industry over the next few decades.\\n\\nCurrently, most financial firms, including hedge funds, investment and retail banks,\\nand fintech firms, are adopting and investing heavily in machine learning. Going for‐\\nward, financial institutions will need a growing number of machine learning and data\\nscience experts.\\n\\nMachine learning in finance has become more prominent recently due to the availa‐\\nbility of vast amounts of data and more affordable computing power. The use of data\\nscience and machine learning is exploding exponentially across all areas of finance.\\n\\nThe  success  of  machine  learning  in  finance  depends  upon  building  efficient  infra‐\\nstructure, using the correct toolkit, and applying the right algorithms. The concepts\\nrelated to these building blocks of machine learning in finance are demonstrated and\\nutilized throughout this book.\\n\\nIn this chapter, we provide an introduction to the current and future application of\\nmachine learning in finance, including a brief overview of different types of machine\\nlearning.  This  chapter  and  the  two  that  follow  serve  as  the  foundation  for  the  case\\nstudies presented in the rest of the book.\\n\\n1\\n\\n\\x0cCurrent and Future Machine Learning Applications\\nin Finance\\nLet’s  take  a  look  at  some  promising  machine  learning  applications  in  finance.  The\\ncase studies presented in this book cover all the applications mentioned here.\\n\\nAlgorithmic Trading\\nAlgorithmic trading (or simply algo trading) is the use of algorithms to conduct trades\\nautonomously. With origins going back to the 1970s, algorithmic trading (sometimes\\ncalled  Automated  Trading  Systems,  which  is  arguably  a  more  accurate  description)\\ninvolves  the  use  of  automated  preprogrammed  trading  instructions  to  make\\nextremely fast, objective trading decisions.\\n\\nMachine  learning  stands  to  push  algorithmic  trading  to  new  levels.  Not  only  can\\nmore advanced strategies be employed and adapted in real time, but machine learn‐\\ning–based  techniques  can  offer  even  more  avenues  for  gaining  special  insight  into\\nmarket movements. Most hedge funds and financial institutions do not openly dis‐\\nclose  their  machine  learning–based  approaches  to  trading  (for  good  reason),  but\\nmachine  learning  is  playing  an  increasingly  important  role  in  calibrating  trading\\ndecisions in real time.\\n\\nPortfolio Management and Robo-Advisors\\nAsset and wealth management firms are exploring potential artificial intelligence (AI)\\nsolutions for improving their investment decisions and making use of their troves of\\nhistorical data.\\n\\nOne example of this is the use of robo-advisors, algorithms built to calibrate a finan‐\\ncial  portfolio  to  the  goals  and  risk  tolerance  of  the  user.  Additionally,  they  provide\\nautomated financial guidance and service to end investors and clients.\\n\\nA user enters their financial goals (e.g., to retire at age 65 with $250,000 in savings),\\nage,  income,  and  current  financial  assets.  The  advisor  (the  allocator)  then  spreads\\ninvestments across asset classes and financial instruments in order to reach the user’s\\ngoals.\\n\\nThe system then calibrates to changes in the user’s goals and real-time changes in the\\nmarket, aiming always to find the best fit for the user’s original goals. Robo-advisors\\nhave gained significant traction among consumers who do not need a human advisor\\nto feel comfortable investing.\\n\\n2 \\n\\n| \\n\\nChapter 1: Machine Learning in Finance: The Landscape\\n\\n\\x0cFraud Detection\\nFraud is a massive problem for financial institutions and one of the foremost reasons\\nto leverage machine learning in finance.\\n\\nThere is currently a significant data security risk due to high computing power, fre‐\\nquent internet use, and an increasing amount of company data being stored online.\\nWhile previous financial fraud detection systems depended heavily on complex and\\nrobust sets of rules, modern fraud detection goes beyond following a checklist of risk\\nfactors—it actively learns and calibrates to new potential (or real) security threats.\\n\\nMachine  learning  is  ideally  suited  to  combating  fraudulent  financial  transactions.\\nThis  is  because  machine  learning  systems  can  scan  through  vast  datasets,  detect\\nunusual  activities,  and  flag  them  instantly.  Given  the  incalculably  high  number  of\\nways  that  security  can  be  breached,  genuine  machine  learning  systems  will  be  an\\nabsolute necessity in the days to come.\\n\\nLoans/Credit Card/Insurance Underwriting\\nUnderwriting could be described as a perfect job for machine learning in finance, and\\nindeed there is a great deal of worry in the industry that machines will replace a large\\nswath of underwriting positions that exist today.\\n\\nEspecially  at  large  companies  (big  banks  and  publicly  traded  insurance  firms),\\nmachine  learning  algorithms  can  be  trained  on  millions  of  examples  of  consumer\\ndata and financial lending or insurance outcomes, such as whether a person defaulted\\non their loan or mortgage.\\n\\nUnderlying  financial  trends  can  be  assessed  with  algorithms  and  continuously  ana‐\\nlyzed  to  detect  trends  that  might  influence  lending  and  underwriting  risk  in  the\\nfuture.  Algorithms  can  perform  automated  tasks  such  as  matching  data  records,\\nidentifying exceptions, and calculating whether an applicant qualifies for a credit or\\ninsurance product.\\n\\nAutomation and Chatbots\\nAutomation  is  patently  well  suited  to  finance.  It  reduces  the  strain  that  repetitive,\\nlow-value tasks put on human employees. It tackles the routine, everyday processes,\\nfreeing up teams to finish their high-value work. In doing so, it drives enormous time\\nand cost savings.\\n\\nAdding machine learning and AI into the automation mix adds another level of sup‐\\nport for employees. With access to relevant data, machine learning and AI can pro‐\\nvide  an  in-depth  data  analysis  to  support  finance  teams  with  difficult  decisions.  In\\nsome cases, it may even be able to recommend the best course of action for employ‐\\nees to approve and enact.\\n\\nCurrent and Future Machine Learning Applications in Finance \\n\\n| \\n\\n3\\n\\n\\x0cAI and automation in the financial sector can also learn to recognize errors, reducing\\nthe  time  wasted  between  discovery  and  resolution.  This  means  that  human  team\\nmembers are less likely to be delayed in providing their reports and are able to com‐\\nplete their work with fewer errors.\\n\\nAI chatbots can be implemented to support finance and banking customers. With the\\nrise in popularity of live chat software in banking and finance businesses, chatbots are\\nthe natural evolution.\\n\\nRisk Management\\nMachine  learning  techniques  are  transforming  how  we  approach  risk  management.\\nAll  aspects  of  understanding  and  controlling  risk  are  being  revolutionized  through\\nthe growth of solutions driven by machine learning. Examples range from deciding\\nhow  much  a  bank  should  lend  a  customer  to  improving  compliance  and  reducing\\nmodel risk.\\n\\nAsset Price Prediction\\nAsset price prediction is considered the most frequently discussed and most sophisti‐\\ncated area in finance. Predicting asset prices allows one to understand the factors that\\ndrive  the  market  and  speculate  asset  performance.  Traditionally,  asset  price  predic‐\\ntion  was  performed  by  analyzing  past  financial  reports  and  market  performance  to\\ndetermine what position to take for a specific security or asset class. However, with\\na tremendous increase in the amount of financial data, the traditional approaches for\\nanalysis  and  stock-selection  strategies  are  being  supplemented  with  ML-based\\ntechniques.\\n\\nDerivative Pricing\\nRecent  machine  learning  successes,  as  well  as  the  fast  pace  of  innovation,  indicate\\nthat ML applications for derivatives pricing should become widely used in the com‐\\ning years. The world of Black-Scholes models, volatility smiles, and Excel spreadsheet\\nmodels should wane as more advanced methods become readily available.\\n\\nThe classic derivative pricing models are built on several impractical assumptions to\\nreproduce the empirical relationship between the underlying input data (strike price,\\ntime to maturity, option type) and the price of the derivatives observed in the market.\\nMachine learning methods do not rely on several assumptions; they just try to esti‐\\nmate a function between the input data and price, minimizing the difference between\\nthe results of the model and the target.\\n\\nThe faster deployment times achieved with state-of-the-art ML tools are just one of\\nthe advantages that will accelerate the use of machine learning in derivatives pricing.\\n\\n4 \\n\\n| \\n\\nChapter 1: Machine Learning in Finance: The Landscape\\n\\n\\x0cSentiment Analysis\\nSentiment  analysis  involves  the  perusal  of  enormous  volumes  of  unstructured  data,\\nsuch  as  videos,  transcriptions,  photos,  audio  files,  social  media  posts,  articles,  and\\nbusiness documents, to determine market sentiment. Sentiment analysis is crucial for\\nall businesses in today’s workplace and is an excellent example of machine learning in\\nfinance.\\n\\nThe most common use of sentiment analysis in the financial sector is the analysis of\\nfinancial  news—in  particular,  predicting  the  behaviors  and  possible  trends  of  mar‐\\nkets. The stock market moves in response to myriad human-related factors, and the\\nhope is that machine learning will be able to replicate and enhance human intuition\\nabout financial activity by discovering new trends and telling signals.\\n\\nHowever, much of the future applications of machine learning will be in understand‐\\ning social media, news trends, and other data sources related to predicting the senti‐\\nments of customers toward market developments. It will not be limited to predicting\\nstock prices and trades.\\n\\nTrade Settlement\\nTrade settlement is the process of transferring securities into the account of a buyer\\nand cash into the seller’s account following a transaction of a financial asset.\\n\\nDespite the majority of trades being settled automatically, and with little or no inter‐\\naction by human beings, about 30% of trades need to be settled manually.\\n\\nThe use of machine learning not only can identify the reason for failed trades, but it\\nalso can analyze why the trades were rejected, provide a solution, and predict which\\ntrades  may  fail  in  the  future.  What  usually  would  take  a  human  being  five  to  ten\\nminutes to fix, machine learning can do in a fraction of a second.\\n\\nMoney Laundering\\nA United Nations report estimates that the amount of money laundered worldwide\\nper year is 2%–5% of global GDP. Machine learning techniques can analyze internal,\\npublicly  existing,  and  transactional  data  from  a  client’s  broader  network  in  an\\nattempt to spot money laundering signs.\\n\\nMachine Learning, Deep Learning, Artificial Intelligence,\\nand Data Science\\nFor the majority of people, the terms machine learning, deep learning, artificial intelli‐\\ngence,  and  data  science  are  confusing.  In  fact,  a  lot  of  people  use  one  term  inter‐\\nchangeably with the others.\\n\\nMachine Learning, Deep Learning, Artificial Intelligence, and Data Science \\n\\n| \\n\\n5\\n\\n\\x0cFigure 1-1 shows the relationships between AI, machine learning, deep learning and\\ndata  science.  Machine  learning  is  a  subset  of  AI  that  consists  of  techniques  that\\nenable  computers  to  identify  patterns  in  data  and  to  deliver  AI  applications.  Deep\\nlearning, meanwhile, is a subset of machine learning that enables computers to solve\\nmore complex problems.\\n\\nData science isn’t exactly a subset of machine learning, but it uses machine learning,\\ndeep learning, and AI to analyze data and reach actionable conclusions. It combines\\nmachine learning, deep learning and AI with other disciplines such as big data ana‐\\nlytics and cloud computing.\\n\\nFigure 1-1. AI, machine learning, deep learning, and data science\\n\\nThe following is a summary of the details about artificial intelligence, machine learn‐\\ning, deep learning, and data science:\\n\\nArtificial intelligence\\n\\nArtificial intelligence is the field of study by which a computer (and its systems)\\ndevelop the ability to successfully accomplish complex tasks that usually require\\nhuman intelligence. These tasks include, but are not limited to, visual perception,\\nspeech  recognition,  decision  making,  and  translation  between  languages.  AI  is\\nusually defined as the science of making computers do things that require intelli‐\\ngence when done by humans.\\n\\nMachine learning\\n\\nMachine learning is an application of artificial intelligence that provides the AI\\nsystem  with  the  ability  to  automatically  learn  from  the  environment  and  apply\\nthose  lessons  to  make  better  decisions.  There  are  a  variety  of  algorithms  that\\n\\n6 \\n\\n| \\n\\nChapter 1: Machine Learning in Finance: The Landscape\\n\\n\\x0cmachine learning uses to iteratively learn, describe and improve data, spot pat‐\\nterns, and then perform actions on these patterns.\\n\\nDeep learning\\n\\nDeep  learning  is  a  subset  of  machine  learning  that  involves  the  study  of  algo‐\\nrithms related to artificial neural networks that contain many blocks (or layers)\\nstacked on each other. The design of deep learning models is inspired by the bio‐\\nlogical neural network of the human brain. It strives to analyze data with a logical\\nstructure similar to how a human draws conclusions.\\n\\nData science\\n\\nData science is an interdisciplinary field similar to data mining that uses scien‐\\ntific methods, processes, and systems to extract knowledge or insights from data\\nin various forms, either structured or unstructured. Data science is different from\\nML and AI because its goal is to gain insight into and understanding of the data\\nby  using  different  scientific  tools  and  techniques.  However,  there  are  several\\ntools and techniques common to both ML and data science, some of which are\\ndemonstrated in this book.\\n\\nMachine Learning Types\\nThis section will outline all types of machine learning that are used in different case\\nstudies  presented  in  this  book  for  various  financial  applications.  The  three  types  of\\nmachine  learning,  as  shown  in  Figure  1-2,  are  supervised  learning,  unsupervised\\nlearning, and reinforcement learning.\\n\\nFigure 1-2. Machine learning types\\n\\nSupervised\\nThe main goal in supervised learning is to train a model from labeled data that allows\\nus to make predictions about unseen or future data. Here, the term supervised refers\\nto a set of samples where the desired output signals (labels) are already known. There\\nare two types of supervised learning algorithms: classification and regression.\\n\\nMachine Learning Types \\n\\n| \\n\\n7\\n\\n\\x0cClassification\\n\\nClassification is a subcategory of supervised learning in which the goal is to predict\\nthe categorical class labels of new instances based on past observations.\\n\\nRegression\\n\\nRegression  is  another  subcategory  of  supervised  learning  used  in  the  prediction  of\\ncontinuous  outcomes.  In  regression,  we  are  given  a  number  of  predictor  (explana‐\\ntory) variables and a continuous response variable (outcome or target), and we try to\\nfind a relationship between those variables that allows us to predict an outcome.\\n\\nAn example of regression versus classification is shown in Figure 1-3. The chart on\\nthe left shows an example of regression. The continuous response variable is return,\\nand the observed values are plotted against the predicted outcomes. On the right, the\\noutcome  is  a  categorical  class  label,  whether  the  market  is  bull  or  bear,  and  is  an\\nexample of classification.\\n\\nFigure 1-3. Regression versus classification\\n\\nUnsupervised\\nUnsupervised  learning  is  a  type  of  machine  learning  used  to  draw  inferences  from\\ndatasets  consisting  of  input  data  without  labeled  responses.  There  are  two  types  of\\nunsupervised learning: dimensionality reduction and clustering.\\n\\nDimensionality reduction\\n\\nDimensionality reduction is the process of reducing the number of features, or vari‐\\nables, in a dataset while preserving information and overall model performance. It is\\na  common  and  powerful  way  to  deal  with  datasets  that  have  a  large  number  of\\ndimensions.\\n\\n8 \\n\\n| \\n\\nChapter 1: Machine Learning in Finance: The Landscape\\n\\n\\x0cFigure 1-4 illustrates this concept, where the dimension of data is converted from two\\ndimensions  (X1  and  X2)  to  one  dimension  (Z1).  Z1  conveys  similar  information\\nembedded in X1 and X2 and reduces the dimension of the data.\\n\\nFigure 1-4. Dimensionality reduction\\n\\nClustering\\n\\nClustering is a subcategory of unsupervised learning techniques that allows us to dis‐\\ncover hidden structures in data. The goal of clustering is to find a natural grouping in\\ndata  so  that  items  in  the  same  cluster  are  more  similar  to  each  other  than  to  those\\nfrom different clusters.\\n\\nAn  example  of  clustering  is  shown  in  Figure  1-5,  where  we  can  see  the  entire  data\\nclustered into two distinct groups by the clustering algorithm.\\n\\nFigure 1-5. Clustering\\n\\nReinforcement Learning\\nLearning  from  experiences,  and  the  associated  rewards  or  punishments,  is  the  core\\nconcept  behind  reinforcement  learning  (RL).  It  is  about  taking  suitable  actions  to\\nmaximize reward in a particular situation. The learning system, called an agent, can\\nobserve the environment, select and perform actions, and receive rewards (or penal‐\\nties in the form of negative rewards) in return, as shown in Figure 1-6.\\n\\nReinforcement  learning  differs  from  supervised  learning  in  this  way:  In  supervised\\nlearning, the training data has the answer key, so the model is trained with the correct\\nanswers available. In reinforcement learning, there is no explicit answer. The learning\\n\\nMachine Learning Types \\n\\n| \\n\\n9\\n\\n\\x0csystem (agent) decides what to do to perform the given task and learns whether that\\nwas a correct action based on the reward. The algorithm determines the answer key\\nthrough its experience.\\n\\nFigure 1-6. Reinforcement learning\\n\\nThe steps of the reinforcement learning are as follows:\\n\\n1. First, the agent interacts with the environment by performing an action.\\n\\n2. Then the agent receives a reward based on the action it performed.\\n\\n3. Based on the reward, the agent receives an observation and understands whether\\nthe action was good or bad. If the action was good—that is, if the agent received a\\npositive reward—then the agent will prefer performing that action. If the reward\\nwas less favorable, the agent will try performing another action to receive a posi‐\\ntive reward. It is basically a trial-and-error learning process.\\n\\nNatural Language Processing\\nNatural language processing (NLP) is a branch of AI that deals with the problems of\\nmaking a machine understand the structure and the meaning of natural language as\\nused by humans. Several techniques of machine learning and deep learning are used\\nwithin NLP.\\n\\nNLP has many applications in the finance sectors in areas such as sentiment analysis,\\nchatbots,  and  document  processing.  A  lot  of  information,  such  as  sell  side  reports,\\nearnings calls, and newspaper headlines, is communicated via text message, making\\nNLP quite useful in the financial domain.\\n\\nGiven  the  extensive  application  of  NLP  algorithms  based  on  machine  learning  in\\nfinance, there is a separate chapter of this book (Chapter 10) dedicated to NLP and\\nrelated case studies.\\n\\n10 \\n\\n| \\n\\nChapter 1: Machine Learning in Finance: The Landscape\\n\\n\\x0cChapter Summary\\nMachine learning is making significant inroads across all the verticals of the financial\\nservices industry. This chapter covered different applications of machine learning in\\nfinance,  from  algorithmic  trading  to  robo-advisors.  These  applications  will  be  cov‐\\nered in the case studies later in this book.\\n\\nNext Steps\\nIn terms of the platforms used for machine learning, the Python ecosystem is grow‐\\ning and is one of the most dominant programming languages for machine learning.\\nIn  the  next  chapter,  we  will  learn  about  the  model  development  steps,  from  data\\npreparation to model deployment in a Python-based framework.\\n\\nChapter Summary \\n\\n| \\n\\n11\\n\\n\\x0c\\x0cCHAPTER 2\\nDeveloping a Machine Learning\\nModel in Python\\n\\nIn terms of the platforms used for machine learning, there are many algorithms and\\nprogramming  languages.  However,  the  Python  ecosystem  is  one  of  the  most  domi‐\\nnant and fastest-growing programming languages for machine learning.\\n\\nGiven  the  popularity  and  high  adoption  rate  of  Python,  we  will  use  it  as  the  main\\nprogramming language throughout the book. This chapter provides an overview of a\\nPython-based  machine  learning  framework.  First,  we  will  review  the  details  of\\nPython-based  packages  used  for  machine  learning,  followed  by  the  model  develop‐\\nment steps in the Python framework.\\n\\nThe  steps  of  model  development  in  Python  presented  in  this  chapter  serve  as  the\\nfoundation for the case studies presented in the rest of the book. The Python frame‐\\nwork can also be leveraged while developing any machine learning–based model in\\nfinance.\\n\\nWhy Python?\\nSome reasons for Python’s popularity are as follows:\\n\\n• High-level  syntax  (compared  to  lower-level  languages  of  C,  Java,  and  C++).\\nApplications  can  be  developed  by  writing  fewer  lines  of  code,  making  Python\\nattractive to beginners and advanced programmers alike.\\n\\n• Efficient development lifecycle.\\n\\n• Large collection of community-managed, open-source libraries.\\n\\n• Strong portability.\\n\\n13\\n\\n\\x0cThe  simplicity  of  Python  has  attracted  many  developers  to  create  new  libraries  for\\nmachine learning, leading to strong adoption of Python.\\n\\nPython Packages for Machine Learning\\nThe main Python packages used for machine learning are highlighted in Figure 2-1.\\n\\nFigure 2-1. Python packages\\n\\nHere is a brief summary of each of these packages:\\n\\nNumPy\\n\\nProvides support for large, multidimensional arrays as well as an extensive col‐\\nlection of mathematical functions.\\n\\nPandas\\n\\nA library for data manipulation and analysis. Among other features, it offers data\\nstructures to handle tables and the tools to manipulate them.\\n\\nMatplotlib\\n\\nA plotting library that allows the creation of 2D charts and plots.\\n\\nSciPy\\n\\nThe  combination  of  NumPy,  Pandas,  and  Matplotlib  is  generally  referred  to  as\\nSciPy.  SciPy  is  an  ecosystem  of  Python  libraries  for  mathematics,  science,  and\\nengineering.\\n\\nScikit-learn (or sklearn)\\n\\nA machine learning library offering a wide range of algorithms and utilities.\\n\\n14 \\n\\n| \\n\\nChapter 2: Developing a Machine Learning Model in Python\\n\\n\\x0cStatsModels\\n\\nA Python module that provides classes and functions for the estimation of many\\ndifferent statistical models, as well as for conducting statistical tests and statistical\\ndata exploration.\\n\\nTensorFlow and Theano\\n\\nDataflow programming libraries that facilitate working with neural networks.\\n\\nKeras\\n\\nAn  artificial  neural  network  library  that  can  act  as  a  simplified  interface  to\\nTensorFlow/Theano packages.\\n\\nSeaborn\\n\\nA data visualization library based on Matplotlib. It provides a high-level interface\\nfor drawing attractive and informative statistical graphics.\\n\\npip and Conda\\n\\nThese  are  Python  package  managers.  pip  is  a  package  manager  that  facilitates\\ninstallation, upgrade, and uninstallation of Python packages. Conda is a package\\nmanager that handles Python packages as well as library dependencies outside of\\nthe Python packages.\\n\\nPython and Package Installation\\nThere are different ways of installing Python. However, it is strongly recommended\\nthat  you  install  Python  through  Anaconda.  Anaconda  contains  Python,  SciPy,  and\\nScikit-learn.\\n\\nAfter  installing  Anaconda,  a  Jupyter  server  can  be  started  locally  by  opening  the\\nmachine’s terminal and typing in the following code:\\n\\n$jupyter notebook\\n\\nAll  code  samples  in  this  book  use  Python  3  and  are  presented  in\\nJupyter notebooks. Several Python packages, especially Scikit-learn\\nand Keras, are extensively used in the case studies.\\n\\nSteps for Model Development in Python Ecosystem\\nWorking through machine learning problems from end to end is critically important.\\nApplied machine learning will not come alive unless the steps from beginning to end\\nare well defined.\\n\\nFigure  2-2  provides  an  outline  of  the  simple  seven-step  machine  learning  project\\ntemplate that can be used to jump-start any machine learning model in Python. The\\n\\nSteps for Model Development in Python Ecosystem \\n\\n| \\n\\n15\\n\\n\\x0cfirst few steps include exploratory data analysis and data preparation, which are typi‐\\ncal  data  science–based  steps  aimed  at  extracting  meaning  and  insights  from  data.\\nThese steps are followed by model evaluation, fine-tuning, and finalizing the model.\\n\\nFigure 2-2. Model development steps\\n\\nAll  the  case  studies  in  this  book  follow  the  standard  seven-step\\nmodel development process. However, there are a few case studies\\nin  which  some  of  the  steps  are  skipped,  renamed,  or  reordered\\nbased on the appropriateness and intuitiveness of the steps.\\n\\nModel Development Blueprint\\nThe  following  section  covers  the  details  of  each  model  development  step  with  sup‐\\nporting Python code.\\n\\n1. Problem definition\\n\\nThe first step in any project is defining the problem. Powerful algorithms can be used\\nfor solving the problem, but the results will be meaningless if the wrong problem is\\nsolved.\\n\\nThe following framework should be used for defining the problem:\\n\\n1. Describe  the  problem  informally  and  formally.  List  assumptions  and  similar\\n\\nproblems.\\n\\n2. List the motivation for solving the problem, the benefits a solution provides, and\\n\\nhow the solution will be used.\\n\\n3. Describe how the problem would be solved using the domain knowledge.\\n\\n16 \\n\\n| \\n\\nChapter 2: Developing a Machine Learning Model in Python\\n\\n\\x0c2. Loading the data and packages\\n\\nThe second step gives you everything needed to start working on the problem. This\\nincludes  loading  libraries,  packages,  and  individual  functions  needed  for  the  model\\ndevelopment.\\n\\n2.1. Load libraries.    A sample code for loading libraries is as follows:\\n\\n# Load libraries\\nimport pandas as pd\\nfrom matplotlib import pyplot\\n\\nThe details of the libraries and modules for specific functionalities are defined further\\nin the individual case studies.\\n\\n2.2. Load data.     The  following  items  should  be  checked  and  removed  before  loading\\nthe data:\\n\\n• Column headers\\n\\n• Comments or special characters\\n\\n• Delimiter\\n\\nThere are many ways of loading data. Some of the most common ways are as follows:\\n\\nLoad CSV files with Pandas\\n\\nfrom pandas import read_csv\\nfilename = \\'xyz.csv\\'\\ndata = read_csv(filename, names=names)\\n\\nLoad file from URL\\n\\nfrom pandas import read_csv\\nurl = \\'https://goo.gl/vhm1eU\\'\\nnames = [\\'age\\', \\'class\\']\\ndata = read_csv(url, names=names)\\n\\nLoad file using pandas_datareader\\n\\nimport pandas_datareader.data as web\\n\\nccy_tickers = [\\'DEXJPUS\\', \\'DEXUSUK\\']\\nidx_tickers = [\\'SP500\\', \\'DJIA\\', \\'VIXCLS\\']\\n\\nstk_data = web.DataReader(stk_tickers, \\'yahoo\\')\\nccy_data = web.DataReader(ccy_tickers, \\'fred\\')\\nidx_data = web.DataReader(idx_tickers, \\'fred\\')\\n\\nSteps for Model Development in Python Ecosystem \\n\\n| \\n\\n17\\n\\n\\x0c3. Exploratory data analysis\\n\\nIn this step, we look at the dataset.\\n\\n3.1. Descriptive statistics.     Understanding  the  dataset  is  one  of  the  most  important\\nsteps of model development. The steps to understanding data include:\\n\\n1. Viewing the raw data.\\n\\n2. Reviewing the dimensions of the dataset.\\n\\n3. Reviewing the data types of attributes.\\n\\n4. Summarizing the distribution, descriptive statistics, and relationship among the\\n\\nvariables in the dataset.\\n\\nThese steps are demonstrated below using sample Python code:\\n\\nViewing the data\\n\\nset_option(\\'display.width\\', 100)\\ndataset.head(1)\\n\\nOutput\\n\\nAge\\n67\\n\\nSex\\nmale\\n\\n0\\n\\nJob Housing SavingAccounts\\n2\\n\\nown\\n\\nNaN\\n\\nCheckingAccount\\nlittle\\n\\nCreditAmount Duration Purpose Risk\\nradio/TV good\\n1169\\n\\n6\\n\\nReviewing the dimensions of the dataset\\n\\ndataset.shape\\n\\nOutput\\n\\n(284807, 31)\\n\\nThe results show the dimension of the dataset and mean that the dataset has 284,807\\nrows and 31 columns.\\n\\nReviewing the data types of the attributes in the data\\n\\n# types\\nset_option(\\'display.max_rows\\', 500)\\ndataset.dtypes\\n\\nSummarizing the data using descriptive statistics\\n\\n# describe data\\nset_option(\\'precision\\', 3)\\ndataset.describe()\\n\\n18 \\n\\n| \\n\\nChapter 2: Developing a Machine Learning Model in Python\\n\\n\\x0cOutput\\n\\nAge\\n1000.000\\n\\nJob\\n1000.000\\n\\nCreditAmount Duration\\n1000.000\\n1000.000\\n\\ncount\\n\\nmean 35.546\\n\\nstd\\n\\nmin\\n\\n11.375\\n\\n19.000\\n\\n25% 27.000\\n\\n50% 33.000\\n\\n75% 42.000\\n\\nmax\\n\\n75.000\\n\\n1.904\\n\\n0.654\\n\\n0.000\\n\\n2.000\\n\\n2.000\\n\\n2.000\\n\\n3.000\\n\\n3271.258\\n\\n2822.737\\n\\n250.000\\n\\n1365.500\\n\\n2319.500\\n\\n3972.250\\n\\n18424.000\\n\\n20.903\\n\\n12.059\\n\\n4.000\\n\\n12.000\\n\\n18.000\\n\\n24.000\\n\\n72.000\\n\\n3.2. Data visualization.    The fastest way to learn more about the data is to visualize it.\\nVisualization involves independently understanding each attribute of the dataset.\\n\\nSome of the plot types are as follows:\\n\\nUnivariate plots\\n\\nHistograms and density plots\\n\\nMultivariate plots\\n\\nCorrelation matrix plot and scatterplot\\n\\nThe Python code for univariate plot types is illustrated with examples below:\\n\\nUnivariate plot: histogram\\n\\nfrom matplotlib import pyplot\\ndataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1,\\\\\\nfigsize=(10,4))\\npyplot.show()\\n\\nUnivariate plot: density plot\\n\\nfrom matplotlib import pyplot\\ndataset.plot(kind=\\'density\\', subplots=True, layout=(3,3), sharex=False,\\\\\\nlegend=True, fontsize=1, figsize=(10,4))\\npyplot.show()\\n\\nSteps for Model Development in Python Ecosystem \\n\\n| \\n\\n19\\n\\n\\x0cFigure 2-3 illustrates the output.\\n\\nFigure 2-3. Histogram (top) and density plot (bottom)\\n\\nThe Python code for multivariate plot types is illustrated with examples below:\\n\\nMultivariate plot: correlation matrix plot\\n\\nfrom matplotlib import pyplot\\nimport seaborn as sns\\ncorrelation = dataset.corr()\\npyplot.figure(figsize=(5,5))\\npyplot.title(\\'Correlation Matrix\\')\\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap=\\'cubehelix\\')\\n\\nMultivariate plot: scatterplot matrix\\n\\nfrom pandas.plotting import scatter_matrix\\nscatter_matrix(dataset)\\n\\nFigure 2-4 illustrates the output.\\n\\n20 \\n\\n| \\n\\nChapter 2: Developing a Machine Learning Model in Python\\n\\n\\x0cFigure 2-4. Correlation (left) and scatterplot (right)\\n\\n4. Data preparation\\n\\nData preparation is a preprocessing step in which data from one or more sources is\\ncleaned and transformed to improve its quality prior to its use.\\n\\n4.1. Data cleaning.    In machine learning modeling, incorrect data can be costly. Data\\ncleaning involves checking the following:\\n\\nValidity\\n\\nThe data type, range, etc.\\n\\nAccuracy\\n\\nThe degree to which the data is close to the true values.\\n\\nCompleteness\\n\\nThe degree to which all required data is known.\\n\\nUniformity\\n\\nThe degree to which the data is specified using the same unit of measure.\\n\\nThe different options for performing data cleaning include:\\n\\nDropping “NA” values within data\\n\\ndataset.dropna(axis=0)\\n\\nFilling “NA” with 0\\n\\ndataset.fillna(0)\\n\\nFilling NAs with the mean of the column\\n\\ndataset[\\'col\\'] = dataset[\\'col\\'].fillna(dataset[\\'col\\'].mean())\\n\\nSteps for Model Development in Python Ecosystem \\n\\n| \\n\\n21\\n\\n\\x0c4.2. Feature selection.     The  data  features  used  to  train  the  machine  learning  models\\nhave  a  huge  influence  on  the  performance.  Irrelevant  or  partially  relevant  features\\ncan  negatively  impact  model  performance.  Feature  selection1  is  a  process  in  which\\nfeatures  in  data  that  contribute  most  to  the  prediction  variable  or  output  are  auto‐\\nmatically selected.\\n\\nThe benefits of performing feature selection before modeling the data are:\\n\\nReduces overfitting2\\n\\nLess redundant data means fewer opportunities for the model to make decisions\\nbased on noise.\\n\\nImproves performance\\n\\nLess misleading data means improved modeling performance.\\n\\nReduces training time and memory footprint\\n\\nLess data means faster training and lower memory footprint.\\n\\nThe  following  sample  feature  is  an  example  demonstrating  when  the  best  two  fea‐\\ntures  are  selected  using  the  SelectKBest  function  under  sklearn.  The  SelectKBest\\nfunction  scores  the  features  using  an  underlying  function  and  then  removes  all  but\\nthe k highest scoring feature:\\n\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import chi2\\nbestfeatures = SelectKBest( k=5)\\nfit = bestfeatures.fit(X,Y)\\ndfscores = pd.DataFrame(fit.scores_)\\ndfcolumns = pd.DataFrame(X.columns)\\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\\nprint(featureScores.nlargest(2,\\'Score\\'))  #print 2 best features\\n\\nOutput\\n\\n                  Specs      Score\\n2              Variable1  58262.490\\n3              Variable2    321.031\\n\\nWhen  features  are  irrelevant,  they  should  be  dropped.  Dropping  the  irrelevant  fea‐\\ntures is illustrated in the following sample code:\\n\\n#dropping the old features\\ndataset.drop([\\'Feature1\\',\\'Feature2\\',\\'Feature3\\'],axis=1,inplace=True)\\n\\n1 Feature selection is more relevant for supervised learning models and is described in detail in the individual\\n\\ncase studies in Chapters 5 and 6.\\n\\n2 Overfitting is covered in detail in Chapter 4.\\n\\n22 \\n\\n| \\n\\nChapter 2: Developing a Machine Learning Model in Python\\n\\n\\x0c4.3. Data transformation.    Many machine learning algorithms make assumptions about\\nthe  data.  It  is  a  good  practice  to  perform  the  data  preparation  in  such  a  way  that\\nexposes the data in the best possible manner to the machine learning algorithms. This\\ncan be accomplished through data transformation.\\n\\nThe different data transformation approaches are as follows:\\n\\nRescaling\\n\\nWhen  data  comprises  attributes  with  varying  scales,  many  machine  learning\\nalgorithms  can  benefit  from  rescaling  all  the  attributes  to  the  same  scale.\\nAttributes are often rescaled in the range between zero and one. This is useful for\\noptimization algorithms used in the core of machine learning algorithms, and it\\nalso helps to speed up the calculations in an algorithm:\\n\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler(feature_range=(0, 1))\\nrescaledX = pd.DataFrame(scaler.fit_transform(X))\\n\\nStandardization\\n\\nStandardization is a useful technique to transform attributes to a standard nor‐\\nmal distribution with a mean of zero and a standard deviation of one. It is most\\nsuitable for techniques that assume the input variables represent a normal distri‐\\nbution:\\n\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler().fit(X)\\nStandardisedX = pd.DataFrame(scaler.fit_transform(X))\\n\\nNormalization\\n\\nNormalization refers to rescaling each observation (row) to have a length of one\\n(called  a  unit  norm  or  a  vector).  This  preprocessing  method  can  be  useful  for\\nsparse datasets of attributes of varying scales when using algorithms that weight\\ninput values:\\n\\nfrom sklearn.preprocessing import Normalizer\\nscaler = Normalizer().fit(X)\\nNormalizedX = pd.DataFrame(scaler.fit_transform(X))\\n\\n5. Evaluate models\\n\\nOnce  we  estimate  the  performance  of  our  algorithm,  we  can  retrain  the  final  algo‐\\nrithm on the entire training dataset and get it ready for operational use. The best way\\nto do this is to evaluate the performance of the algorithm on a new dataset. Different\\nmachine learning techniques require different evaluation metrics. Other than model\\nperformance,  several  other  factors  such  as  simplicity,  interpretability,  and  training\\ntime are considered when selecting a model. The details regarding these factors are\\ncovered in Chapter 4.\\n\\nSteps for Model Development in Python Ecosystem \\n\\n| \\n\\n23\\n\\n\\x0c5.1. Training and test split.     The  simplest  method  we  can  use  to  evaluate  the  perfor‐\\nmance of a machine learning algorithm is to use different training and testing data‐\\nsets. We can take our original dataset and split it into two parts: train the algorithm\\non the first part, make predictions on the second part, and evaluate the predictions\\nagainst the expected results. The size of the split can depend on the size and specifics\\nof  the  dataset,  although  it  is  common  to  use  80%  of  the  data  for  training  and  the\\nremaining 20% for testing. The differences in the training and test datasets can result\\nin meaningful differences in the estimate of accuracy. The data can easily be split into\\nthe training and test sets using the train_test_split function available in sklearn:\\n\\n# split out validation dataset for the end\\nvalidation_size = 0.2\\nseed = 7\\nX_train, X_validation, Y_train, Y_validation =\\\\\\ntrain_test_split(X, Y, test_size=validation_size, random_state=seed)\\n\\n5.2. Identify evaluation metrics.     Choosing  which  metric  to  use  to  evaluate  machine\\nlearning algorithms is very important. An important aspect of evaluation metrics is\\nthe  capability  to  discriminate  among  model  results.  Different  types  of  evaluation\\nmetrics  used  for  different  kinds  of  ML  models  are  covered  in  detail  across  several\\nchapters of this book.\\n\\n5.3. Compare models and algorithms.    Selecting a machine learning model or algorithm\\nis both an art and a science. There is no one solution or approach that fits all. There\\nare several factors over and above the model performance that can impact the deci‐\\nsion to choose a machine learning algorithm.\\n\\nLet’s understand the process of model comparison with a simple example. We define\\ntwo variables, X and Y, and try to build a model to predict Y using X. As a first step,\\nthe data is divided into training and test split as mentioned in the preceding section:\\n\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nvalidation_size = 0.2\\nseed = 7\\nX = 2 - 3 * np.random.normal(0, 1, 20)\\nY = X - 2 * (X ** 2) + 0.5 * (X ** 3) + np.exp(-X)+np.random.normal(-3, 3, 20)\\n# transforming the data to include another axis\\nX = X[:, np.newaxis]\\nY = Y[:, np.newaxis]\\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y,\\\\\\ntest_size=validation_size, random_state=seed)\\n\\nWe have no idea which algorithms will do well on this problem. Let’s design our test\\nnow.  We  will  use  two  models—one  linear  regression  and  the  second  polynomial\\nregression  to  fit  Y  against  X.  We  will  evaluate  algorithms  using  the  Root  Mean\\n\\n24 \\n\\n| \\n\\nChapter 2: Developing a Machine Learning Model in Python\\n\\n\\x0cSquared  Error  (RMSE)  metric,  which  is  one  of  the  measures  of  the  model  perfor‐\\nmance. RMSE will give a gross idea of how wrong all predictions are (zero is perfect):\\n\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error, r2_score\\nfrom sklearn.preprocessing import PolynomialFeatures\\n\\nmodel = LinearRegression()\\nmodel.fit(X_train, Y_train)\\nY_pred = model.predict(X_train)\\n\\nrmse_lin = np.sqrt(mean_squared_error(Y_train,Y_pred))\\nr2_lin = r2_score(Y_train,Y_pred)\\nprint(\"RMSE for Linear Regression:\", rmse_lin)\\n\\npolynomial_features= PolynomialFeatures(degree=2)\\nx_poly = polynomial_features.fit_transform(X_train)\\n\\nmodel = LinearRegression()\\nmodel.fit(x_poly, Y_train)\\nY_poly_pred = model.predict(x_poly)\\n\\nrmse = np.sqrt(mean_squared_error(Y_train,Y_poly_pred))\\nr2 = r2_score(Y_train,Y_poly_pred)\\nprint(\"RMSE for Polynomial Regression:\", rmse)\\n\\nOutput\\n\\nRMSE for Linear Regression: 6.772942423315028\\nRMSE for Polynomial Regression: 6.420495127266883\\n\\nWe can see that the RMSE of the polynomial regression is slightly better than that of\\nthe linear regression.3 With the former having the better fit, it is the preferred model\\nin this step.\\n\\n6. Model tuning\\n\\nFinding  the  best  combination  of  hyperparameters  of  a  model  can  be  treated  as  a\\nsearch problem.4 This searching exercise is often known as model tuning and is one of\\nthe most important steps of model development. It is achieved by searching for the\\nbest  parameters  of  the  model  by  using  techniques  such  as  a  grid  search.  In  a  grid\\nsearch,  you  create  a  grid  of  all  possible  hyperparameter  combinations  and  train\\nthe  model  using  each  one  of  them.  Besides  a  grid  search,  there  are  several  other\\n\\n3 It should be noted that the difference in RMSE is small in this case and may not replicate with a different split\\n\\nof the train/test data.\\n\\n4 Hyperparameters are the external characteristics of the model, can be considered the model’s settings, and are\\n\\nnot estimated based on data-like model parameters.\\n\\nSteps for Model Development in Python Ecosystem \\n\\n| \\n\\n25\\n\\n\\x0ctechniques  for  model  tuning,  including  randomized  search,  Bayesian  optimization,\\nand hyperbrand.\\n\\nIn  the  case  studies  presented  in  this  book,  we  focus  primarily  on  grid  search  for\\nmodel tuning.\\n\\nContinuing on from the preceding example, with the polynomial as the best model:\\nnext, run a grid search for the model, refitting the polynomial regression with differ‐\\nent degrees. We compare the RMSE results for all the models:\\n\\nDeg= [1,2,3,6,10]\\nresults=[]\\nnames=[]\\nfor deg in Deg:\\n    polynomial_features= PolynomialFeatures(degree=deg)\\n    x_poly = polynomial_features.fit_transform(X_train)\\n\\n    model = LinearRegression()\\n    model.fit(x_poly, Y_train)\\n    Y_poly_pred = model.predict(x_poly)\\n\\n    rmse = np.sqrt(mean_squared_error(Y_train,Y_poly_pred))\\n    r2 = r2_score(Y_train,Y_poly_pred)\\n    results.append(rmse)\\n    names.append(deg)\\nplt.plot(names, results,\\'o\\')\\nplt.suptitle(\\'Algorithm Comparison\\')\\n\\nOutput\\n\\nThe  RMSE  decreases  when  the  degree  increases,  and  the  lowest  RMSE  is  for  the\\nmodel with degree 10. However, models with degrees lower than 10 performed very\\nwell, and the test set will be used to finalize the best model.\\n\\n26 \\n\\n| \\n\\nChapter 2: Developing a Machine Learning Model in Python\\n\\n\\x0cWhile the generic set of input parameters for each algorithm provides a starting point\\nfor analysis, it may not have the optimal configurations for the particular dataset and\\nbusiness problem.\\n\\n7. Finalize the model\\n\\nHere, we perform the final steps for selecting the model. First, we run predictions on\\nthe test dataset with the trained model. Then we try to understand the model intu‐\\nition and save it for further usage.\\n\\n7.1. Performance on the test set.    The model selected during the training steps is further\\nevaluated  on  the  test  set.  The  test  set  allows  us  to  compare  different  models  in  an\\nunbiased way, by basing the comparisons in data that were not used in any part of the\\ntraining. The test results for the model developed in the previous step are shown in\\nthe following example:\\n\\nDeg= [1,2,3,6,8,10]\\nfor deg in Deg:\\n    polynomial_features= PolynomialFeatures(degree=deg)\\n    x_poly = polynomial_features.fit_transform(X_train)\\n    model = LinearRegression()\\n    model.fit(x_poly, Y_train)\\n    x_poly_test = polynomial_features.fit_transform(X_test)\\n    Y_poly_pred_test = model.predict(x_poly_test)\\n    rmse = np.sqrt(mean_squared_error(Y_test,Y_poly_pred_test))\\n    r2 = r2_score(Y_test,Y_poly_pred_test)\\n    results_test.append(rmse)\\n    names_test.append(deg)\\nplt.plot(names_test, results_test,\\'o\\')\\nplt.suptitle(\\'Algorithm Comparison\\')\\n\\nOutput\\n\\nSteps for Model Development in Python Ecosystem \\n\\n| \\n\\n27\\n\\n\\x0cIn the training set we saw that the RMSE decreases with an increase in the degree of\\npolynomial model, and the polynomial of degree 10 had the lowest RMSE. However,\\nas shown in the preceding output for the polynomial of degree 10, although the train‐\\ning set had the best results, the results in the test set are poor. For the polynomial of\\ndegree  8,  the  RMSE  in  the  test  set  is  relatively  higher.  The  polynomial  of  degree  6\\nshows  the  best  result  in  the  test  set  (although  the  difference  is  small  compared  to\\nother lower-degree polynomials in the test set) as well as good results in the training\\nset. For these reasons, this is the preferred model.\\n\\nIn  addition  to  the  model  performance,  there  are  several  other  factors  to  consider\\nwhen selecting a model, such as simplicity, interpretability, and training time. These\\nfactors will be covered in the upcoming chapters.\\n\\n7.2. Model/variable intuition.     This  step  involves  considering  a  holistic  view  of  the\\napproach taken to solve the problem, including the model’s limitations as it relates to\\nthe desired outcome, the variables used, and the selected model parameters. Details\\non model and variable intuition regarding different types of machine learning models\\nare presented in the subsequent chapters and case studies.\\n\\n7.3. Save/deploy.    After finding an accurate machine learning model, it must be saved\\nand loaded in order to ensure its usage later.\\n\\nPickle is one of the packages for saving and loading a trained model in Python. Using\\npickle operations, trained machine learning models can be saved in the serialized for‐\\nmat to a file. Later, this serialized file can be loaded to de-serialize the model for its\\nusage. The following sample code demonstrates how to save the model to a file and\\nload it to make predictions on new data:\\n\\n# Save Model Using Pickle\\nfrom pickle import dump\\nfrom pickle import load\\n# save the model to disk\\nfilename = \\'finalized_model.sav\\'\\ndump(model, open(filename, \\'wb\\'))\\n# load the model from disk\\nloaded_model = load(filename)\\n\\nIn  recent  years,  frameworks  such  as  AutoML  have  been  built  to\\nautomate  the  maximum  number  of  steps  in  a  machine  learning\\nmodel  development  process.  Such  frameworks  allow  the  model\\ndevelopers to build ML models with high scale, efficiency, and pro‐\\nductivity. Readers are encouraged to explore such frameworks.\\n\\n28 \\n\\n| \\n\\nChapter 2: Developing a Machine Learning Model in Python\\n\\n\\x0cChapter Summary\\nGiven  its  popularity,  rate  of  adoption,  and  flexibility,  Python  is  often  the  preferred\\nlanguage for machine learning development. There are many available Python pack‐\\nages  to  perform  numerous  tasks,  including  data  cleaning,  visualization,  and  model\\ndevelopment. Some of these key packages are Scikit-learn and Keras.\\n\\nThe  seven  steps  of  model  development  mentioned  in  this  chapter  can  be  leveraged\\nwhile developing any machine learning–based model in finance.\\n\\nNext Steps\\nIn the next chapter, we will cover the key algorithm for machine learning—the artifi‐\\ncial  neural  network.  The  artificial  neural  network  is  another  building  block  of\\nmachine learning in finance and is used across all types of machine learning and deep\\nlearning algorithms.\\n\\nChapter Summary \\n\\n| \\n\\n29\\n\\n\\x0c\\x0cCHAPTER 3\\nArtificial Neural Networks\\n\\nThere  are  many  different  types  of  models  used  in  machine  learning.  However,  one\\nclass of machine learning models that stands out is artificial neural networks (ANNs).\\nGiven that artificial neural networks are used across all machine learning types, this\\nchapter will cover the basics of ANNs.\\n\\nANNs  are  computing  systems  based  on  a  collection  of  connected  units  or  nodes\\ncalled artificial neurons, which loosely model the neurons in a biological brain. Each\\nconnection,  like  the  synapses  in  a  biological  brain,  can  transmit  a  signal  from  one\\nartificial neuron to another. An artificial neuron that receives a signal can process it\\nand then signal additional artificial neurons connected to it.\\n\\nDeep learning involves the study of complex ANN-related algorithms. The complex‐\\nity  is  attributed  to  elaborate  patterns  of  how  information  flows  throughout  the\\nmodel. Deep learning has the ability to represent the world as a nested hierarchy of\\nconcepts, with each concept defined in relation to a simpler concept. Deep learning\\ntechniques are extensively used in reinforcement learning and natural language pro‐\\ncessing applications that we will look at in Chapters 9 and 10.\\n\\n31\\n\\n\\x0cWe  will  review  detailed  terminology  and  processes  used  in  the  field  of  ANNs1  and\\ncover the following topics:\\n\\n• Architecture of ANNs: Neurons and layers\\n\\n• Training an ANN: Forward propagation, backpropagation and gradient descent\\n\\n• Hyperparameters  of  ANNs:  Number  of  layers  and  nodes,  activation  function,\\n\\nloss function, learning rate, etc.\\n\\n• Defining and training a deep neural network–based model in Python\\n\\n• Improving the training speed of ANNs and deep learning models\\n\\nANNs: Architecture, Training, and Hyperparameters\\nANNs contain multiple neurons arranged in layers. An ANN goes through a training\\nphase by comparing the modeled output to the desired output, where it learns to rec‐\\nognize patterns in data. Let us go through the components of ANNs.\\n\\nArchitecture\\nAn ANN architecture comprises neurons, layers, and weights.\\n\\nNeurons\\n\\nThe building blocks for ANNs are neurons (also known as artificial neurons, nodes,\\nor perceptrons). Neurons have one or more inputs and one output. It is possible to\\nbuild  a  network  of  neurons  to  compute  complex  logical  propositions.  Activation\\nfunctions  in  these  neurons  create  complicated,  nonlinear  functional  mappings\\nbetween the inputs and the output.2\\n\\nAs  shown  in  Figure  3-1,  a  neuron  takes  an  input  (x1,  x2…xn),  applies  the  learning\\nparameters to generate a weighted sum (z), and then passes that sum to an activation\\nfunction (f) that computes the output f(z).\\n\\n1 Readers are encouraged to refer to the book Deep Learning by Aaron Courville, Ian Goodfellow, and Yoshua\\n\\nBengio (MIT Press) for more details on ANN and deep learning.\\n\\n2 Activation functions are described in detail later in this chapter.\\n\\n32 \\n\\n| \\n\\nChapter 3: Artificial Neural Networks\\n\\n\\x0cFigure 3-1. An artificial neuron\\n\\nLayers\\n\\nThe  output  f(z)  from  a  single  neuron  (as  shown  in  Figure  3-1)  will  not  be  able  to\\nmodel complex tasks. So, in order to handle more complex structures, we have multi‐\\nple layers of such neurons. As we keep stacking neurons horizontally and vertically,\\nthe class of functions we can get becomes increasing complex. Figure 3-2 shows an\\narchitecture of an ANN with an input layer, an output layer, and a hidden layer.\\n\\nFigure 3-2. Neural network architecture\\n\\nInput layer.    The input layer takes input from the dataset and is the exposed part of\\nthe network. A neural network is often drawn with an input layer of one neuron per\\ninput value (or column) in the dataset. The neurons in the input layer simply pass the\\ninput value though to the next layer.\\n\\nANNs: Architecture, Training, and Hyperparameters \\n\\n| \\n\\n33\\n\\n\\x0cHidden layers.     Layers  after  the  input  layer  are  called  hidden  layers  because  they  are\\nnot directly exposed to the input. The simplest network structure is to have a single\\nneuron in the hidden layer that directly outputs the value.\\n\\nA  multilayer  ANN  is  capable  of  solving  more  complex  machine  learning–related\\ntasks  due  to  its  hidden  layer(s).  Given  increases  in  computing  power  and  efficient\\nlibraries,  neural  networks  with  many  layers  can  be  constructed.  ANNs  with  many\\nhidden layers (more than three) are known as deep neural networks. Multiple hidden\\nlayers allow deep neural networks to learn features of the data in a so-called feature\\nhierarchy,  because  simple  features  recombine  from  one  layer  to  the  next  to  form\\nmore  complex  features.  ANNs  with  many  layers  pass  input  data  (features)  through\\nmore mathematical operations than do ANNs with few layers and are therefore more\\ncomputationally intensive to train.\\n\\nOutput layer.    The final layer is called the output layer; it is responsible for outputting\\na  value  or  vector  of  values  that  correspond  to  the  format  required  to  solve  the\\nproblem.\\n\\nNeuron weights\\n\\nA  neuron  weight  represents  the  strength  of  the  connection  between  units  and\\nmeasures the influence the input will have on the output. If the weight from neuron\\none  to  neuron  two  has  greater  magnitude,  it  means  that  neuron  one  has  a  greater\\ninfluence  over  neuron  two.  Weights  near  zero  mean  changing  this  input  will  not\\nchange  the  output.  Negative  weights  mean  increasing  this  input  will  decrease  the\\noutput.\\n\\nTraining\\nTraining a neural network basically means calibrating all of the weights in the ANN.\\nThis optimization is performed using an iterative approach involving forward propa‐\\ngation and backpropagation steps.\\n\\nForward propagation\\n\\nForward propagation is a process of feeding input values to the neural network and\\ngetting an output, which we call predicted value. When we feed the input values to the\\nneural  network’s  first  layer,  it  goes  without  any  operations.  The  second  layer  takes\\nvalues from the first layer and applies multiplication, addition, and activation opera‐\\ntions  before  passing  this  value  to  the  next  layer.  The  same  process  repeats  for  any\\nsubsequent layers until an output value from the last layer is received.\\n\\n34 \\n\\n| \\n\\nChapter 3: Artificial Neural Networks\\n\\n\\x0cBackpropagation\\n\\nAfter  forward  propagation,  we  get  a  predicted  value  from  the  ANN.  Suppose  the\\ndesired output of a network is Y and the predicted value of the network from forward\\npropagation is Y′. The difference between the predicted output and the desired out‐\\nput (Y–Y′ ) is converted into the loss (or cost) function J(w), where w represents the\\nweights in ANN.3 The goal is to optimize the loss function (i.e., make the loss as small\\nas possible) over the training set.\\n\\nThe optimization method used is gradient descent. The goal of the gradient descent\\nmethod is to find the gradient of J(w) with respect to w at the current point and take a\\nsmall  step  in  the  direction  of  the  negative  gradient  until  the  minimum  value  is\\nreached, as shown in Figure 3-3.\\n\\nFigure 3-3. Gradient descent\\n\\nIn  an  ANN,  the  function  J(w)  is  essentially  a  composition  of  multiple  layers,  as\\nexplained in the preceding text. So, if layer one is represented as function p(), layer\\ntwo as q(), and layer three as r(), then the overall function is J(w)=r(q(p())). w consists\\nof all weights in all three layers. We want to find the gradient of J(w) with respect to\\neach component of w.\\n\\nSkipping the mathematical details, the above essentially implies that the gradient of a\\ncomponent w in the first layer would depend on the gradients in the second and third\\nlayers. Similarly, the gradients in the second layer will depend on the gradients in the\\nthird  layer.  Therefore,  we  start  computing  the  derivatives  in  the  reverse  direction,\\nstarting with the last layer, and use backpropagation to compute gradients of the pre‐\\nvious layer.\\n\\n3 There are many available loss functions discussed in the next section. The nature of our problem dictates our\\n\\nchoice of loss function.\\n\\nANNs: Architecture, Training, and Hyperparameters \\n\\n| \\n\\n35\\n\\n\\x0cOverall, in the process of backpropagation, the model error (difference between pre‐\\ndicted  and  desired  output)  is  propagated  back  through  the  network,  one  layer  at  a\\ntime, and the weights are updated according to the amount they contributed to the\\nerror.\\n\\nAlmost all ANNs use gradient descent and backpropagation. Backpropagation is one\\nof the cleanest and most efficient ways to find the gradient.\\n\\nHyperparameters\\nHyperparameters  are  the  variables  that  are  set  before  the  training  process,  and  they\\ncannot  be  learned  during  training.  ANNs  have  a  large  number  of  hyperparameters,\\nwhich  makes  them  quite  flexible.  However,  this  flexibility  makes  the  model  tuning\\nprocess difficult. Understanding the hyperparameters and the intuition behind them\\nhelps give an idea of what values are reasonable for each hyperparameter so we can\\nrestrict the search space. Let’s start with the number of hidden layers and nodes.\\n\\nNumber of hidden layers and nodes\\n\\nMore hidden layers or nodes per layer means more parameters in the ANN, allowing\\nthe model to fit more complex functions. To have a trained network that generalizes\\nwell, we need to pick an optimal number of hidden layers, as well as of the nodes in\\neach hidden layer. Too few nodes and layers will lead to high errors for the system, as\\nthe predictive factors might be too complex for a small number of nodes to capture.\\nToo many nodes and layers will overfit to the training data and not generalize well.\\n\\nThere is no hard-and-fast rule to decide the number of layers and nodes.\\n\\nThe number of hidden layers primarily depends on the complexity of the task. Very\\ncomplex  tasks,  such  as  large  image  classification  or  speech  recognition,  typically\\nrequire networks with dozens of layers and a huge amount of training data. For the\\nmajority of the problems, we can start with just one or two hidden layers and then\\ngradually ramp up the number of hidden layers until we start overfitting the training\\nset.\\n\\nThe number of hidden nodes should have a relationship to the number of input and\\noutput nodes, the amount of training data available, and the complexity of the func‐\\ntion being modeled. As a rule of thumb, the number of hidden nodes in each layer\\nshould be somewhere between the size of the input layer and the size of the output\\nlayer,  ideally  the  mean.  The  number  of  hidden  nodes  shouldn’t  exceed  twice  the\\nnumber of input nodes in order to avoid overfitting.\\n\\nLearning rate\\n\\nWhen we train ANNs, we use many iterations of forward propagation and backpro‐\\npagation to optimize the weights. At each iteration we calculate the derivative of the\\n\\n36 \\n\\n| \\n\\nChapter 3: Artificial Neural Networks\\n\\n\\x0closs function with respect to each weight and subtract it from that weight. The learn‐\\ning rate determines how quickly or slowly we want to update our weight (parameter)\\nvalues. This learning rate should be high enough so that it converges in a reasonable\\namount of time. Yet it should be low enough so that it finds the minimum value of\\nthe loss function.\\n\\nActivation functions\\n\\nActivation  functions  (as  shown  in  Figure  3-1)  refer  to  the  functions  used  over  the\\nweighted  sum  of  inputs  in  ANNs  to  get  the  desired  output.  Activation  functions\\nallow the network to combine the inputs in more complex ways, and they provide a\\nricher capability in the relationship they can model and the output they can produce.\\nThey decide which neurons will be activated—that is, what information is passed to\\nfurther layers.\\n\\nWithout  activation  functions,  ANNs  lose  a  bulk  of  their  representation  learning\\npower. There are several activation functions. The most widely used are as follows:\\n\\nLinear (identity) function\\n\\nRepresented by the equation of a straight line (i.e.,  f (x) = mx + c), where activa‐\\ntion  is  proportional  to  the  input.  If  we  have  many  layers,  and  all  the  layers  are\\nlinear in nature, then the final activation function of the last layer is the same as\\nthe linear function of the first layer. The range of a linear function is –inf to +inf.\\n\\nSigmoid function\\n\\nRefers  to  a  function  that  is  projected  as  an  S-shaped  graph  (as  shown  in\\nFigure  3-4).  It  is  represented  by  the  mathematical  equation  f (x) = 1 / (1 + e –x)\\nand ranges from 0 to 1. A large positive input results in a large positive output; a\\nlarge  negative  input  results  in  a  large  negative  output.  It  is  also  referred  to  as\\nlogistic activation function.\\n\\nto  sigmoid  activation \\n\\nTanh function\\nSimilar \\nfunction  with  a  mathematical  equation\\nTanh (x) = 2Sigmoid(2x) – 1,  where  Sigmoid  represents  the  sigmoid  function\\ndiscussed above. The output of this function ranges from –1 to 1, with an equal\\nmass on both sides of the zero-axis, as shown in Figure 3-4.\\n\\nReLU function\\n\\nfor \\n\\nis  represented  as\\nthe  Rectified  Linear  Unit  and \\nReLU  stands \\nf (x) = max(x, 0). So, if the input is a positive number, the function returns the\\nnumber  itself,  and  if  the  input  is  a  negative  number,  then  the  function  returns\\nzero. It is the most commonly used function because of its simplicity.\\n\\nANNs: Architecture, Training, and Hyperparameters \\n\\n| \\n\\n37\\n\\n\\x0cFigure 3-4 shows a summary of the activation functions discussed in this section.\\n\\nFigure 3-4. Activation functions\\n\\nThere  is  no  hard-and-fast  rule  for  activation  function  selection.  The  decision  com‐\\npletely relies on the properties of the problem and the relationships being modeled.\\nWe can try different activation functions and select the one that helps provide faster\\nconvergence and a more efficient training process. The choice of activation function\\nin the output layer is strongly constrained by the type of problem that is modeled.4\\n\\nCost functions\\n\\nCost  functions  (also  known  as  loss  functions)  are  a  measure  of  the  ANN  perfor‐\\nmance, measuring how well the ANN fits empirical data. The two most common cost\\nfunctions are:\\n\\nMean squared error (MSE)\\n\\nThis is the cost function used primarily for regression problems, where output is\\na  continuous  value.  MSE  is  measured  as  the  average  of  the  squared  difference\\n\\n4 Deriving a regression or classification output by changing the activation function of the output layer is\\n\\ndescribed further in Chapter 4.\\n\\n38 \\n\\n| \\n\\nChapter 3: Artificial Neural Networks\\n\\n\\x0cbetween  predictions  and  actual  observation.  MSE  is  described  further  in\\nChapter 4.\\n\\nCross-entropy (or log loss)\\n\\nThis cost function is used primarily for classification problems, where output is a\\nprobability value between zero and one. Cross-entropy loss increases as the pre‐\\ndicted probability diverges from the actual label. A perfect model would have a\\ncross-entropy of zero.\\n\\nOptimizers\\n\\nOptimizers update the weight parameters to minimize the loss function.5 Cost func‐\\ntion  acts  as  a  guide  to  the  terrain,  telling  the  optimizer  if  it  is  moving  in  the  right\\ndirection  to  reach  the  global  minimum.  Some  of  the  common  optimizers  are  as\\nfollows:\\n\\nMomentum\\n\\nThe momentum optimizer looks at previous gradients in addition to the current\\nstep. It will take larger steps if the previous updates and the current update move\\nthe weights in the same direction (gaining momentum). It will take smaller steps\\nif  the  direction  of  the  gradient  is  opposite.  A  clever  way  to  visualize  this  is  to\\nthink of a ball rolling down a valley—it will gain momentum as it approaches the\\nvalley bottom.\\n\\nAdaGrad (Adaptive Gradient Algorithm)\\n\\nAdaGrad adapts the learning rate to the parameters, performing smaller updates\\nfor parameters associated with frequently occurring features, and larger updates\\nfor parameters associated with infrequent features.\\n\\nRMSProp\\n\\nRMSProp stands for Root Mean Square Propagation. In RMSProp, the learning\\nrate gets adjusted automatically, and it chooses a different learning rate for each\\nparameter.\\n\\nAdam (Adaptive Moment Estimation)\\n\\nAdam combines the best properties of the AdaGrad and RMSProp algorithms to\\nprovide an optimization and is one of the most popular gradient descent optimi‐\\nzation algorithms.\\n\\n5 Refer to https://oreil.ly/FSt-8 for more details on optimization.\\n\\nANNs: Architecture, Training, and Hyperparameters \\n\\n| \\n\\n39\\n\\n\\x0cEpoch\\n\\nOne round of updating the network for the entire training dataset is called an epoch.\\nA network may be trained for tens, hundreds, or many thousands of epochs depend‐\\ning on the data size and computational constraints.\\n\\nBatch size\\n\\nThe batch size is the number of training examples in one forward/backward pass. A\\nbatch size of 32 means that 32 samples from the training dataset will be used to esti‐\\nmate the error gradient before the model weights are updated. The higher the batch\\nsize, the more memory space is needed.\\n\\nCreating an Artificial Neural Network Model in Python\\nIn Chapter 2 we discussed the steps for end-to-end model development in Python. In\\nthis section, we dig deeper into the steps involved in building an ANN-based model\\nin Python.\\n\\nOur first step will be to look at Keras, the Python package specifically built for ANN\\nand deep learning.\\n\\nInstalling Keras and Machine Learning Packages\\nThere are several Python libraries that allow building ANN and deep learning models\\neasily and quickly without getting into the details of underlying algorithms. Keras is\\none of the most user-friendly packages that enables an efficient numerical computa‐\\ntion related to ANNs. Using Keras, complex deep learning models can be defined and\\nimplemented  in  a  few  lines  of  code.  We  will  primarily  be  using  Keras  packages  for\\nimplementing deep learning models in several of the book’s case studies.\\n\\nKeras  is  simply  a  wrapper  around  more  complex  numerical  computation  engines\\nsuch  as  TensorFlow  and  Theano.  In  order  to  install  Keras,  TensorFlow  or  Theano\\nneeds to be installed first.\\n\\nThis section describes the steps to define and compile an ANN-based model in Keras,\\nwith a focus on the following steps.6\\n\\nImporting the packages\\n\\nBefore you can start to build an ANN model, you need to import two modules from\\nthe Keras package: Sequential and Dense:\\n\\n6 The steps and Python code related to implementing deep learning models using Keras, as demonstrated in\\n\\nthis section, are used in several case studies in the subsequent chapters.\\n\\n40 \\n\\n| \\n\\nChapter 3: Artificial Neural Networks\\n\\n\\x0cfrom Keras.models import Sequential\\nfrom Keras.layers import Dense\\nimport numpy as np\\n\\nLoading data\\n\\nThis example makes use of the random module of NumPy to quickly generate some\\ndata  and  labels  to  be  used  by  ANN  that  we  build  in  the  next  step.  Specifically,  an\\narray with size (1000,10) is first constructed. Next, we create a labels array that con‐\\nsists of zeros and ones with a size (1000,1):\\n\\ndata = np.random.random((1000,10))\\nY = np.random.randint(2,size= (1000,1))\\nmodel = Sequential()\\n\\nModel construction—defining the neural network architecture\\n\\nA quick way to get started is to use the Keras Sequential model, which is a linear stack\\nof layers. We create a Sequential model and add layers one at a time until the network\\ntopology is finalized. The first thing to get right is to ensure the input layer has the\\nright  number  of  inputs.  We  can  specify  this  when  creating  the  first  layer.  We  then\\nselect a dense or fully connected layer to indicate that we are dealing with an input\\nlayer by using the argument input_dim.\\n\\nWe  add  a  layer  to  the  model  with  the  add()  function,  and  the  number  of  nodes  in\\neach layer is specified. Finally, another dense layer is added as an output layer.\\n\\nThe architecture for the model shown in Figure 3-5 is as follows:\\n\\n• The model expects rows of data with 10 variables (input_dim_=10 argument).\\n• The first hidden layer has 32 nodes and uses the relu activation function.\\n• The second hidden layer has 32 nodes and uses the relu activation function.\\n• The output layer has one node and uses the sigmoid activation function.\\n\\nCreating an Artificial Neural Network Model in Python \\n\\n| \\n\\n41\\n\\n\\x0cFigure 3-5. An ANN architecture\\n\\nThe Python code for the network in Figure 3-5 is shown below:\\n\\nmodel = Sequential()\\nmodel.add(Dense(32, input_dim=10, activation= \\'relu\\' ))\\nmodel.add(Dense(32, activation= \\'relu\\' ))\\nmodel.add(Dense(1, activation= \\'sigmoid\\'))\\n\\nCompiling the model\\n\\nWith the model constructed, it can be compiled with the help of the compile() func‐\\ntion. Compiling the model leverages the efficient numerical libraries in the Theano or\\nTensorFlow  packages.  When  compiling,  it  is  important  to  specify  the  additional\\nproperties  required  when  training  the  network.  Training  a  network  means  finding\\nthe best set of weights to make predictions for the problem at hand. So we must spec‐\\nify  the  loss  function  used  to  evaluate  a  set  of  weights,  the  optimizer  used  to  search\\nthrough different weights for the network, and any optional metrics we would like to\\ncollect and report during training.\\n\\nIn  the  following  example,  we  use  cross-entropy  loss  function,  which  is  defined  in\\nKeras  as  binary_crossentropy.  We  will  also  use  the  adam  optimizer,  which  is  the\\ndefault  option.  Finally,  because  it  is  a  classification  problem,  we  will  collect  and\\nreport the classification accuracy as the metric.7 The Python code follows:\\n\\nmodel.compile(loss= \\'binary_crossentropy\\' , optimizer= \\'adam\\' , \\\\\\n  metrics=[ \\'accuracy\\' ])\\n\\n7 A detailed discussion of the evaluation metrics for classification models is presented in Chapter 4.\\n\\n42 \\n\\n| \\n\\nChapter 3: Artificial Neural Networks\\n\\n\\x0cFitting the model\\n\\nWith our model defined and compiled, it is time to execute it on data. We can train\\nor fit our model on our loaded data by calling the fit() function on the model.\\n\\nThe training process will run for a fixed number of iterations (epochs) through the\\ndataset,  specified  using  the  nb_epoch  argument.  We  can  also  set  the  number  of\\ninstances that are evaluated before a weight update in the network is performed. This\\nis set using the batch_size argument. For this problem we will run a small number\\nof epochs (10) and use a relatively small batch size of 32. Again, these can be chosen\\nexperimentally through trial and error. The Python code follows:\\n\\nmodel.fit(data, Y, nb_epoch=10, batch_size=32)\\n\\nEvaluating the model\\n\\nWe have trained our neural network on the entire dataset and can evaluate the per‐\\nformance of the network on the same dataset. This will give us an idea of how well we\\nhave modeled the dataset (e.g., training accuracy) but will not provide insight on how\\nwell the algorithm will perform on new data. For this, we separate the data into train‐\\ning and test datasets. The model is evaluated on the training dataset using the evalua\\ntion() function. This will generate a prediction for each input and output pair and\\ncollect  scores,  including  the  average  loss  and  any  metrics  configured,  such  as  accu‐\\nracy. The Python code follows:\\n\\nscores = model.evaluate(X_test, Y_test)\\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\\n\\nRunning an ANN Model Faster: GPU and Cloud Services\\nFor  training  ANNs  (especially  deep  neural  networks  with  many  layers),  a  large\\namount  of  computation  power  is  required.  Available  CPUs,  or  Central  Processing\\nUnits, are responsible for processing and executing instructions on a local machine.\\nSince CPUs are limited in the number of cores and take up the job sequentially, they\\ncannot do rapid matrix computations for the large number of matrices required for\\ntraining  deep  learning  models.  Hence,  the  training  of  deep  learning  models  can  be\\nextremely slow on the CPUs.\\n\\nThe following alternatives are useful for running ANNs that generally require a sig‐\\nnificant amount of time to run on a CPU:\\n\\n• Running notebooks locally on a GPU.\\n\\n• Running notebooks on Kaggle Kernels or Google Colaboratory.\\n\\n• Using Amazon Web Services.\\n\\nCreating an Artificial Neural Network Model in Python \\n\\n| \\n\\n43\\n\\n\\x0cGPU\\n\\nA  GPU  is  composed  of  hundreds  of  cores  that  can  handle  thousands  of  threads\\nsimultaneously. Running ANNs and deep learning models can be accelerated by the\\nuse of GPUs.\\n\\nGPUs are particularly adept at processing complex matrix operations. The GPU cores\\nare highly specialized, and they massively accelerate processes such as deep learning\\ntraining by offloading the processing from CPUs to the cores in the GPU subsystem.\\n\\nAll the Python packages related to machine learning, including Tensorflow, Theano,\\nand Keras, can be configured for the use of GPUs.\\n\\nCloud services such as Kaggle and Google Colab\\n\\nIf you have a GPU-enabled computer, you can run ANNs locally. If you do not, we\\nrecommend you use a service such as Kaggle Kernels, Google Colab, or AWS:\\n\\nKaggle\\n\\nA popular data science website owned by Google that hosts Jupyter service and is\\nalso referred to as Kaggle Kernels. Kaggle Kernels are free to use and come with\\nthe most frequently used packages preinstalled. You can connect a kernel to any\\ndataset hosted on Kaggle, or alternatively, you can just upload a new dataset on\\nthe fly.\\n\\nGoogle Colaboratory\\n\\nA  free  Jupyter  Notebook  environment  provided  by  Google  where  you  can  use\\nfree GPUs. The features of Google Colaboratory are the same as Kaggle.\\n\\nAmazon Web Services (AWS)\\n\\nAWS Deep Learning provides an infrastructure to accelerate deep learning in the\\ncloud,  at  any  scale.  You  can  quickly  launch  AWS  server  instances  preinstalled\\nwith popular deep learning frameworks and interfaces to train sophisticated, cus‐\\ntom AI models, experiment with new algorithms, or learn new skills and techni‐\\nques. These web servers can run longer than Kaggle Kernels. So for big projects,\\nit might be worth using an AWS instead of a kernel.\\n\\n44 \\n\\n| \\n\\nChapter 3: Artificial Neural Networks\\n\\n\\x0cChapter Summary\\nANNs  comprise  a  family  of  algorithms  used  across  all  types  of  machine  learning.\\nThese models are inspired by the biological neural networks containing neurons and\\nlayers of neurons that constitute animal brains. ANNs with many layers are referred\\nto as deep neural networks. Several steps, including forward propagation and back‐\\npropagation,  are  required  for  training  these  ANNs.  Python  packages  such  as  Keras\\nmake the training of these ANNs possible in a few lines of code. The training of these\\ndeep neural networks require more computational power, and CPUs alone might not\\nbe enough. Alternatives include using a GPU or cloud service such as Kaggle Kernels,\\nGoogle Colaboratory, or Amazon Web Services for training deep neural networks.\\n\\nNext Steps\\nAs a next step, we will be going into the details of the machine learning concepts for\\nsupervised  learning,  followed  by  case  studies  using  the  concepts  covered  in  this\\nchapter.\\n\\nChapter Summary \\n\\n| \\n\\n45\\n\\n\\x0c\\x0cPART II\\nSupervised Learning\\n\\n\\x0c\\x0cCHAPTER 4\\nSupervised Learning: Models and Concepts\\n\\nSupervised learning is an area of machine learning where the chosen algorithm tries\\nto fit a target using the given input. A set of training data that contains labels is sup‐\\nplied to the algorithm. Based on a massive set of data, the algorithm will learn a rule\\nthat  it  uses  to  predict  the  labels  for  new  observations.  In  other  words,  supervised\\nlearning algorithms are provided with historical data and asked to find the relation‐\\nship that has the best predictive power.\\n\\nThere  are  two  varieties  of  supervised  learning  algorithms:  regression  and  classifica‐\\ntion algorithms. Regression-based supervised learning methods try to predict outputs\\nbased  on  input  variables.  Classification-based  supervised  learning  methods  identify\\nwhich  category  a  set  of  data  items  belongs  to.  Classification  algorithms  are\\nprobability-based, meaning the outcome is the category for which the algorithm finds\\nthe highest probability that the dataset belongs to it. Regression algorithms, in con‐\\ntrast,  estimate  the  outcome  of  problems  that  have  an  infinite  number  of  solutions\\n(continuous set of possible outcomes).\\n\\nIn the context of finance, supervised learning models represent one of the most-used\\nclass of machine learning models. Many algorithms that are widely applied in algo‐\\nrithmic  trading  rely  on  supervised  learning  models  because  they  can  be  efficiently\\ntrained, they are relatively robust to noisy financial data, and they have strong links\\nto the theory of finance.\\n\\nRegression-based algorithms have been leveraged by academic and industry research‐\\ners  to  develop  numerous  asset  pricing  models.  These  models  are  used  to  predict\\nreturns  over  various  time  periods  and  to  identify  significant  factors  that  drive  asset\\nreturns.  There  are  many  other  use  cases  of  regression-based  supervised  learning  in\\nportfolio management and derivatives pricing.\\n\\n49\\n\\n\\x0cClassification-based algorithms, on the other hand, have been leveraged across many\\nareas  within  finance  that  require  predicting  a  categorical  response.  These  include\\nfraud  detection,  default  prediction,  credit  scoring,  directional  forecast  of  asset  price\\nmovement,  and  Buy/Sell  recommendations.  There  are  many  other  use  cases  of\\nclassification-based  supervised  learning  in  portfolio  management  and  algorithmic\\ntrading.\\n\\nMany  use  cases  of  regression-based  and  classification-based  supervised  machine\\nlearning are presented in Chapters 5 and 6.\\n\\nPython  and  its  libraries  provide  methods  and  ways  to  implement  these  supervised\\nlearning models in few lines of code. Some of these libraries were covered in Chap‐\\nter  2.  With  easy-to-use  machine  learning  libraries  like  Scikit-learn  and  Keras,  it  is\\nstraightforward to fit different machine learning models on a given predictive model‐\\ning dataset.\\n\\nIn this chapter, we present a high-level overview of supervised learning models. For a\\nthorough coverage of the topics, the reader is referred to Hands-On Machine Learn‐\\ning  with  Scikit-Learn,  Keras,  and  TensorFlow,  2nd  Edition,  by  Aurélien  Géron\\n(O’Reilly).\\n\\nThe following topics are covered in this chapter:\\n\\n• Basic concepts of supervised learning models (both regression and classification).\\n\\n• How to implement different supervised learning models in Python.\\n\\n• How to tune the models and identify the optimal parameters of the models using\\n\\ngrid search.\\n\\n• Overfitting versus underfitting and bias versus variance.\\n\\n• Strengths and weaknesses of several supervised learning models.\\n\\n• How to use ensemble models, ANN, and deep learning models for both regres‐\\n\\nsion and classification.\\n\\n• How  to  select  a  model  on  the  basis  of  several  factors,  including  model\\n\\nperformance.\\n\\n• Evaluation metrics for classification and regression models.\\n\\n• How to perform cross validation.\\n\\n50 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cSupervised Learning Models: An Overview\\nClassification predictive modeling problems are different from regression predictive\\nmodeling problems, as classification is the task of predicting a discrete class label and\\nregression  is  the  task  of  predicting  a  continuous  quantity.  However,  both  share  the\\nsame concept of utilizing known variables to make predictions, and there is a signifi‐\\ncant overlap between the two models. Hence, the models for classification and regres‐\\nsion  are  presented  together  in  this  chapter.  Figure  4-1  summarizes  the  list  of  the\\nmodels commonly used for classification and regression.\\n\\nSome models can be used for both classification and regression with small modifica‐\\ntions.  These  are  K-nearest  neighbors,  decision  trees,  support  vector,  ensemble  bag‐\\nging/boosting  methods,  and  ANNs  (including  deep  neural  networks),  as  shown  in\\nFigure 4-1. However, some models, such as linear regression and logistic regression,\\ncannot (or cannot easily) be used for both problem types.\\n\\nFigure 4-1. Models for regression and classification\\n\\nThis section contains the following details about the models:\\n\\n• Theory of the models.\\n\\n• Implementation in Scikit-learn or Keras.\\n\\n• Grid search for different models.\\n\\n• Pros and cons of the models.\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n51\\n\\n\\x0cIn finance, a key focus is on models that extract signals from previ‐\\nously observed data in order to predict future values for the same\\ntime series. This family of time series models predicts continuous\\noutput and is more aligned with the supervised regression models.\\nTime series models are covered separately in the supervised regres‐\\nsion chapter (Chapter 5).\\n\\nLinear Regression (Ordinary Least Squares)\\nLinear regression (Ordinary Least Squares Regression or OLS Regression) is perhaps\\none  of  the  most  well-known  and  best-understood  algorithms  in  statistics  and\\nmachine  learning.  Linear  regression  is  a  linear  model,  e.g.,  a  model  that  assumes  a\\nlinear relationship between the input variables (x) and the single output variable (y).\\nThe goal of linear regression is to train a linear model to predict a new y given a pre‐\\nviously unseen x with as little error as possible.\\n\\nOur model will be a function that predicts y given x1, x2...xi:\\n\\ny = β0 + β1x1 + ... + βi xi\\n\\nwhere, β0 is called intercept and β1...βi are the coefficient of the regression.\\n\\nImplementation in Python\\n\\nfrom sklearn.linear_model import LinearRegression\\nmodel = LinearRegression()\\nmodel.fit(X, Y)\\n\\nIn the following section, we cover the training of a linear regression model and grid\\nsearch of the model. However, the overall concepts and related approaches are appli‐\\ncable to all other supervised learning models.\\n\\nTraining a model\\n\\nAs we mentioned in Chapter 3, training a model basically means retrieving the model\\nparameters by minimizing the cost (loss) function. The two steps for training a linear\\nregression model are:\\n\\nDefine a cost function (or loss function)\\n\\nMeasures how inaccurate the model’s predictions are. The sum of squared residu‐\\nals (RSS) as defined in Equation 4-1 measures the squared sum of the difference\\nbetween  the  actual  and  predicted  value  and  is  the  cost  function  for  linear\\nregression.\\n\\n52 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cEquation 4-1. Sum of squared residuals\\n\\nn\\nRSS = ∑\\ni=1\\n\\nn\\n(yi – β0 – ∑\\nj=1\\n\\n2\\n\\nβj xij)\\n\\nIn this equation, β0 is the intercept; βj represents the coefficient; β1, .., βj are the\\ncoefficients  of  the  regression;  and  xij  represents  the  i th   observation  and  j th\\nvariable.\\n\\nFind the parameters that minimize loss\\n\\nFor example, make our model as accurate as possible. Graphically, in two dimen‐\\nsions,  this  results  in  a  line  of  best  fit  as  shown  in  Figure  4-2.  In  higher  dimen‐\\nsions, we would have higher-dimensional hyperplanes. Mathematically, we look\\nat the difference between each real data point (y) and our model’s prediction (ŷ).\\nSquare  these  differences  to  avoid  negative  numbers  and  penalize  larger  differ‐\\nences, and then add them up and take the average. This is a measure of how well\\nour data fits the line.\\n\\nFigure 4-2. Linear regression\\n\\nGrid search\\n\\nThe overall idea of the grid search is to create a grid of all possible hyperparameter\\ncombinations and train the model using each one of them. Hyperparameters are the\\nexternal characteristic of the model, can be considered the model’s settings, and are\\nnot  estimated  based  on  data-like  model  parameters.  These  hyperparameters  are\\ntuned during grid search to achieve better model performance.\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n53\\n\\n\\x0cDue to its exhaustive search, a grid search is guaranteed to find the optimal parame‐\\nter within the grid. The drawback is that the size of the grid grows exponentially with\\nthe addition of more parameters or more considered values.\\n\\nThe GridSearchCV class in the model_selection module of the sklearn package facil‐\\nitates the systematic evaluation of all combinations of the hyperparameter values that\\nwe would like to test.\\n\\nThe first step is to create a model object. We then define a dictionary where the key‐\\nwords  name  the  hyperparameters  and  the  values  list  the  parameter  settings  to  be\\ntested.  For  linear  regression,  the  hyperparameter  is  fit_intercept,  which  is  a\\nboolean  variable  that  determines  whether  or  not  to  calculate  the  intercept  for  this\\nmodel. If set to False, no intercept will be used in calculations:\\n\\nmodel = LinearRegression()\\nparam_grid = {\\'fit_intercept\\': [True, False]}\\n}\\n\\nThe second step is to instantiate the GridSearchCV object and provide the estimator\\nobject and parameter grid, as well as a scoring method and cross validation choice, to\\nthe initialization method. Cross validation is a resampling procedure used to evaluate\\nmachine  learning  models,  and  scoring  parameter  is  the  evaluation  metrics  of  the\\nmodel:1\\n\\nWith all settings in place, we can fit GridSearchCV:\\n\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring= \\'r2\\', \\\\\\n  cv=kfold)\\ngrid_result = grid.fit(X, Y)\\n\\nAdvantages and disadvantages\\n\\nIn terms of advantages, linear regression is easy to understand and interpret. How‐\\never, it may not work well when there is a nonlinear relationship between predicted\\nand predictor variables. Linear regression is prone to overfitting (which we will dis‐\\ncuss in the next section) and when a large number of features are present, it may not\\nhandle irrelevant features well. Linear regression also requires the data to follow cer‐\\ntain  assumptions,  such  as  the  absence  of  multicollinearity.  If  the  assumptions  fail,\\nthen we cannot trust the results obtained.\\n\\n1 Cross validation will be covered in detail later in this chapter.\\n\\n54 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cRegularized Regression\\nWhen  a  linear  regression  model  contains  many  independent  variables,  their  coeffi‐\\ncients will be poorly determined, and the model will have a tendency to fit extremely\\nwell to the training data (data used to build the model) but fit poorly to testing data\\n(data  used  to  test  how  good  the  model  is).  This  is  known  as  overfitting  or  high\\nvariance.\\n\\nOne  popular  technique  to  control  overfitting  is  regularization,  which  involves  the\\naddition of a penalty term to the error or loss function to discourage the coefficients\\nfrom reaching large values. Regularization, in simple terms, is a penalty mechanism\\nthat applies shrinkage to model parameters (driving them closer to zero) in order to\\nbuild a model with higher prediction accuracy and interpretation. Regularized regres‐\\nsion has two advantages over linear regression:\\n\\nPrediction accuracy\\n\\nThe  performance  of  the  model  working  better  on  the  testing  data  suggests  that\\nthe  model  is  trying  to  generalize  from  training  data.  A  model  with  too  many\\nparameters might try to fit noise specific to the training data. By shrinking or set‐\\nting  some  coefficients  to  zero,  we  trade  off  the  ability  to  fit  complex  models\\n(higher bias) for a more generalizable model (lower variance).\\n\\nInterpretation\\n\\nA large number of predictors may complicate the interpretation or communica‐\\ntion of the big picture of the results. It may be preferable to sacrifice some detail\\nto limit the model to a smaller subset of parameters with the strongest effects.\\n\\nThe common ways to regularize a linear regression model are as follows:\\n\\nL1 regularization or Lasso regression\\n\\nLasso regression performs L1 regularization by adding a factor of the sum of the\\nabsolute value of coefficients in the cost function (RSS) for linear regression, as\\nmentioned in Equation 4-1. The equation for lasso regularization can be repre‐\\nsented as follows:\\n\\nCostFunction = RSS + λ * ∑ j=1\\n\\np\\n\\n|βj|\\n\\nL1 regularization can lead to zero coefficients (i.e., some of the features are com‐\\npletely neglected for the evaluation of output). The larger the value of λ, the more\\nfeatures are shrunk to zero. This can eliminate some features entirely and give us\\na subset of predictors, reducing model complexity. So Lasso regression not only\\nhelps in reducing overfitting, but also can help in feature selection. Predictors not\\nshrunk  toward  zero  signify  that  they  are  important,  and  thus  L1  regularization\\nallows  for  feature  selection  (sparse  selection).  The  regularization  parameter  (λ)\\ncan be controlled, and a lambda value of zero produces the basic linear regression\\nequation.\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n55\\n\\n\\x0cA lasso regression model can be constructed using the Lasso class of the sklearn\\npackage of Python, as shown in the code snippet that follows:\\n\\nfrom sklearn.linear_model import Lasso\\nmodel = Lasso()\\nmodel.fit(X, Y)\\n\\nL2 regularization or Ridge regression\\n\\nRidge regression performs L2 regularization by adding a factor of the sum of the\\nsquare  of  coefficients  in  the  cost  function  (RSS)  for  linear  regression,  as  men‐\\ntioned in Equation 4-1. The equation for ridge regularization can be represented\\nas follows:\\n\\nCostFunction = RSS + λ * ∑ j=1\\n\\n2\\n\\np β j\\n\\nRidge regression puts constraint on the coefficients. The penalty term (λ) regu‐\\nlarizes the coefficients such that if the coefficients take large values, the optimiza‐\\ntion function is penalized. So ridge regression shrinks the coefficients and helps\\nto reduce the model complexity. Shrinking the coefficients leads to a lower var‐\\niance and a lower error value. Therefore, ridge regression decreases the complex‐\\nity of a model but does not reduce the number of variables; it just shrinks their\\neffect. When λ is closer to zero, the cost function becomes similar to the linear\\nregression cost function. So the lower the constraint (low λ) on the features, the\\nmore the model will resemble the linear regression model.\\n\\nA ridge regression model can be constructed using the Ridge class of the sklearn\\npackage of Python, as shown in the code snippet that follows:\\n\\nfrom sklearn.linear_model import Ridge\\nmodel = Ridge()\\nmodel.fit(X, Y)\\n\\nElastic net\\n\\nElastic  nets  add  regularization  terms  to  the  model,  which  are  a  combination  of\\nboth L1 and L2 regularization, as shown in the following equation:\\n\\nCostFunction = RSS + λ * ((1 – α) / 2 * ∑ j=1\\n\\np β j\\n\\n2 + α * ∑ j =1\\n\\np\\n\\n|βj|)\\n\\nIn addition to setting and choosing a λ value, an elastic net also allows us to tune\\nthe alpha parameter, where α = 0 corresponds to ridge and α = 1 to lasso. There‐\\nfore, we can choose an α value between 0 and 1 to optimize the elastic net. Effec‐\\ntively, this will shrink some coefficients and set some to 0 for sparse selection.\\n\\nAn elastic net regression model can be constructed using the ElasticNet class of\\nthe sklearn package of Python, as shown in the following code snippet:\\n\\nfrom sklearn.linear_model import ElasticNet\\nmodel = ElasticNet()\\nmodel.fit(X, Y)\\n\\n56 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cFor all the regularized regression, λ is the key parameter to tune during grid search in\\nPython. In an elastic net, α can be an additional parameter to tune.\\n\\nLogistic Regression\\nLogistic  regression  is  one  of  the  most  widely  used  algorithms  for  classification.  The\\nlogistic regression model arises from the desire to model the probabilities of the out‐\\nput classes given a function that is linear in x, at the same time ensuring that output\\nprobabilities  sum  up  to  one  and  remain  between  zero  and  one  as  we  would  expect\\nfrom probabilities.\\n\\nIf we train a linear regression model on several examples where Y = 0 or 1, we might\\nend  up  predicting  some  probabilities  that  are  less  than  zero  or  greater  than  one,\\nwhich  doesn’t  make  sense.  Instead,  we  use  a  logistic  regression  model  (or  logit\\nmodel), which is a modification of linear regression that makes sure to output a prob‐\\nability between zero and one by applying the sigmoid function.2\\n\\nEquation  4-2  shows  the  equation  for  a  logistic  regression  model.  Similar  to  linear\\nregression, input values (x) are combined linearly using weights or coefficient values\\nto predict an output value (y). The output coming from Equation 4-2 is a probability\\nthat is transformed into a binary value (0 or 1) to get the model prediction.\\n\\nEquation 4-2. Logistic regression equation\\n\\ny =\\n\\nexp (β0 + β1x1 + .... + βi x1)\\n1 + exp (β0 + β1x1 + .... + βi x1)\\n\\nWhere y is the predicted output, β0 is the bias or intercept term and B1 is the coeffi‐\\ncient for the single input value (x). Each column in the input data has an associated β\\ncoefficient (a constant real value) that must be learned from the training data.\\n\\nIn  logistic  regression,  the  cost  function  is  basically  a  measure  of  how  often  we\\npredicted  one  when  the  true  answer  was  zero,  or  vice  versa.  Training  the  logistic\\nregression coefficients is done using techniques such as maximum likelihood estima‐\\ntion (MLE) to predict values close to 1 for the default class and close to 0 for the other\\nclass.3\\n\\n2 See the activation function section of Chapter 3 for details on the sigmoid function.\\n\\n3 MLE is a method of estimating the parameters of a probability distribution so that under the assumed statisti‐\\n\\ncal model the observed data is most probable.\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n57\\n\\n\\x0cA logistic regression model can be constructed using the LogisticRegression class\\nof the sklearn package of Python, as shown in the following code snippet:\\n\\nfrom sklearn.linear_model import LogisticRegression\\nmodel = LogisticRegression()\\nmodel.fit(X, Y)\\n\\nHyperparameters\\n\\nRegularization (penalty in sklearn)\\n\\nSimilar  to  linear  regression,  logistic  regression  can  have  regularization,  which\\ncan be L1, L2, or elasticnet. The values in the sklearn library are [l1, l2, elasticnet].\\n\\nRegularization strength (C in sklearn)\\n\\nThis parameter controls the regularization strength. Good values of the penalty\\nparameters can be [100, 10, 1.0, 0.1, 0.01].\\n\\nAdvantages and disadvantages\\n\\nIn  terms  of  the  advantages,  the  logistic  regression  model  is  easy  to  implement,  has\\ngood interpretability, and performs very well on linearly separable classes. The out‐\\nput  of  the  model  is  a  probability,  which  provides  more  insight  and  can  be  used  for\\nranking.  The  model  has  small  number  of  hyperparameters.  Although  there  may  be\\nrisk  of  overfitting,  this  may  be  addressed  using  L1/L2  regularization,  similar  to  the\\nway we addressed overfitting for the linear regression models.\\n\\nIn terms of disadvantages, the model may overfit when provided with large numbers\\nof features. Logistic regression can only learn linear functions and is less suitable to\\ncomplex relationships between features and the target variable. Also, it may not han‐\\ndle irrelevant features well, especially if the features are strongly correlated.\\n\\nSupport Vector Machine\\nThe objective of the support vector machine (SVM) algorithm is to maximize the mar‐\\ngin  (shown  as  shaded  area  in  Figure  4-3),  which  is  defined  as  the  distance  between\\nthe separating hyperplane (or decision boundary) and the training samples that are\\nclosest to this hyperplane, the so-called support vectors. The margin is calculated as\\nthe  perpendicular  distance  from  the  line  to  only  the  closest  points,  as  shown  in\\nFigure  4-3.  Hence,  SVM  calculates  a  maximum-margin  boundary  that  leads  to  a\\nhomogeneous partition of all data points.\\n\\n58 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cFigure 4-3. Support vector machine\\n\\nIn  practice,  the  data  is  messy  and  cannot  be  separated  perfectly  with  a  hyperplane.\\nThe constraint of maximizing the margin of the line that separates the classes must be\\nrelaxed. This change allows some points in the training data to violate the separating\\nline. An additional set of coefficients is introduced that give the margin wiggle room\\nin  each  dimension.  A  tuning  parameter  is  introduced,  simply  called  C,  that  defines\\nthe magnitude of the wiggle allowed across all dimensions. The larger the value of C,\\nthe more violations of the hyperplane are permitted.\\n\\nIn some cases, it is not possible to find a hyperplane or a linear decision boundary,\\nand kernels are used. A kernel is just a transformation of the input data that allows\\nthe SVM algorithm to treat/process the data more easily. Using kernels, the original\\ndata is projected into a higher dimension to classify the data better.\\n\\nSVM is used for both classification and regression. We achieve this by converting the\\noriginal  optimization  problem  into  a  dual  problem.  For  regression,  the  trick  is  to\\nreverse  the  objective.  Instead  of  trying  to  fit  the  largest  possible  street  between  two\\nclasses while limiting margin violations, SVM regression tries to fit as many instances\\nas possible on the street (shaded area in Figure 4-3) while limiting margin violations.\\nThe width of the street is controlled by a hyperparameter.\\n\\nThe SVM regression and classification models can be constructed using the sklearn\\npackage of Python, as shown in the following code snippets:\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n59\\n\\n\\x0cRegression\\n\\nfrom sklearn.svm import SVR\\nmodel = SVR()\\nmodel.fit(X, Y)\\n\\nClassification\\n\\nfrom sklearn.svm import SVC\\nmodel = SVC()\\nmodel.fit(X, Y)\\n\\nHyperparameters\\n\\nThe following key parameters are present in the sklearn implementation of SVM and\\ncan be tweaked while performing the grid search:\\n\\nKernels (kernel in sklearn)\\n\\nThe  choice  of  kernel  controls  the  manner  in  which  the  input  variables  will  be\\nprojected.  There  are  many  kernels  to  choose  from,  but  linear  and  RBF  are  the\\nmost common.\\n\\nPenalty (C in sklearn)\\n\\nThe penalty parameter tells the SVM optimization how much you want to avoid\\nmisclassifying each training example. For large values of the penalty parameter,\\nthe optimization will choose a smaller-margin hyperplane. Good values might be\\na log scale from 10 to 1,000.\\n\\nAdvantages and disadvantages\\n\\nIn terms of advantages, SVM is fairly robust against overfitting, especially in higher\\ndimensional space. It handles the nonlinear relationships quite well, with many ker‐\\nnels to choose from. Also, there is no distributional requirement for the data.\\n\\nIn terms of disadvantages, SVM can be inefficient to train and memory-intensive to\\nrun and tune. It doesn’t perform well with large datasets. It requires the feature scal‐\\ning of the data. There are also many hyperparameters, and their meanings are often\\nnot intuitive.\\n\\nK-Nearest Neighbors\\nK-nearest  neighbors  (KNN)  is  considered  a  “lazy  learner,”  as  there  is  no  learning\\nrequired  in  the  model.  For  a  new  data  point,  predictions  are  made  by  searching\\nthrough the entire training set for the K most similar instances (the neighbors) and\\nsummarizing the output variable for those K instances.\\n\\nTo determine which of the K instances in the training dataset are most similar to a\\nnew input, a distance measure is used. The most popular distance measure is Eucli‐\\n\\n60 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cdean distance, which is calculated as the square root of the sum of the squared differ‐\\nences  between  a  point  a  and  a  point  b  across  all  input  attributes  i,  and  which  is\\n(ai – bi)2. Euclidean distance is a good distance meas‐\\nrepresented as d(a, b) = ∑i =1\\nure to use if the input variables are similar in type.\\n\\nn\\n\\nAnother distance metric is Manhattan distance, in which the distance between point\\nn\\na and point b is represented as d(a, b) = ∑i=1\\n| ai – bi | . Manhattan distance is a good\\nmeasure to use if the input variables are not similar in type.\\n\\nThe steps of KNN can be summarized as follows:\\n\\n1. Choose the number of K and a distance metric.\\n\\n2. Find the K-nearest neighbors of the sample that we want to classify.\\n\\n3. Assign the class label by majority vote.\\n\\nKNN regression and classification models can be constructed using the sklearn pack‐\\nage of Python, as shown in the following code:\\n\\nClassification\\n\\nfrom sklearn.neighbors import KNeighborsClassifier\\nmodel = KNeighborsClassifier()\\nmodel.fit(X, Y)\\n\\nRegression\\n\\nfrom sklearn.neighbors import KNeighborsRegressor\\nmodel = KNeighborsRegressor()\\nmodel.fit(X, Y)\\n\\nHyperparameters\\n\\nThe following key parameters are present in the sklearn implementation of KNN and\\ncan be tweaked while performing the grid search:\\n\\nNumber of neighbors (n_neighbors in sklearn)\\n\\nThe  most  important  hyperparameter  for  KNN  is  the  number  of  neighbors\\n(n_neighbors). Good values are between 1 and 20.\\n\\nDistance metric (metric in sklearn)\\n\\nIt may also be interesting to test different distance metrics for choosing the com‐\\nposition of the neighborhood. Good values are euclidean and manhattan.\\n\\nAdvantages and disadvantages\\n\\nIn terms of advantages, no training is involved and hence there is no learning phase.\\nSince the algorithm requires no training before making predictions, new data can be\\nadded seamlessly without impacting the accuracy of the algorithm. It is intuitive and\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n61\\n\\n\\x0ceasy  to  understand.  The  model  naturally  handles  multiclass  classification  and  can\\nlearn complex decision boundaries. KNN is effective if the training data is large. It is\\nalso robust to noisy data, and there is no need to filter the outliers.\\n\\nIn terms of the disadvantages, the distance metric to choose is not obvious and diffi‐\\ncult to justify in many cases. KNN performs poorly on high dimensional datasets. It\\nis expensive and slow to predict new instances because the distance to all neighbors\\nmust be recalculated. KNN is sensitive to noise in the dataset. We need to manually\\ninput missing values and remove outliers. Also, feature scaling (standardization and\\nnormalization) is required before applying the KNN algorithm to any dataset; other‐\\nwise, KNN may generate wrong predictions.\\n\\nLinear Discriminant Analysis\\nThe  objective  of  the  linear  discriminant  analysis  (LDA)  algorithm  is  to  project  the\\ndata onto a lower-dimensional space in a way that the class separability is maximized\\nand the variance within a class is minimized.4\\n\\nDuring the training of the LDA model, the statistical properties (i.e., mean and cova‐\\nriance matrix) of each class are computed. The statistical properties are estimated on\\nthe basis of the following assumptions about the data:\\n\\n• Data  is  normally  distributed,  so  that  each  variable  is  shaped  like  a  bell  curve\\n\\nwhen plotted.\\n\\n• Each attribute has the same variance, and the values of each variable vary around\\n\\nthe mean by the same amount on average.\\n\\nTo make a prediction, LDA estimates the probability that a new set of inputs belongs\\nto every class. The output class is the one that has the highest probability.\\n\\nImplementation in Python and hyperparameters\\n\\nThe  LDA  classification  model  can  be  constructed  using  the  sklearn  package  of\\nPython, as shown in the following code snippet:\\n\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\nmodel = LinearDiscriminantAnalysis()\\nmodel.fit(X, Y)\\n\\nThe  key  hyperparameter  for  the  LDA  model  is  number  of  components  for  dimen‐\\nsionality reduction, which is represented by n_components in sklearn.\\n\\n4 The approach of projecting data is similar to the PCA algorithm discussed in Chapter 7.\\n\\n62 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cAdvantages and disadvantages\\n\\nIn  terms  of  advantages,  LDA  is  a  relatively  simple  model  with  fast  implementation\\nand  is  easy  to  implement.  In  terms  of  disadvantages,  it  requires  feature  scaling  and\\ninvolves complex matrix operations.\\n\\nClassification and Regression Trees\\nIn the most general terms, the purpose of an analysis via tree-building algorithms is\\nto determine a set of if–then logical (split) conditions that permit accurate prediction\\nor classification of cases. Classification and regression trees (or CART or decision tree\\nclassifiers) are attractive models if we care about interpretability. We can think of this\\nmodel as breaking down our data and making a decision based on asking a series of\\nquestions.  This  algorithm  is  the  foundation  of  ensemble  methods  such  as  random\\nforest and gradient boosting method.\\n\\nRepresentation\\n\\nThe model can be represented by a binary tree (or decision tree), where each node is\\nan input variable x with a split point and each leaf contains an output variable y for\\nprediction.\\n\\nFigure 4-4 shows an example of a simple classification tree to predict whether a per‐\\nson is a male or a female based on two inputs of height (in centimeters) and weight\\n(in kilograms).\\n\\nFigure 4-4. Classification and regression tree example\\n\\nLearning a CART model\\n\\nCreating a binary tree is actually a process of dividing up the input space. A greedy\\napproach called recursive binary splitting is used to divide the space. This is a numeri‐\\ncal procedure in which all the values are lined up and different split points are tried\\nand  tested  using  a  cost  (loss)  function.  The  split  with  the  best  cost  (lowest  cost,\\nbecause we minimize cost) is selected. All input variables and all possible split points\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n63\\n\\n\\x0care evaluated and chosen in a greedy manner (e.g., the very best split point is chosen\\neach time).\\n\\nFor regression predictive modeling problems, the cost function that is minimized to\\nchoose  split  points  is  the  sum  of  squared  errors  across  all  training  samples  that  fall\\nwithin the rectangle:\\n\\nn\\n∑i=1\\n\\n(yi – predictioni)2\\n\\nwhere  yi is the output for the training sample and prediction is the predicted output\\nfor the rectangle. For classification, the Gini cost function is used; it provides an indi‐\\ncation of how pure the leaf nodes are (i.e., how mixed the training data assigned to\\neach node is) and is defined as:\\n\\nG = ∑i=1\\n\\nn pk * (1 – pk)\\n\\nwhere  G  is  the  Gini  cost  over  all  classes  and  pk   is  the  number  of  training  instances\\nwith class k in the rectangle of interest. A node that has all classes of the same type\\n(perfect class purity) will have G = 0, while a node that has a 50–50 split of classes for\\na binary classification problem (worst purity) will have G = 0.5.\\n\\nStopping criterion\\n\\nThe recursive binary splitting procedure described in the preceding section needs to\\nknow when to stop splitting as it works its way down the tree with the training data.\\nThe most common stopping procedure is to use a minimum count on the number of\\ntraining instances assigned to each leaf node. If the count is less than some minimum,\\nthen the split is not accepted and the node is taken as a final leaf node.\\n\\nPruning the tree\\n\\nThe stopping criterion is important as it strongly influences the performance of the\\ntree. Pruning can be used after learning the tree to further lift performance. The com‐\\nplexity of a decision tree is defined as the number of splits in the tree. Simpler trees\\nare preferred as they are faster to run and easy to understand, consume less memory\\nduring processing and storage, and are less likely to overfit the data. The fastest and\\nsimplest pruning method is to work through each leaf node in the tree and evaluate\\nthe  effect  of  removing  it  using  a  test  set.  A  leaf  node  is  removed  only  if  doing  so\\nresults  in  a  drop  in  the  overall  cost  function  on  the  entire  test  set.  The  removal  of\\nnodes can be stopped when no further improvements can be made.\\n\\n64 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cImplementation in Python\\n\\nCART  regression  and  classification  models  can  be  constructed  using  the  sklearn\\npackage of Python, as shown in the following code snippet:\\n\\nClassification\\n\\nfrom sklearn.tree import DecisionTreeClassifier\\nmodel = DecisionTreeClassifier()\\nmodel.fit(X, Y)\\n\\nRegression\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\nmodel = DecisionTreeRegressor ()\\nmodel.fit(X, Y)\\n\\nHyperparameters\\n\\nCART  has  many  hyperparameters.  However,  the  key  hyperparameter  is  the  maxi‐\\nmum depth of the tree model, which is the number of components for dimensional‐\\nity reduction, and which is represented by  max_depth in the sklearn package. Good\\nvalues can range from 2 to 30 depending on the number of features in the data.\\n\\nAdvantages and disadvantages\\n\\nIn  terms  of  advantages,  CART  is  easy  to  interpret  and  can  adapt  to  learn  complex\\nrelationships. It requires little data preparation, and data typically does not need to be\\nscaled. Feature importance is built in due to the way decision nodes are built. It per‐\\nforms well on large datasets. It works for both regression and classification problems.\\n\\nIn terms of disadvantages, CART is prone to overfitting unless pruning is used. It can\\nbe  very  nonrobust,  meaning  that  small  changes  in  the  training  dataset  can  lead  to\\nquite major differences in the hypothesis function that gets learned. CART generally\\nhas worse performance than ensemble models, which are covered next.\\n\\nEnsemble Models\\nThe goal of ensemble models is to combine different classifiers into a meta-classifier\\nthat has better generalization performance than each individual classifier alone. For\\nexample, assuming that we collected predictions from 10 experts, ensemble methods\\nwould allow us to strategically combine their predictions to come up with a predic‐\\ntion that is more accurate and robust than the experts’ individual predictions.\\n\\nThe two most popular ensemble methods are bagging and boosting. Bagging (or boot‐\\nstrap aggregation) is an ensemble technique of training several individual models in a\\nparallel way. Each model is trained by a random subset of the data. Boosting, on the\\nother  hand,  is  an  ensemble  technique  of  training  several  individual  models  in  a\\nsequential  way.  This  is  done  by  building  a  model  from  the  training  data  and  then\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n65\\n\\n\\x0ccreating a second model that attempts to correct the errors of the first model. Models\\nare added until the training set is predicted perfectly or a maximum number of mod‐\\nels  is  added.  Each  individual  model  learns  from  mistakes  made  by  the  previous\\nmodel. Just like the decision trees themselves, bagging and boosting can be used for\\nclassification and regression problems.\\n\\nBy combining individual models, the ensemble model tends to be more flexible (less\\nbias)  and  less  data-sensitive  (less  variance).5  Ensemble  methods  combine  multiple,\\nsimpler algorithms to obtain better performance.\\n\\nIn this section we will cover random forest, AdaBoost, the gradient boosting method,\\nand extra trees, along with their implementation using sklearn package.\\n\\nRandom forest.    Random forest is a tweaked version of bagged decision trees. In order\\nto  understand  a  random  forest  algorithm,  let  us  first  understand  the  bagging  algo‐\\nrithm.  Assuming  we  have  a  dataset  of  one  thousand  instances,  the  steps  of  bagging\\nare:\\n\\n1. Create many (e.g., one hundred) random subsamples of our dataset.\\n\\n2. Train a CART model on each sample.\\n\\n3. Given  a  new  dataset,  calculate  the  average  prediction  from  each  model  and\\naggregate the prediction by each tree to assign the final label by majority vote.\\n\\nA  problem  with  decision  trees  like  CART  is  that  they  are  greedy.  They  choose  the\\nvariable to split by using a greedy algorithm that minimizes error. Even after bagging,\\nthe decision trees can have a lot of structural similarities and result in high correla‐\\ntion in their predictions. Combining predictions from multiple models in ensembles\\nworks  better  if  the  predictions  from  the  submodels  are  uncorrelated,  or  at  best  are\\nweakly correlated. Random forest changes the learning algorithm in such a way that\\nthe resulting predictions from all of the subtrees have less correlation.\\n\\nIn  CART,  when  selecting  a  split  point,  the  learning  algorithm  is  allowed  to  look\\nthrough all variables and all variable values in order to select the most optimal split\\npoint.  The  random  forest  algorithm  changes  this  procedure  such  that  each  subtree\\ncan  access  only  a  random  sample  of  features  when  selecting  the  split  points.  The\\nnumber of features that can be searched at each split point (m) must be specified as a\\nparameter to the algorithm.\\n\\nAs  the  bagged  decision  trees  are  constructed,  we  can  calculate  how  much  the  error\\nfunction drops for a variable at each split point. In regression problems, this may be\\nthe drop in sum squared error, and in classification, this might be the Gini cost. The\\n\\n5 Bias and variance are described in detail later in this chapter.\\n\\n66 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cbagged  method  can  provide  feature  importance  by  calculating  and  averaging  the\\nerror function drop for individual variables.\\n\\nImplementation in Python.    Random forest regression and classification models can be\\nconstructed using the sklearn package of Python, as shown in the following code:\\n\\nClassification\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\nmodel = RandomForestClassifier()\\nmodel.fit(X, Y)\\n\\nRegression\\n\\nfrom sklearn.ensemble import RandomForestRegressor\\nmodel = RandomForestRegressor()\\nmodel.fit(X, Y)\\n\\nHyperparameters.    Some of the main hyperparameters that are present in the sklearn\\nimplementation of random forest and that can be tweaked while performing the grid\\nsearch are:\\n\\nMaximum number of features (max_features in sklearn)\\n\\nThis  is  the  most  important  parameter.  It  is  the  number  of  random  features  to\\nsample at each split point. You could try a range of integer values, such as 1 to 20,\\nor 1 to half the number of input features.\\n\\nNumber of estimators (n_estimators in sklearn)\\n\\nThis parameter represents the number of trees. Ideally, this should be increased\\nuntil no further improvement is seen in the model. Good values might be a log\\nscale from 10 to 1,000.\\n\\nAdvantages and disadvantages.     The  random  forest  algorithm  (or  model)  has  gained\\nhuge  popularity  in  ML  applications  during  the  last  decade  due  to  its  good  perfor‐\\nmance, scalability, and ease of use. It is flexible and naturally assigns feature impor‐\\ntance  scores,  so  it  can  handle  redundant  feature  columns.  It  scales  to  large  datasets\\nand  is  generally  robust  to  overfitting.  The  algorithm  doesn’t  need  the  data  to  be\\nscaled and can model a nonlinear relationship.\\n\\nIn  terms  of  disadvantages,  random  forest  can  feel  like  a  black  box  approach,  as  we\\nhave very little control over what the model does, and the results may be difficult to\\ninterpret.  Although  random  forest  does  a  good  job  at  classification,  it  may  not  be\\ngood for regression problems, as it does not give a precise continuous nature predic‐\\ntion. In the case of regression, it doesn’t predict beyond the range in the training data\\nand may overfit datasets that are particularly noisy.\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n67\\n\\n\\x0cExtra trees\\n\\nExtra trees, otherwise known as extremely randomized trees, is a variant of a random\\nforest; it builds multiple trees and splits nodes using random subsets of features simi‐\\nlar to random forest. However, unlike random forest, where observations are drawn\\nwith replacement, the observations are drawn without replacement in extra trees. So\\nthere is no repetition of observations.\\n\\nAdditionally, random forest selects the best split to convert the parent into the two\\nmost homogeneous child nodes.6 However, extra trees selects a random split to divide\\nthe  parent  node  into  two  random  child  nodes.  In  extra  trees,  randomness  doesn’t\\ncome  from  bootstrapping  the  data;  it  comes  from  the  random  splits  of  all\\nobservations.\\n\\nIn real-world cases, performance is comparable to an ordinary random forest, some‐\\ntimes a bit better. The advantages and disadvantages of extra trees are similar to those\\nof random forest.\\n\\nImplementation in Python.    Extra trees regression and classification models can be con‐\\nstructed using the sklearn package of Python, as shown in the following code snippet.\\nThe hyperparameters of extra trees are similar to random forest, as shown in the pre‐\\nvious section:\\n\\nClassification\\n\\nfrom sklearn.ensemble import ExtraTreesClassifier\\nmodel = ExtraTreesClassifier()\\nmodel.fit(X, Y)\\n\\nRegression\\n\\nfrom sklearn.ensemble import ExtraTreesRegressor\\nmodel = ExtraTreesRegressor()\\nmodel.fit(X, Y)\\n\\nAdaptive Boosting (AdaBoost)\\n\\nAdaptive Boosting or AdaBoost is a boosting technique in which the basic idea is to\\ntry predictors sequentially, and each subsequent model attempts to fix the errors of\\nits predecessor. At each iteration, the AdaBoost algorithm changes the sample distri‐\\nbution  by  modifying  the  weights  attached  to  each  of  the  instances.  It  increases  the\\nweights  of  the  wrongly  predicted  instances  and  decreases  the  ones  of  the  correctly\\npredicted instances.\\n\\n6 Split is the process of converting a nonhomogeneous parent node into two homogeneous child nodes (best\\n\\npossible).\\n\\n68 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cThe steps of the AdaBoost algorithm are:\\n\\n1. Initially, all observations are given equal weights.\\n\\n2. A model is built on a subset of data, and using this model, predictions are made\\non  the  whole  dataset.  Errors  are  calculated  by  comparing  the  predictions  and\\nactual values.\\n\\n3. While creating the next model, higher weights are given to the data points that\\nwere predicted incorrectly. Weights can be determined using the error value. For\\ninstance, the higher the error, the more weight is assigned to the observation.\\n\\n4. This  process  is  repeated  until  the  error  function  does  not  change,  or  until  the\\n\\nmaximum limit of the number of estimators is reached.\\n\\nImplementation in Python.    AdaBoost regression and classification models can be con‐\\nstructed using the sklearn package of Python, as shown in the following code snippet:\\n\\nClassification\\n\\nfrom sklearn.ensemble import AdaBoostClassifier\\nmodel = AdaBoostClassifier()\\nmodel.fit(X, Y)\\n\\nRegression\\n\\nfrom sklearn.ensemble import AdaBoostRegressor\\nmodel = AdaBoostRegressor()\\nmodel.fit(X, Y)\\n\\nHyperparameters.    Some of the main hyperparameters that are present in the sklearn\\nimplementation  of  AdaBoost  and  that  can  be  tweaked  while  performing  the  grid\\nsearch are as follows:\\n\\nLearning rate (learning_rate in sklearn)\\n\\nLearning rate shrinks the contribution of each classifier/regressor. It can be con‐\\nsidered on a log scale. The sample values for grid search can be 0.001, 0.01, and\\n0.1.\\n\\nNumber of estimators (n_estimators in sklearn)\\n\\nThis parameter represents the number of trees. Ideally, this should be increased\\nuntil no further improvement is seen in the model. Good values might be a log\\nscale from 10 to 1,000.\\n\\nAdvantages and disadvantages.    In terms of advantages, AdaBoost has a high degree of\\nprecision.  AdaBoost  can  achieve  similar  results  to  other  models  with  much  less\\ntweaking of parameters or settings. The algorithm doesn’t need the data to be scaled\\nand can model a nonlinear relationship.\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n69\\n\\n\\x0cIn terms of disadvantages, the training of AdaBoost is time consuming. AdaBoost can\\nbe sensitive to noisy data and outliers, and data imbalance leads to a decrease in clas‐\\nsification accuracy\\n\\nGradient boosting method\\n\\nGradient boosting method (GBM) is another boosting technique similar to AdaBoost,\\nwhere the general idea is to try predictors sequentially. Gradient boosting works by\\nsequentially  adding  the  previous  underfitted  predictions  to  the  ensemble,  ensuring\\nthe errors made previously are corrected.\\n\\nThe following are the steps of the gradient boosting algorithm:\\n\\n1. A model (which can be referred to as the first weak learner) is built on a subset of\\n\\ndata. Using this model, predictions are made on the whole dataset.\\n\\n2. Errors are calculated by comparing the predictions and actual values, and the loss\\n\\nis calculated using the loss function.\\n\\n3. A new model is created using the errors of the previous step as the target vari‐\\nable. The objective is to find the best split in the data to minimize the error. The\\npredictions  made  by  this  new  model  are  combined  with  the  predictions  of  the\\nprevious. New errors are calculated using this predicted value and actual value.\\n\\n4. This  process  is  repeated  until  the  error  function  does  not  change  or  until  the\\n\\nmaximum limit of the number of estimators is reached.\\n\\nContrary  to  AdaBoost,  which  tweaks  the  instance  weights  at  every  interaction,  this\\nmethod  tries  to  fit  the  new  predictor  to  the  residual  errors  made  by  the  previous\\npredictor.\\n\\nImplementation in Python and hyperparameters.     Gradient  boosting  method  regression\\nand classification models can be constructed using the sklearn package of Python, as\\nshown  in  the  following  code  snippet.  The  hyperparameters  of  gradient  boosting\\nmethod are similar to AdaBoost, as shown in the previous section:\\n\\nClassification\\n\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nmodel = GradientBoostingClassifier()\\nmodel.fit(X, Y)\\n\\nRegression\\n\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nmodel = GradientBoostingRegressor()\\nmodel.fit(X, Y)\\n\\n70 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cAdvantages and disadvantages.     In  terms  of  advantages,  gradient  boosting  method  is\\nrobust to missing data, highly correlated features, and irrelevant features in the same\\nway  as  random  forest.  It  naturally  assigns  feature  importance  scores,  with  slightly\\nbetter  performance  than  random  forest.  The  algorithm  doesn’t  need  the  data  to  be\\nscaled and can model a nonlinear relationship.\\n\\nIn terms of disadvantages, it may be more prone to overfitting than random forest, as\\nthe main purpose of the boosting approach is to reduce bias and not variance. It has\\nmany hyperparameters to tune, so model development may not be as fast. Also, fea‐\\nture importance may not be robust to variation in the training dataset.\\n\\nANN-Based Models\\nIn  Chapter  3  we  covered  the  basics  of  ANNs,  along  with  the  architecture  of  ANNs\\nand their training and implementation in Python. The details provided in that chap‐\\nter are applicable across all areas of machine learning, including supervised learning.\\nHowever, there are a few additional details from the supervised learning perspective,\\nwhich we will cover in this section.\\n\\nNeural networks are reducible to a classification or regression model with the activa‐\\ntion function of the node in the output layer. In the case of a regression problem, the\\noutput node has linear activation function (or no activation function). A linear func‐\\ntion  produces  a  continuous  output  ranging  from  -inf  to  +inf.  Hence,  the  output\\nlayer will be the linear function of the nodes in the layer before the output layer, and\\nit will be a regression-based model.\\n\\nIn the case of a classification problem, the output node has a sigmoid or softmax acti‐\\nvation  function.  A  sigmoid  or  softmax  function  produces  an  output  ranging  from\\nzero to one to represent the probability of target value. Softmax function can also be\\nused for multiple groups for classification.\\n\\nANN using sklearn\\n\\nANN regression and classification models can be constructed using the sklearn pack‐\\nage of Python, as shown in the following code snippet:\\n\\nClassification\\n\\nfrom sklearn.neural_network import MLPClassifier\\nmodel = MLPClassifier()\\nmodel.fit(X, Y)\\n\\nRegression\\n\\nfrom sklearn.neural_network import MLPRegressor\\nmodel = MLPRegressor()\\nmodel.fit(X, Y)\\n\\nSupervised Learning Models: An Overview \\n\\n| \\n\\n71\\n\\n\\x0cHyperparameters\\n\\nAs we saw in Chapter 3, ANN has many hyperparameters. Some of the hyperparame‐\\nters that are present in the sklearn implementation of ANN and can be tweaked while\\nperforming the grid search are:\\n\\nHidden Layers (hidden_layer_sizes in sklearn)\\n\\nIt represents the number of layers and nodes in the ANN architecture. In sklearn\\nimplementation  of  ANN,  the  ith  element  represents  the  number  of  neurons  in\\nthe ith hidden layer. A sample value for grid search in the sklearn implementa‐\\ntion can be [(20,), (50,), (20, 20), (20, 30, 20)].\\n\\nActivation Function (activation in sklearn)\\n\\nIt  represents  the  activation  function  of  a  hidden  layer.  Some  of  the  activation\\nfunctions defined in Chapter 3, such as sigmoid, relu, or tanh, can be used.\\n\\nDeep neural network\\n\\nANNs with more than a single hidden layer are often called deep networks. We pre‐\\nfer  using  the  library  Keras  to  implement  such  networks,  given  the  flexibility  of  the\\nlibrary. The detailed implementation of a deep neural network in Keras was shown in\\nChapter 3. Similar to MLPClassifier and MLPRegressor in sklearn for classification\\nand regression, Keras has modules called KerasClassifier and KerasRegressor that\\ncan be used for creating classification and regression models with deep network.\\n\\nA popular problem in finance is time series prediction, which is predicting the next\\nvalue  of  a  time  series  based  on  a  historical  overview.  Some  of  the  deep  neural  net‐\\nworks, such as recurrent neural network (RNN), can be directly used for time series\\nprediction. The details of this approach are provided in Chapter 5.\\n\\nAdvantages and disadvantages\\n\\nThe main advantage of an ANN is that it captures the nonlinear relationship between\\nthe variables quite well. ANN can more easily learn rich representations and is good\\nwith a large number of input features with a large dataset. ANN is flexible in how it\\ncan  be  used.  This  is  evident  from  its  use  across  a  wide  variety  of  areas  in  machine\\nlearning  and  AI,  including  reinforcement  learning  and  NLP,  as  discussed  in\\nChapter 3.\\n\\nThe main disadvantage of ANN is the interpretability of the model, which is a draw‐\\nback  that  often  cannot  be  ignored  and  is  sometimes  the  determining  factor  when\\nchoosing a model. ANN is not good with small datasets and requires a lot of tweaking\\nand  guesswork.  Choosing  the  right  topology/algorithms  to  solve  a  problem  is  diffi‐\\ncult. Also, ANN is computationally expensive and can take a lot of time to train.\\n\\n72 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cUsing ANNs for supervised learning in finance\\n\\nIf  a  simple  model  such  as  linear  or  logistic  regression  perfectly  fits  your  problem,\\ndon’t bother with ANN. However, if you are modeling a complex dataset and feel a\\nneed  for  better  prediction  power,  give  ANN  a  try.  ANN  is  one  of  the  most  flexible\\nmodels in adapting itself to the shape of the data, and using it for supervised learning\\nproblems can be an interesting and valuable exercise.\\n\\nModel Performance\\nIn the previous section, we discussed grid search as a way to find the right hyperpara‐\\nmeter to achieve better performance. In this section, we will expand on that process\\nby  discussing  the  key  components  of  evaluating  the  model  performance,  which  are\\noverfitting, cross validation, and evaluation metrics.\\n\\nOverfitting and Underfitting\\nA common problem in machine learning is overfitting, which is defined by learning a\\nfunction  that  perfectly  explains  the  training  data  that  the  model  learned  from  but\\ndoesn’t generalize well to unseen test data. Overfitting happens when a model over‐\\nlearns from the training data to the point that it starts picking up idiosyncrasies that\\naren’t representative of patterns in the real world. This becomes especially problem‐\\natic as we make our models increasingly more complex. Underfitting is a related issue\\nin  which  the  model  is  not  complex  enough  to  capture  the  underlying  trend  in  the\\ndata.  Figure  4-5  illustrates  overfitting  and  underfitting.  The  left-hand  panel  of\\nFigure 4-5 shows a linear regression model; a straight line clearly underfits the true\\nfunction.  The  middle  panel  shows  that  a  high  degree  polynomial  approximates  the\\ntrue  relationship  reasonably  well.  On  the  other  hand,  a  polynomial  of  a  very  high\\ndegree fits the small sample almost perfectly, and performs best on the training data,\\nbut  this  doesn’t  generalize,  and  it  would  do  a  horrible  job  at  explaining  a  new  data\\npoint.\\n\\nThe concepts of overfitting and underfitting are closely linked to bias-variance trade-\\noff. Bias refers to the error due to overly simplistic assumptions or faulty assumptions\\nin the learning algorithm. Bias results in underfitting of the data, as shown in the left-\\nhand  panel  of  Figure  4-5.  A  high  bias  means  our  learning  algorithm  is  missing\\nimportant  trends  among  the  features.  Variance  refers  to  the  error  due  to  an  overly\\ncomplex  model  that  tries  to  fit  the  training  data  as  closely  as  possible.  In  high  var‐\\niance cases, the model’s predicted values are extremely close to the actual values from\\nthe  training  set.  High  variance  gives  rise  to  overfitting,  as  shown  in  the  right-hand\\npanel of Figure 4-5. Ultimately, in order to have a good model, we need low bias and\\nlow variance.\\n\\nModel Performance \\n\\n| \\n\\n73\\n\\n\\x0cFigure 4-5. Overfitting and underfitting\\n\\nThere can be two ways to combat overfitting:\\n\\nUsing more training data\\n\\nThe more training data we have, the harder it is to overfit the data by learning\\ntoo much from any single training example.\\n\\nUsing regularization\\n\\nAdding a penalty in the loss function for building a model that assigns too much\\nexplanatory  power  to  any  one  feature,  or  allows  too  many  features  to  be  taken\\ninto account.\\n\\nThe  concept  of  overfitting  and  the  ways  to  combat  it  are  applicable  across  all  the\\nsupervised learning models. For example, regularized regressions address overfitting\\nin linear regression, as discussed earlier in this chapter.\\n\\nCross Validation\\nOne of the challenges of machine learning is training models that are able to general‐\\nize well to unseen data (overfitting versus underfitting or a bias-variance trade-off).\\nThe main idea behind cross validation is to split the data one time or several times so\\nthat each split is used once as a validation set and the remainder is used as a training\\nset:  part  of  the  data  (the  training  sample)  is  used  to  train  the  algorithm,  and  the\\nremaining  part  (the  validation  sample)  is  used  for  estimating  the  risk  of  the  algo‐\\nrithm. Cross validation allows us to obtain reliable estimates of the model’s generali‐\\nzation error. It is easiest to understand it with an example. When doing k-fold cross\\nvalidation, we randomly split the training data into k folds. Then we train the model\\nusing k-1 folds and evaluate the performance on the kth fold. We repeat this process\\nk times and average the resulting scores.\\n\\nFigure 4-6 shows an example of cross validation, where the data is split into five sets\\nand in each round one of the sets is used for validation.\\n\\n74 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cFigure 4-6. Cross validation\\n\\nA  potential  drawback  of  cross  validation  is  the  computational  cost,  especially  when\\npaired  with  a  grid  search  for  hyperparameter  tuning.  Cross  validation  can  be  per‐\\nformed in a couple of lines using the sklearn package; we will perform cross valida‐\\ntion in the supervised learning case studies.\\n\\nIn the next section, we cover the evaluation metrics for the supervised learning mod‐\\nels that are used to measure and compare the models’ performance.\\n\\nEvaluation Metrics\\nThe  metrics  used  to  evaluate  the  machine  learning  algorithms  are  very  important.\\nThe  choice  of  metrics  to  use  influences  how  the  performance  of  machine  learning\\nalgorithms is measured and compared. The metrics influence both how you weight\\nthe importance of different characteristics in the results and your ultimate choice of\\nalgorithm.\\n\\nThe  main  evaluation  metrics  for  regression  and  classification  are  illustrated  in\\nFigure 4-7.\\n\\nFigure 4-7. Evaluation metrics for regression and classification\\n\\nLet us first look at the evaluation metrics for supervised regression.\\n\\nModel Performance \\n\\n| \\n\\n75\\n\\n\\x0cMean absolute error\\n\\nThe mean absolute error (MAE) is the sum of the absolute differences between pre‐\\ndictions and actual values. The MAE is a linear score, which means that all the indi‐\\nvidual differences are weighted equally in the average. It gives an idea of how wrong\\nthe predictions were. The measure gives an idea of the magnitude of the error, but no\\nidea of the direction (e.g., over- or underpredicting).\\n\\nMean squared error\\n\\nThe mean squared error (MSE) represents the sample standard deviation of the dif‐\\nferences  between  predicted  values  and  observed  values  (called  residuals).  This  is\\nmuch like the mean absolute error in that it provides a gross idea of the magnitude of\\nthe error. Taking the square root of the mean squared error converts the units back to\\nthe  original  units  of  the  output  variable  and  can  be  meaningful  for  description  and\\npresentation. This is called the root mean squared error (RMSE).\\n\\nR² metric\\n\\nThe  R²  metric  provides  an  indication  of  the  “goodness  of  fit”  of  the  predictions  to\\nactual value. In statistical literature this measure is called the coefficient of determi‐\\nnation. This is a value between zero and one, for no-fit and perfect fit, respectively.\\n\\nAdjusted R² metric\\n\\nJust like R², adjusted R² also shows how well terms fit a curve or line but adjusts for\\nthe number of terms in a model. It is given in the following formula:\\n\\nRadj\\n\\n2 = 1 –\\n\\n(1 – R 2)(n – 1))\\nn – k – 1\\n\\nwhere  n  is  the  total  number  of  observations  and  k  is  the  number  of  predictors.\\nAdjusted R² will always be less than or equal to R².\\n\\nSelecting an evaluation metric for supervised regression\\n\\nIn terms of a preference among these evaluation metrics, if the main goal is predictive\\naccuracy, then RMSE is best. It is computationally simple and is easily differentiable.\\nThe loss is symmetric, but larger errors weigh more in the calculation. The MAEs are\\nsymmetric but do not weigh larger errors more. R² and adjusted R² are often used for\\nexplanatory  purposes  by  indicating  how  well  the  selected  independent  variable(s)\\nexplains the variability in the dependent variable(s).\\n\\nLet us first look at the evaluation metrics for supervised classification.\\n\\n76 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cClassification\\n\\nFor simplicity, we will mostly discuss things in terms of a binary classification prob‐\\nlem (i.e., only two outcomes, such as true or false); some common terms are:\\n\\nTrue positives (TP)\\n\\nPredicted positive and are actually positive.\\n\\nFalse positives (FP)\\n\\nPredicted positive and are actually negative.\\n\\nTrue negatives (TN)\\n\\nPredicted negative and are actually negative.\\n\\nFalse negatives (FN)\\n\\nPredicted negative and are actually positive.\\n\\nThe  difference  between  three  commonly  used  evaluation  metrics  for  classification,\\naccuracy, precision, and recall, is illustrated in Figure 4-8.\\n\\nFigure 4-8. Accuracy, precision, and recall\\n\\nAccuracy\\n\\nAs shown in Figure 4-8, accuracy is the number of correct predictions made as a ratio\\nof all predictions made. This is the most common evaluation metric for classification\\nproblems  and  is  also  the  most  misused.  It  is  most  suitable  when  there  are  an  equal\\nnumber of observations in each class (which is rarely the case) and when all predic‐\\ntions and the related prediction errors are equally important, which is often not the\\ncase.\\n\\nPrecision\\n\\nPrecision  is  the  percentage  of  positive  instances  out  of  the  total  predicted  positive\\ninstances. Here, the denominator is the model prediction done as positive from the\\nwhole given dataset. Precision is a good measure to determine when the cost of false\\npositives is high (e.g., email spam detection).\\n\\nModel Performance \\n\\n| \\n\\n77\\n\\n\\x0cRecall\\n\\nRecall (or sensitivity or true positive rate) is the percentage of positive instances out of\\nthe total actual positive instances. Therefore, the denominator (true positive + false\\nnegative) is the actual number of positive instances present in the dataset. Recall is a\\ngood  measure  when  there  is  a  high  cost  associated  with  false  negatives  (e.g.,  fraud\\ndetection).\\n\\nIn addition to accuracy, precision, and recall, some of the other commonly used eval‐\\nuation metrics for classification are discussed in the following sections.\\n\\nArea under ROC curve\\n\\nArea under ROC curve (AUC) is an evaluation metric for binary classification prob‐\\nlems. ROC is a probability curve, and AUC represents degree or measure of separa‐\\nbility. It tells how much the model is capable of distinguishing between classes. The\\nhigher the AUC, the better the model is at predicting zeros as zeros and ones as ones.\\nAn  AUC  of  0.5  means  that  the  model  has  no  class  separation  capacity  whatsoever.\\nThe  probabilistic  interpretation  of  the  AUC  score  is  that  if  you  randomly  choose  a\\npositive case and a negative case, the probability that the positive case outranks the\\nnegative case according to the classifier is given by the AUC.\\n\\nConfusion matrix\\n\\nA confusion matrix lays out the performance of a learning algorithm. The confusion\\nmatrix is simply a square matrix that reports the counts of the true positive (TP), true\\nnegative (TN), false positive (FP), and false negative (FN) predictions of a classifier,\\nas shown in Figure 4-9.\\n\\nFigure 4-9. Confusion matrix\\n\\nThe confusion matrix is a handy presentation of the accuracy of a model with two or\\nmore classes. The table presents predictions on the x-axis and accuracy outcomes on\\nthe y-axis. The cells of the table are the number of predictions made by the model.\\nFor example, a model can predict zero or one, and each prediction may actually have\\nbeen a zero or a one. Predictions for zero that were actually zero appear in the cell for\\n\\n78 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cprediction  =  0  and  actual  =  0,  whereas  predictions  for  zero  that  were  actually  one\\nappear in the cell for prediction = 0 and actual = 1.\\n\\nSelecting an evaluation metric for supervised classification\\n\\nThe  evaluation  metric  for  classification  depends  heavily  on  the  task  at  hand.  For\\nexample, recall is a good measure when there is a high cost associated with false nega‐\\ntives such as fraud detection. We will further examine these evaluation metrics in the\\ncase studies.\\n\\nModel Selection\\nSelecting the perfect machine learning model is both an art and a science. Looking at\\nmachine learning models, there is no one solution or approach that fits all. There are\\nseveral factors that can affect your choice of a machine learning model. The main cri‐\\nteria in most of the cases is the model performance that we discussed in the previous\\nsection. However, there are many other factors to consider while performing model\\nselection. In the following section, we will go over all such factors, followed by a dis‐\\ncussion of model trade-offs.\\n\\nFactors for Model Selection\\nThe factors considered for the model selection process are as follows:\\n\\nSimplicity\\n\\nThe degree of simplicity of the model. Simplicity usually results in quicker, more\\nscalable, and easier to understand models and results.\\n\\nTraining time\\n\\nSpeed, performance, memory usage and overall time taken for model training.\\n\\nHandle nonlinearity in the data\\n\\nThe  ability  of  the  model  to  handle  the  nonlinear  relationship  between  the\\nvariables.\\n\\nRobustness to overfitting\\n\\nThe ability of the model to handle overfitting.\\n\\nSize of the dataset\\n\\nThe  ability  of  the  model  to  handle  large  number  of  training  examples  in  the\\ndataset.\\n\\nNumber of features\\n\\nThe ability of the model to handle high dimensionality of the feature space.\\n\\nModel Selection \\n\\n| \\n\\n79\\n\\n\\x0cModel interpretation\\n\\nHow  explainable  is  the  model?  Model  interpretability  is  important  because  it\\nallows us to take concrete actions to solve the underlying problem.\\n\\nFeature scaling\\n\\nDoes the model require variables to be scaled or normally distributed?\\n\\nFigure 4-10 compares the supervised learning models on the factors mentioned pre‐\\nviously and outlines a general rule-of-thumb to narrow down the search for the best\\nmachine  learning  algorithm7  for  a  given  problem.  The  table  is  based  on  the  advan‐\\ntages and disadvantages of different models discussed in the individual model section\\nin this chapter.\\n\\nFigure 4-10. Model selection\\n\\nWe  can  see  from  the  table  that  relatively  simple  models  include  linear  and  logistic\\nregression and as we move towards the ensemble and ANN, the complexity increases.\\nIn  terms  of  the  training  time,  the  linear  models  and  CART  are  relatively  faster  to\\ntrain as compared to ensemble methods and ANN.\\n\\nLinear  and  logistic  regression  can’t  handle  nonlinear  relationships,  while  all  other\\nmodels  can.  SVM  can  handle  the  nonlinear  relationship  between  dependent  and\\nindependent variables with nonlinear kernels.\\n\\n7 In this table we do not include AdaBoost and extra trees as their overall behavior across all the parameters are\\n\\nsimilar to Gradient Boosting and Random Forest, respectively.\\n\\n80 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cSVM  and  random  forest  tend  to  overfit  less  as  compared  to  the  linear  regression,\\nlogistic  regression,  gradient  boosting,  and  ANN.  The  degree  of  overfitting  also\\ndepends on other parameters, such as size of the data and model tuning, and can be\\nchecked  by  looking  at  the  results  of  the  test  set  for  each  model.  Also,  the  boosting\\nmethods such as gradient boosting have higher overfitting risk compared to the bag‐\\nging methods, such as random forest. Recall the focus of gradient boosting is to mini‐\\nmize the bias and not variance.\\n\\nLinear and logistic regressions are not able to handle large datasets and large number\\nof features well. However, CART, ensemble methods, and ANN are capable of han‐\\ndling large datasets and many features quite well. The linear and logistic regression\\ngenerally  perform  better  than  other  models  in  case  the  size  of  the  dataset  is  small.\\nApplication of variable reduction techniques (shown in Chapter 7) enables the linear\\nmodels to handle large datasets. The performance of ANN increases with an increase\\nin the size of the dataset.\\n\\nGiven linear regression, logistic regression, and CART are relatively simpler models,\\nthey  have  better  model  interpretation  as  compared  to  the  ensemble  models  and\\nANN.\\n\\nModel Trade-off\\nOften, it’s a trade-off between different factors when selecting a model. ANN, SVM,\\nand some ensemble methods can be used to create very accurate predictive models,\\nbut they may lack simplicity and interpretability and may take a significant amount\\nof resources to train.\\n\\nIn terms of selecting the final model, models with lower interpretability may be pre‐\\nferred when predictive performance is the most important goal, and it’s not necessary\\nto  explain  how  the  model  works  and  makes  predictions.  In  some  cases,  however,\\nmodel interpretability is mandatory.\\n\\nInterpretability-driven  examples  are  often  seen  in  the  financial  industry.  In  many\\ncases, choosing a machine learning algorithm has less to do with the optimization or\\nthe technical aspects of the algorithm and more to do with business decisions. Sup‐\\npose  a  machine  learning  algorithm  is  used  to  accept  or  reject  an  individual’s  credit\\ncard  application.  If  the  applicant  is  rejected  and  decides  to  file  a  complaint  or  take\\nlegal action, the financial institution will need to explain how that decision was made.\\nWhile that can be nearly impossible for ANN, it’s relatively straightforward for deci‐\\nsion tree–based models.\\n\\nDifferent  classes  of  models  are  good  at  modeling  different  types  of  underlying  pat‐\\nterns in data. So a good first step is to quickly test out a few different classes of mod‐\\nels  to  know  which  ones  capture  the  underlying  structure  of  the  dataset  most\\n\\nModel Selection \\n\\n| \\n\\n81\\n\\n\\x0cefficiently. We will follow this approach while performing model selection in all our\\nsupervised learning–based case studies.\\n\\nChapter Summary\\nIn  this  chapter,  we  discussed  the  importance  of  supervised  learning  models  in\\nfinance,  followed  by  a  brief  introduction  to  several  supervised  learning  models,\\nincluding linear and logistic regression, SVM, decision trees, ensemble, KNN, LDA,\\nand  ANN.  We  demonstrated  training  and  tuning  of  these  models  in  a  few  lines  of\\ncode using sklearn and Keras libraries.\\n\\nWe discussed the most common error metrics for regression and classification mod‐\\nels, explained the bias-variance trade-off, and illustrated the various tools for manag‐\\ning the model selection process using cross validation.\\n\\nWe introduced the strengths and weaknesses of each model and discussed the factors\\nto consider when selecting the best model. We also discussed the trade-off between\\nmodel performance and interpretability.\\n\\nIn the following chapter, we will dive into the case studies for regression and classifi‐\\ncation.  All  case  studies  in  the  next  two  chapters  leverage  the  concepts  presented  in\\nthis chapter and in the previous two chapters.\\n\\n82 \\n\\n| \\n\\nChapter 4: Supervised Learning: Models and Concepts\\n\\n\\x0cCHAPTER 5\\nSupervised Learning: Regression\\n(Including Time Series Models)\\n\\nSupervised  regression–based  machine  learning  is  a  predictive  form  of  modeling  in\\nwhich the goal is to model the relationship between a target and the predictor vari‐\\nable(s) in order to estimate a continuous set of possible outcomes. These are the most\\nused machine learning models in finance.\\n\\nOne of the focus areas of analysts in financial institutions (and finance in general) is\\nto  predict  investment  opportunities,  typically  predictions  of  asset  prices  and  asset\\nreturns.  Supervised  regression–based  machine  learning  models  are  inherently  suit‐\\nable  in  this  context.  These  models  help  investment  and  financial  managers  under‐\\nstand the properties of the predicted variable and its relationship with other variables,\\nand help them identify significant factors that drive asset returns. This helps investors\\nestimate return profiles, trading costs, technical and financial investment required in\\ninfrastructure,  and  thus  ultimately  the  risk  profile  and  profitability  of  a  strategy  or\\nportfolio.\\n\\nWith the availability of large volumes of data and processing techniques, supervised\\nregression–based machine learning isn’t just limited to asset price prediction. These\\nmodels are applied to a wide range of areas within finance, including portfolio man‐\\nagement, insurance pricing, instrument pricing, hedging, and risk management.\\n\\nIn  this  chapter  we  cover  three  supervised  regression–based  case  studies  that  span\\ndiverse  areas,  including  asset  price  prediction,  instrument  pricing,  and  portfolio\\nmanagement. All of the case studies follow the standardized seven-step model devel‐\\nopment  process  presented  in  Chapter  2;  those  steps  include  defining  the  problem,\\nloading the data, exploratory data analysis, data preparation, model evaluation, and\\n\\n83\\n\\n\\x0cmodel tuning.1 The case studies are designed not only to cover a diverse set of topics\\nfrom the finance standpoint but also to cover multiple machine learning and model‐\\ning concepts, including models from basic linear regression to advanced deep learn‐\\ning that were presented in Chapter 4.\\n\\nA  substantial  amount  of  asset  modeling  and  prediction  problems  in  the  financial\\nindustry involve a time component and estimation of a continuous output. As such,\\nit  is  also  important  to  address  time  series  models.  In  its  broadest  form,  time  series\\nanalysis is about inferring what has happened to a series of data points in the past and\\nattempting to predict what will happen to it in the future. There have been a lot of\\ncomparisons  and  debates  in  academia  and  the  industry  regarding  the  differences\\nbetween  supervised  regression  and  time  series  models.  Most  time  series  models  are\\nparametric (i.e., a known function is assumed to represent the data), while the major‐\\nity of supervised regression models are nonparametric. Time series models primarily\\nuse historical data of the predicted variables for prediction, and supervised learning\\nalgorithms  use  exogenous  variables  as  predictor  variables.2  However,  supervised\\nregression  can  embed  the  historical  data  of  the  predicted  variable  through  a  time-\\ndelay approach (covered later in this chapter), and a time series model (such as ARI‐\\nMAX, also covered later in this chapter) can use exogenous variables for prediction.\\nHence, time series and supervised regression models are similar in the sense that both\\ncan  use  exogenous  variables  as  well  as  historical  data  of  the  predicted  variable  for\\nforecasting. In terms of the final output, both supervised regression and time series\\nmodels estimate a continuous set of possible outcomes of a variable.\\n\\nIn  Chapter  4,  we  covered  the  concepts  of  models  that  are  common  between  super‐\\nvised regression and supervised classification. Given that time series models are more\\nclosely  aligned  with  supervised  regression  than  supervised  classification,  we  will  go\\nthrough  the  concepts  of  time  series  models  separately  in  this  chapter.  We  will  also\\ndemonstrate  how  we  can  use  time  series  models  on  financial  data  to  predict  future\\nvalues.  Comparison  of  time  series  models  against  the  supervised  regression  models\\nwill be presented in the case studies. Additionally, some machine learning and deep\\nlearning models (such as LSTM) can be directly used for time series forecasting, and\\nthose will be discussed as well.\\n\\n1 There may be reordering or renaming of the steps or substeps based on the appropriateness and intuitiveness\\n\\nof the steps/substeps.\\n\\n2 An exogenous variable is one whose value is determined outside the model and imposed on the model.\\n\\n84 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cIn “Case Study 1: Stock Price Prediction” on page 95, we demonstrate one of the most\\npopular prediction problems in finance, that of predicting stock returns. In addition\\nto predicting future stock prices accurately, the purpose of this case study is to dis‐\\ncuss the machine learning–based framework for general asset class price prediction in\\nfinance.  In  this  case  study  we  will  discuss  several  machine  learning  and  time  series\\nconcepts, along with focusing on visualization and model tuning.\\n\\nIn “Case Study 2: Derivative Pricing” on page 114, we will delve into derivative pricing\\nusing supervised regression and show how to deploy machine learning techniques in\\nthe context of traditional quant problems. As compared to traditional derivative pric‐\\ning models, machine learning techniques can lead to faster derivative pricing without\\nrelying  on  the  several  impractical  assumptions.  Efficient  numerical  computation\\nusing machine learning can be increasingly beneficial in areas such as financial risk\\nmanagement, where a trade-off between efficiency and accuracy is often inevitable.\\n\\nIn “Case Study 3: Investor Risk Tolerance and Robo-Advisors” on page 125, we illus‐\\ntrate supervised regression–based framework to estimate the risk tolerance of invest‐\\nors. In this case study, we build a robo-advisor dashboard in Python and implement\\nthis  risk  tolerance  prediction  model  in  the  dashboard.  We  demonstrate  how  such\\nmodels can lead to the automation of portfolio management processes, including the\\nuse  of  robo-advisors  for  investment  management.  The  purpose  is  also  to  illustrate\\nhow machine learning can efficiently be used to overcome the problem of traditional\\nrisk  tolerance  profiling  or  risk  tolerance  questionnaires  that  suffer  from  several\\nbehavioral biases.\\n\\nIn “Case Study 4: Yield Curve Prediction” on page 141, we use a supervised regression–\\nbased framework to forecast different yield curve tenors simultaneously. We demon‐\\nstrate how we can produce multiple tenors at the same time to model the yield curve\\nusing machine learning models.\\n\\nIn  this  chapter,  we  will  learn  about  the  following  concepts  related  to  supervised\\nregression and time series techniques:\\n\\n• Application  and  comparison  of  different  time  series  and  machine  learning\\n\\nmodels.\\n\\n• Interpretation of the models and results. Understanding the potential overfitting\\n\\nand underfitting and intuition behind linear versus nonlinear models.\\n\\n• Performing data preparation and transformations to be used in machine learning\\n\\nmodels.\\n\\n• Performing feature selection and engineering to improve model performance.\\n\\n• Using data visualization and data exploration to understand outcomes.\\n\\nSupervised Learning: Regression (Including Time Series Models) \\n\\n| \\n\\n85\\n\\n\\x0c• Algorithm  tuning  to  improve  model  performance.  Understanding,  implement‐\\n\\ning, and tuning time series models such as ARIMA for prediction.\\n\\n• Framing  a  problem  statement  related  to  portfolio  management  and  behavioral\\n\\nfinance in a regression-based machine learning framework.\\n\\n• Understanding how deep learning–based models such as LSTM can be used for\\n\\ntime series prediction.\\n\\nThe models used for supervised regression were presented in Chapters 3 and 4. Prior\\nto the case studies, we will discuss time series models. We highly recommend readers\\nturn to Time Series Analysis and Its Applications, 4th Edition, by Robert H. Shumway\\nand David S. Stoffer (Springer) for a more in-depth understanding of time series con‐\\ncepts, and to Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow,\\n2nd Edition, by Aurélien Géron (O’Reilly) for more on concepts in supervised regres‐\\nsion models.\\n\\nThis Chapter’s Code Repository\\n\\nA Python-based master template for supervised regression, a time\\nseries model template, and the Jupyter notebook for all case studies\\npresented in this chapter are included in the folder Chapter 5 - Sup.\\nLearning - Regression and Time Series models of the code repository\\nfor this book.\\n\\nFor any new supervised regression–based case study, use the com‐\\nmon template from the code repository, modify the elements spe‐\\ncific to the case study, and borrow the concepts and insights from\\nthe  case  studies  presented  in  this  chapter.  The  template  also\\nincludes the implementation and tuning of the ARIMA and LSTM\\nmodels.3 The templates are designed to run on the cloud (i.e., Kag‐\\ngle,  Google  Colab,  and  AWS).  All  the  case  studies  have  been\\ndesigned on a uniform regression template.4\\n\\nTime Series Models\\nA time series is a sequence of numbers that are ordered by a time index.\\n\\nIn  this  section  we  will  cover  the  following  aspects  of  time  series  models,  which  we\\nfurther leverage in the case studies:\\n\\n3 These models are discussed later in this chapter.\\n\\n4 There may be reordering or renaming of the steps or substeps based on the appropriateness and intuitiveness\\n\\nof the steps/substeps.\\n\\n86 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0c• The components of a time series\\n\\n• Autocorrelation and stationarity of time series\\n\\n• Traditional time series models (e.g., ARIMA)\\n\\n• Use of deep learning models for time series modeling\\n\\n• Conversion of time series data for use in a supervised learning framework\\n\\nTime Series Breakdown\\nA time series can be broken down into the following components:\\n\\nTrend Component\\n\\nA trend is a consistent directional movement in a time series. These trends will\\nbe either deterministic or stochastic. The former allows us to provide an underly‐\\ning rationale for the trend, while the latter is a random feature of a series that we\\nwill  be  unlikely  to  explain.  Trends  often  appear  in  financial  series,  and  many\\ntrading models use sophisticated trend identification algorithms.\\n\\nSeasonal Component\\n\\nMany  time  series  contain  seasonal  variation.  This  is  particularly  true  in  series\\nrepresenting business sales or climate levels. In quantitative finance we often see\\nseasonal variation, particularly in series related to holiday seasons or annual tem‐\\nperature variation (such as natural gas).\\n\\nWe can write the components of a time series yt as\\n\\nyt = St + T t + Rt\\n\\nwhere St is the seasonal component, T t is the trend component, and Rt represents the\\nremainder  component  of  the  time  series  not  captured  by  seasonal  or  trend\\ncomponent.\\n\\nThe Python code for breaking down a time series (Y) into its component is as follows:\\n\\nimport statsmodels.api as sm\\nsm.tsa.seasonal_decompose(Y,freq=52).plot()\\n\\nFigure 5-1 shows the time series broken down into trend, seasonality, and remainder\\ncomponents. Breaking down a time series into these components may help us better\\nunderstand the time series and identify its behavior for better prediction.\\n\\nThe three time series components are shown separately in the bottom three panels.\\nThese components can be added together to reconstruct the actual time series shown\\nin the top panel (shown as “observed”). Notice that the time series shows a trending\\ncomponent after 2017. Hence, the prediction model for this time series should incor‐\\n\\nTime Series Models \\n\\n| \\n\\n87\\n\\n\\x0cporate the information regarding the trending behavior after 2017. In terms of sea‐\\nsonality  there  is  some  increase  in  the  magnitude  in  the  beginning  of  the  calendar\\nyear.  The  residual  component  shown  in  the  bottom  panel  is  what  is  left  over  when\\nthe seasonal and trend components have been subtracted from the data. The residual\\ncomponent  is  mostly  flat  with  some  spikes  and  noise  around  2018  and  2019.  Also,\\neach  of  the  plots  are  on  different  scales,  and  the  trend  component  has  maximum\\nrange as shown by the scale on the plot.\\n\\nFigure 5-1. Time series components\\n\\nAutocorrelation and Stationarity\\nWhen we are given one or more time series, it is relatively straightforward to decom‐\\npose the time series into trend, seasonality, and residual components. However, there\\nare other aspects that come into play when dealing with time series data, particularly\\nin finance.\\n\\nAutocorrelation\\n\\nThere are many situations in which consecutive elements of a time series exhibit cor‐\\nrelation. That is, the behavior of sequential points in the series affect each other in a\\ndependent manner. Autocorrelation is the similarity between observations as a func‐\\ntion of the time lag between them. Such relationships can be modeled using an autor‐\\negression  model.  The  term  autoregression  indicates  that  it  is  a  regression  of  the\\nvariable against itself.\\n\\nIn an autoregression model, we forecast the variable of interest using a linear combi‐\\nnation of past values of the variable.\\n\\n88 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cThus, an autoregressive model of order  p can be written as\\n\\nyt = c + ϕ1yt –1 + ϕ2yt –2 + ....ϕp yt – p + ϵ\\n\\nwhere ϵt  is  white  noise.5  An  autoregressive  model  is  like  a  multiple  regression  but\\nwith lagged values of  yt as predictors. We refer to this as an AR(p) model, an autore‐\\ngressive model of order p. Autoregressive models are remarkably flexible at handling\\na wide range of different time series patterns.\\n\\nStationarity\\n\\nA  time  series  is  said  to  be  stationary  if  its  statistical  properties  do  not  change  over\\ntime. Thus a time series with trend or with seasonality is not stationary, as the trend\\nand seasonality will affect the value of the time series at different times. On the other\\nhand, a white noise series is stationary, as it does not matter when you observe it; it\\nshould look similar at any point in time.\\n\\nFigure 5-2 shows some examples of nonstationary series.\\n\\nFigure 5-2. Nonstationary plots\\n\\nIn the first plot, we can clearly see that the mean varies (increases) with time, result‐\\ning in an upward trend. Thus this is a nonstationary series. For a series to be classi‐\\nfied  as  stationary,  it  should  not  exhibit  a  trend.  Moving  on  to  the  second  plot,  we\\ncertainly do not see a trend in the series, but the variance of the series is a function of\\ntime. A stationary series must have a constant variance; hence this series is a nonsta‐\\ntionary series as well. In the third plot, the spread becomes closer as the time increa‐\\nses,  which  implies  that  the  covariance  is  a  function  of  time.  The  three  examples\\nshown in Figure 5-2 represent nonstationary time series. Now look at a fourth plot, as\\nshown in Figure 5-3.\\n\\n5 A white noise process is a random process of random variables that are uncorrelated and have a mean of zero\\n\\nand a finite variance.\\n\\nTime Series Models \\n\\n| \\n\\n89\\n\\n\\x0cFigure 5-3. Stationary plot\\n\\nIn this case, the mean, variance, and covariance are constant with time. This is what a\\nstationary time series looks like. Predicting future values using this fourth plot would\\nbe easier. Most statistical models require the series to be stationary to make effective\\nand precise predictions.\\n\\nThe two major reasons behind nonstationarity of a time series are trend and season‐\\nality, as shown in Figure 5-2. In order to use time series forecasting models, we gener‐\\nally convert any nonstationary series to a stationary series, making it easier to model\\nsince statistical properties don’t change over time.\\n\\nDifferencing\\n\\nDifferencing  is  one  of  the  methods  used  to  make  a  time  series  stationary.  In  this\\nmethod, we compute the difference of consecutive terms in the series. Differencing is\\ntypically performed to get rid of the varying mean. Mathematically, differencing can\\nbe written as:\\n\\nyt\\n\\n′ = yt – yt –1\\n\\nwhere yt is the value at a time t.\\n\\nWhen the differenced series is white noise, the original series is referred to as a non‐\\nstationary series of degree one.\\n\\nTraditional Time Series Models (Including the ARIMA Model)\\nThere are many ways to model a time series in order to make predictions. Most of the\\ntime series models aim at incorporating the trend, seasonality, and remainder com‐\\nponents while addressing the autocorrelation and stationarity embedded in the time\\nseries. For example, the autoregressive (AR) model discussed in the previous section\\naddresses the autocorrelation in the time series.\\n\\nOne of the most widely used models in time series forecasting is the ARIMA model.\\n\\n90 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cARIMA\\n\\nIf  we  combine  stationarity  with  autoregression  and  a  moving  average  model  (dis‐\\ncussed further on in this section), we obtain an ARIMA model. ARIMA is an acro‐\\nnym  for  AutoRegressive  Integrated  Moving  Average,  and  it  has  the  following\\ncomponents:\\n\\nAR(p)\\n\\nIt represents autoregression, i.e., regression of the time series onto itself, as dis‐\\ncussed  in  the  previous  section,  with  an  assumption  that  current  series  values\\ndepend on its previous values with some lag (or several lags). The maximum lag\\nin the model is referred to as p.\\n\\nI(d)\\n\\nIt represents order of integration. It is simply the number of differences needed\\nto make the series stationary.\\n\\nMA(q)\\n\\nIt  represents  moving  average.  Without  going  into  detail,  it  models  the  error  of\\nthe time series; again, the assumption is that current error depends on the previ‐\\nous with some lag, which is referred to as q.\\n\\nThe moving average equation is written as:\\n\\nyt = c + ϵt + θ1ϵt –1 + θ2ϵt –2\\n\\nwhere, ϵt is white noise. We refer to this as an MA(q) model of order q.\\n\\nCombining all the components, the full ARIMA model can be written as:\\n\\nyt\\n\\n′ = c + ϕ1yt –1\\n\\n′ + ⋯ + ϕp yt – p\\n\\n′ + θ1εt –1 + ⋯ + θqεt –q + εt\\n\\n\\' is the differenced series (it may have been differenced more than once). The\\nwhere yt\\n\\' and lagged errors.\\npredictors on the right-hand side include both lagged values of  yt\\nWe call this an ARIMA(p,d,q) model, where p is the order of the autoregressive part,\\nd is the degree of first differencing involved, and q is the order of the moving average\\npart. The same stationarity and invertibility conditions that are used for autoregres‐\\nsive and moving average models also apply to an ARIMA model.\\n\\nThe  Python  code  to  fit  the  ARIMA  model  of  the  order  (1,0,0)  is  shown  in  the\\nfollowing:\\n\\nfrom statsmodels.tsa.arima_model import ARIMA\\nmodel=ARIMA(endog=Y_train,order=[1,0,0])\\n\\nTime Series Models \\n\\n| \\n\\n91\\n\\n\\x0cThe family of ARIMA models has several variants, and some of them are as follows:\\n\\nARIMAX\\n\\nARIMA models with exogenous variables included. We will be using this model\\nin case study 1.\\n\\nSARIMA\\n\\n“S” in this model stands for seasonal, and this model is targeted at modeling the\\nin  the  time  series,  along  with  other\\nseasonality  component  embedded \\ncomponents.\\n\\nVARMA\\n\\nThis is the extension of the model to multivariate case, when there are many vari‐\\nables to be predicted simultaneously. We predict many variables simultaneously\\nin “Case Study 4: Yield Curve Prediction” on page 141.\\n\\nDeep Learning Approach to Time Series Modeling\\nThe traditional time series models such as ARIMA are well understood and effective\\non many problems. However, these traditional methods also suffer from several limi‐\\ntations. Traditional time series models are linear functions, or simple transformations\\nof  linear  functions,  and  they  require  manually  diagnosed  parameters,  such  as  time\\ndependence, and don’t perform well with corrupt or missing data.\\n\\nIf we look at the advancements in the field of deep learning for time series prediction,\\nwe see that recurrent neural network (RNN) has gained increasing attention in recent\\nyears.  These  methods  can  identify  structure  and  patterns  such  as  nonlinearity,  can\\nseamlessly model problems with multiple input variables, and are relatively robust to\\nmissing  data.  The  RNN  models  can  retain  state  from  one  iteration  to  the  next  by\\nusing their own output as input for the next step. These deep learning models can be\\nreferred to as time series models, as they can make future predictions using the data\\npoints in the past, similar to traditional time series models such as ARIMA. There‐\\nfore, there are a wide range of applications in finance where these deep learning mod‐\\nels  can  be  leveraged.  Let  us  look  at  the  deep  learning  models  for  time  series\\nforecasting.\\n\\nRNNs\\n\\nRecurrent  neural  networks  (RNNs)  are  called  “recurrent”  because  they  perform  the\\nsame task for every element of a sequence, with the output being dependent on the\\nprevious  computations.  RNN  models  have  a  memory,  which  captures  information\\nabout what has been calculated so far. As shown in Figure 5-4, a recurrent neural net‐\\nwork can be thought of as multiple copies of the same network, each passing a mes‐\\nsage to a successor.\\n\\n92 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cFigure 5-4. Recurrent Neural Network\\n\\nIn Figure 5-4:\\n\\n• Xt is the input at time step t.\\n• Ot is the output at time step t.\\n• St is the hidden state at time step t. It’s the memory of the network. It is calcula‐\\n\\nted based on the previous hidden state and the input at the current step.\\n\\nThe main feature of an RNN is this hidden state, which captures some information\\nabout a sequence and uses it accordingly whenever needed.\\n\\nLong short-term memory\\n\\nLong  short-term  memory  (LSTM)  is  a  special  kind  of  RNN  explicitly  designed  to\\navoid the long-term dependency problem. Remembering information for long peri‐\\nods  of  time  is  practically  default  behavior  for  an  LSTM  model.6  These  models  are\\ncomposed of a set of cells with features to memorize the sequence of data. These cells\\ncapture and store the data streams. Further, the cells interconnect one module of the\\npast to another module of the present to convey information from several past time\\ninstants to the present one. Due to the use of gates in each cell, data in each cell can\\nbe disposed, filtered, or added for the next cells.\\n\\nThe gates, based on artificial neural network layers, enable the cells to optionally let\\ndata pass through or be disposed. Each layer yields numbers in the range of zero to\\none, depicting the amount of every segment of data that ought to be let through in\\neach  cell.  More  precisely,  an  estimation  of  zero  value  implies  “let  nothing  pass\\nthrough.” An estimation of one indicates “let everything pass through.” Three types\\nof gates are involved in each LSTM, with the goal of controlling the state of each cell:\\n\\n6 A detailed explanation of LSTM models can be found in this blog post by Christopher Olah .\\n\\nTime Series Models \\n\\n| \\n\\n93\\n\\n\\x0cForget Gate\\n\\nOutputs  a  number  between  zero  and  one,  where  one  shows  “completely  keep\\nthis”  and  zero  implies  “completely  ignore  this.”  This  gate  conditionally  decides\\nwhether the past should be forgotten or preserved.\\n\\nInput Gate\\n\\nChooses which new data needs to be stored in the cell.\\n\\nOutput Gate\\n\\nDecides  what  will  yield  out  of  each  cell.  The  yielded  value  will  be  based  on  the\\ncell state along with the filtered and newly added data.\\n\\nKeras wraps the efficient numerical computation libraries and functions and allows\\nus to define and train LSTM neural network models in a few short lines of code. In\\nthe  following  code,  LSTM  module  from  keras.layers  is  used  for  implementing\\nLSTM network. The network is trained with the variable X_train_LSTM. The network\\nhas a hidden layer with 50 LSTM blocks or neurons and an output layer that makes a\\nsingle value prediction. Also refer to Chapter 3 for a more detailed description of all\\nthe terms (i.e., sequential, learning rate, momentum, epoch, and batch size).\\n\\nA sample Python code for implementing an LSTM model in Keras is shown below:\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.optimizers import SGD\\nfrom keras.layers import LSTM\\n\\ndef create_LSTMmodel(learn_rate = 0.01, momentum=0):\\n       # create model\\n   model = Sequential()\\n   model.add(LSTM(50, input_shape=(X_train_LSTM.shape[1],\\\\\\n      X_train_LSTM.shape[2])))\\n   #More number of cells can be added if needed\\n   model.add(Dense(1))\\n   optimizer = SGD(lr=learn_rate, momentum=momentum)\\n   model.compile(loss=\\'mse\\', optimizer=\\'adam\\')\\n   return model\\nLSTMModel = create_LSTMmodel(learn_rate = 0.01, momentum=0)\\nLSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM, validation_data=\\\\\\n  (X_test_LSTM, Y_test_LSTM),epochs=330, batch_size=72, verbose=0, shuffle=False)\\n\\nIn  terms  of  both  learning  and  implementation,  LSTM  provides  considerably  more\\noptions for fine-tuning compared to ARIMA models. Although deep learning models\\nhave several advantages over traditional time series models, deep learning models are\\nmore complicated and difficult to train.7\\n\\n7 An ARIMA model and a Keras-based LSTM model will be demonstrated in one of the case studies.\\n\\n94 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cModifying Time Series Data for Supervised Learning Models\\nA time series is a sequence of numbers that are ordered by a time index. Supervised\\nlearning  is  where  we  have  input  variables  (X)  and  an  output  variable  (Y).  Given  a\\nsequence of numbers for a time series dataset, we can restructure the data into a set of\\npredictor and predicted variables, just like in a supervised learning problem. We can\\ndo this by using previous time steps as input variables and using the next time step as\\nthe output variable. Let’s make this concrete with an example.\\n\\nWe can restructure a time series shown in the left table in Figure 5-5 as a supervised\\nlearning problem by using the value at the previous time step to predict the value at\\nthe next time step. Once we’ve reorganized the time series dataset this way, the data\\nwould look like the table on the right.\\n\\nFigure 5-5. Modifying time series for supervised learning models\\n\\nWe can see that the previous time step is the input (X) and the next time step is the\\noutput (Y) in our supervised learning problem. The order between the observations is\\npreserved and must continue to be preserved when using this dataset to train a super‐\\nvised model. We will delete the first and last row while training our supervised model\\nas we don’t have values for either X or Y.\\n\\nIn  Python,  the  main  function  to  help  transform  time  series  data  into  a  supervised\\nlearning problem is the shift() function from the Pandas library. We will demon‐\\nstrate this approach in the case studies. The use of prior time steps to predict the next\\ntime step is called the sliding window, time delay, or lag method.\\n\\nHaving discussed all the concepts of supervised learning and time series models, let\\nus move to the case studies.\\n\\nCase Study 1: Stock Price Prediction\\nOne of the biggest challenges in finance is predicting stock prices. However, with the\\nonset  of  recent  advancements  in  machine  learning  applications,  the  field  has  been\\nevolving to utilize nondeterministic solutions that learn what is going on in order to\\nmake  more  accurate  predictions.  Machine  learning  techniques  naturally  lend\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n95\\n\\n\\x0cthemselves  to  stock  price  prediction  based  on  historical  data.  Predictions  can  be\\nmade for a single time point ahead or for a set of future time points.\\n\\nAs a high-level overview, other than the historical price of the stock itself, the features\\nthat are generally useful for stock price prediction are as follows:\\n\\nCorrelated assets\\n\\nAn organization depends on and interacts with many external factors, including\\nits competitors, clients, the global economy, the geopolitical situation, fiscal and\\nmonetary policies, access to capital, and so on. Hence, its stock price may be cor‐\\nrelated  not  only  with  the  stock  price  of  other  companies  but  also  with  other\\nassets  such  as  commodities,  FX,  broad-based  indices,  or  even  fixed  income\\nsecurities.\\n\\nTechnical indicators\\n\\nA lot of investors follow technical indicators. Moving average, exponential mov‐\\ning average, and momentum are the most popular indicators.\\n\\nFundamental analysis\\n\\nTwo  primary  data  sources  to  glean  features  that  can  be  used  in  fundamental\\nanalysis include:\\n\\nPerformance reports\\n\\nAnnual and quarterly reports of companies can be used to extract or deter‐\\nmine  key  metrics,  such  as  ROE  (Return  on  Equity)  and  P/E  (Price-to-\\nEarnings).\\n\\nNews\\n\\nNews can indicate upcoming events that can potentially move the stock price\\nin a certain direction.\\n\\nIn  this  case  study,  we  will  use  various  supervised  learning–based  models  to  predict\\nthe stock price of Microsoft using correlated assets and its own historical data. By the\\nend  of  this  case  study,  readers  will  be  familiar  with  a  general  machine  learning\\napproach to stock prediction modeling, from gathering and cleaning data to building\\nand tuning different models.\\n\\nIn this case study, we will focus on:\\n\\n• Looking  at  various  machine  learning  and  time  series  models,  ranging  in  com‐\\n\\nplexity, that can be used to predict stock returns.\\n\\n• Visualization of the data using different kinds of charts (i.e., density, correlation,\\n\\nscatterplot, etc.)\\n\\n• Using deep learning (LSTM) models for time series forecasting.\\n\\n96 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0c• Implementation of the grid search for time series models (i.e., ARIMA model).\\n\\n• Interpretation of the results and examining potential overfitting and underfitting\\n\\nof the data across the models.\\n\\nBlueprint for Using Supervised Learning Models to Predict a\\nStock Price\\n\\n1. Problem definition\\n\\nIn the supervised regression framework used for this case study, the weekly return of\\nMicrosoft stock is the predicted variable. We need to understand what affects Micro‐\\nsoft stock price and incorporate as much information into the model. Out of correla‐\\nted  assets,  technical  indicators,  and  fundamental  analysis  (discussed  in  the  section\\nbefore), we will focus on correlated assets as features in this case study.8\\n\\nFor this case study, other than the historical data of Microsoft, the independent vari‐\\nables used are the following potentially correlated assets:\\n\\nStocks\\n\\nIBM (IBM) and Alphabet (GOOGL)\\n\\nCurrency9\\n\\nUSD/JPY and GBP/USD\\n\\nIndices\\n\\nS&P 500, Dow Jones, and VIX\\n\\nThe dataset used for this case study is extracted from Yahoo Finance and the FRED\\nwebsite. In addition to predicting the stock price accurately, this case study will also\\ndemonstrate the infrastructure and framework for each step of time series and super‐\\nvised  regression–based  modeling  for  stock  price  prediction.  We  will  use  the  daily\\nclosing price of the last 10 years, from 2010 onward.\\n\\n8 Refer to “Case Study 3: Bitcoin Trading Strategy” on page 179 presented in Chapter 6 and “Case Study 1: NLP\\nand Sentiment Analysis–Based Trading Strategies” on page 362 presented in Chapter 10 to understand the\\nusage of technical indicators and news-based fundamental analysis as features in the price prediction.\\n\\n9 Equity markets have trading holidays, while currency markets do not. However, the alignment of the dates\\n\\nacross all the time series is ensured before any modeling or analysis.\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n97\\n\\n\\x0c2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  list  of  the  libraries  used  for  data  loading,  data\\nanalysis, data preparation, model evaluation, and model tuning are shown below. The\\npackages  used  for  different  purposes  have  been  segregated  in  the  Python  code  that\\nfollows. The details of most of these packages and functions were provided in Chap‐\\nters 2 and 4. The use of these packages will be demonstrated in different steps of the\\nmodel development process.\\n\\nFunction and modules for the supervised regression models\\n\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.linear_model import ElasticNet\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.svm import SVR\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.ensemble import ExtraTreesRegressor\\nfrom sklearn.ensemble import AdaBoostRegressor\\nfrom sklearn.neural_network import MLPRegressor\\n\\nFunction and modules for data analysis and model evaluation\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import chi2, f_regression\\n\\nFunction and modules for deep learning models\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.optimizers import SGD\\nfrom keras.layers import LSTM\\nfrom keras.wrappers.scikit_learn import KerasRegressor\\n\\nFunction and modules for time series models\\n\\nfrom statsmodels.tsa.arima_model import ARIMA\\nimport statsmodels.api as sm\\n\\nFunction and modules for data preparation and visualization\\n\\n# pandas, pandas_datareader, numpy and matplotlib\\nimport numpy as np\\nimport pandas as pd\\nimport pandas_datareader.data as web\\nfrom matplotlib import pyplot\\n\\n98 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cfrom pandas.plotting import scatter_matrix\\nimport seaborn as sns\\nfrom sklearn.preprocessing import StandardScaler\\nfrom pandas.plotting import scatter_matrix\\nfrom statsmodels.graphics.tsaplots import plot_acf\\n\\n2.2. Loading the data.    One of the most important steps in machine learning and pre‐\\ndictive modeling is gathering good data. The following steps demonstrate the loading\\nof  data  from  the  Yahoo  Finance  and  FRED  websites  using  the  Pandas  DataReader\\nfunction:10\\n\\nstk_tickers = [\\'MSFT\\', \\'IBM\\', \\'GOOGL\\']\\nccy_tickers = [\\'DEXJPUS\\', \\'DEXUSUK\\']\\nidx_tickers = [\\'SP500\\', \\'DJIA\\', \\'VIXCLS\\']\\n\\nstk_data = web.DataReader(stk_tickers, \\'yahoo\\')\\nccy_data = web.DataReader(ccy_tickers, \\'fred\\')\\nidx_data = web.DataReader(idx_tickers, \\'fred\\')\\n\\nNext,  we  define  our  dependent  (Y)  and  independent  (X)  variables.  The  predicted\\nvariable is the weekly return of Microsoft (MSFT). The number of trading days in a\\nweek is assumed to be five, and we compute the return using five trading days. For\\nindependent variables we use the correlated assets and the historical return of MSFT\\nat different frequencies.\\n\\nThe variables used as independent variables are lagged five-day return of stocks (IBM\\nand  GOOG),  currencies  (USD/JPY  and  GBP/USD),  and  indices  (S&P  500,  Dow\\nJones,  and  VIX),  along  with  lagged  5-day,  15-day,  30-day  and  60-day  return  of\\nMSFT.\\n\\nThe lagged five-day variables embed the time series component by using a time-delay\\napproach, where the lagged variable is included as one of the independent variables.\\nThis step is reframing the time series data into a supervised regression–based model\\nframework.\\n\\nreturn_period = 5\\nY = np.log(stk_data.loc[:, (\\'Adj Close\\', \\'MSFT\\')]).diff(return_period).\\\\\\nshift(-return_period)\\nY.name = Y.name[-1]+\\'_pred\\'\\n\\nX1 = np.log(stk_data.loc[:, (\\'Adj Close\\', (\\'GOOGL\\', \\'IBM\\'))]).diff(return_period)\\nX1.columns = X1.columns.droplevel()\\nX2 = np.log(ccy_data).diff(return_period)\\nX3 = np.log(idx_data).diff(return_period)\\n\\nX4 = pd.concat([np.log(stk_data.loc[:, (\\'Adj Close\\', \\'MSFT\\')]).diff(i) \\\\\\n\\n10 In different case studies across the book we will demonstrate loading the data through different sources (e.g.,\\n\\nCSV, and external websites like quandl).\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n99\\n\\n\\x0cfor i in [return_period, return_period*3,\\\\\\nreturn_period*6, return_period*12]], axis=1).dropna()\\nX4.columns = [\\'MSFT_DT\\', \\'MSFT_3DT\\', \\'MSFT_6DT\\', \\'MSFT_12DT\\']\\n\\nX = pd.concat([X1, X2, X3, X4], axis=1)\\n\\ndataset = pd.concat([Y, X], axis=1).dropna().iloc[::return_period, :]\\nY = dataset.loc[:, Y.name]\\nX = dataset.loc[:, X.columns]\\n\\n3. Exploratory data analysis\\n\\nWe  will  look  at  descriptive  statistics,  data  visualization,  and  time  series  analysis  in\\nthis section.\\n\\n3.1. Descriptive statistics.    Let’s have a look at the dataset we have:\\n\\ndataset.head()\\n\\nOutput\\n\\nThe  variable  MSFT_pred  is  the  return  of  Microsoft  stock  and  is  the  predicted  vari‐\\nable. The dataset contains the lagged series of other correlated stocks, currencies, and\\nindices. Additionally, it also consists of the lagged historical returns of MSFT.\\n\\n3.2. Data visualization.    The fastest way to learn more about the data is to visualize it.\\nThe visualization involves independently understanding each attribute of the dataset.\\nWe will look at the scatterplot and the correlation matrix. These plots give us a sense\\nof  the  interdependence  of  the  data.  Correlation  can  be  calculated  and  displayed  for\\neach  pair  of  the  variables  by  creating  a  correlation  matrix.  Hence,  besides  the  rela‐\\ntionship between independent and dependent variables, it also shows the correlation\\namong  the  independent  variables.  This  is  useful  to  know  because  some  machine\\nlearning algorithms like linear and logistic regression can have poor performance if\\nthere are highly correlated input variables in the data:\\n\\ncorrelation = dataset.corr()\\npyplot.figure(figsize=(15,15))\\npyplot.title(\\'Correlation Matrix\\')\\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap=\\'cubehelix\\')\\n\\n100 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cOutput\\n\\nLooking at the correlation plot (full-size version available on GitHub), we see some\\ncorrelation  of  the  predicted  variable  with  the  lagged  5-day,  15-day,  30-day,  and  60-\\nday returns of MSFT. Also, we see a higher negative correlation of many asset returns\\nversus VIX, which is intuitive.\\n\\nNext,  we  can  visualize  the  relationship  between  all  the  variables  in  the  regression\\nusing the scatterplot matrix shown below:\\n\\npyplot.figure(figsize=(15,15))\\nscatter_matrix(dataset,figsize=(12,12))\\npyplot.show()\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n101\\n\\n\\x0cOutput\\n\\nLooking at the scatterplot (full-size version available on GitHub), we see some linear\\nrelationship  of  the  predicted  variable  with  the  lagged  15-day,  30-day,  and  60-day\\nreturns of MSFT. Otherwise, we do not see any special relationship between our pre‐\\ndicted variable and the features.\\n\\n3.3. Time series analysis.     Next,  we  delve  into  the  time  series  analysis  and  look  at  the\\ndecomposition of the time series of the predicted variable into trend and seasonality\\ncomponents:\\n\\nres = sm.tsa.seasonal_decompose(Y,freq=52)\\nfig = res.plot()\\nfig.set_figheight(8)\\nfig.set_figwidth(15)\\npyplot.show()\\n\\n102 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cOutput\\n\\nWe can see that for MSFT there has been a general upward trend in the return series.\\nThis may be due to the large run-up of MSFT in the recent years, causing more posi‐\\ntive  weekly  return  data  points  than  negative.11  The  trend  may  show  up  in  the  con‐\\nstant/bias terms in our models. The residual (or white noise) term is relatively small\\nover the entire time series.\\n\\n4. Data preparation\\n\\nThis step typically involves data processing, data cleaning, looking at feature impor‐\\ntance, and performing feature reduction. The data obtained for this case study is rela‐\\ntively clean and doesn’t require further processing. Feature reduction might be useful\\nhere, but given the relatively small number of variables considered, we will keep all of\\nthem  as  is.  We  will  demonstrate  data  preparation  in  some  of  the  subsequent  case\\nstudies in detail.\\n\\n5. Evaluate models\\n\\n5.1. Train-test split and evaluation metrics.    As  described  in  Chapter  2,  it  is  a  good  idea\\nto partition the original dataset into a training set and a test set. The test set is a sam‐\\nple of the data that we hold back from our analysis and modeling. We use it right at\\nthe end of our project to confirm the performance of our final model. It is the final\\ntest that gives us confidence on our estimates of accuracy on unseen data. We will use\\n\\n11 The time series is not the stock price but stock return, so the trend is mild compared to the stock price series.\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n103\\n\\n\\x0c80% of the dataset for modeling and use 20% for testing. With time series data, the\\nsequence of values is important. So we do not distribute the dataset into training and\\ntest sets in random fashion, but we select an arbitrary split point in the ordered list of\\nobservations and create two new datasets:\\n\\nvalidation_size = 0.2\\ntrain_size = int(len(X) * (1-validation_size))\\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]\\n\\n5.2. Test options and evaluation metrics.     To  optimize  the  various  hyperparameters  of\\nthe models, we use ten-fold cross validation (CV) and recalculate the results ten times\\nto account for the inherent randomness in some of the models and the CV process.\\nWe will evaluate algorithms using the mean squared error metric. This metric gives\\nan idea of the performance of the supervised regression models. All these concepts,\\nincluding cross validation and evaluation metrics, have been described in Chapter 4:\\n\\nnum_folds = 10\\nscoring = \\'neg_mean_squared_error\\'\\n\\n5.3. Compare models and algorithms.    Now that we have completed the data loading and\\ndesigned the test harness, we need to choose a model.\\n\\n5.3.1. Machine learning models from Scikit-learn.     In  this  step,  the  supervised  regression\\nmodels are implemented using the sklearn package:\\n\\nRegression and tree regression algorithms\\n\\nmodels = []\\nmodels.append((\\'LR\\', LinearRegression()))\\nmodels.append((\\'LASSO\\', Lasso()))\\nmodels.append((\\'EN\\', ElasticNet()))\\nmodels.append((\\'KNN\\', KNeighborsRegressor()))\\nmodels.append((\\'CART\\', DecisionTreeRegressor()))\\nmodels.append((\\'SVR\\', SVR()))\\n\\nNeural network algorithms\\n\\nmodels.append((\\'MLP\\', MLPRegressor()))\\n\\nEnsemble models\\n\\n# Boosting methods\\nmodels.append((\\'ABR\\', AdaBoostRegressor()))\\nmodels.append((\\'GBR\\', GradientBoostingRegressor()))\\n# Bagging methods\\nmodels.append((\\'RFR\\', RandomForestRegressor()))\\nmodels.append((\\'ETR\\', ExtraTreesRegressor()))\\n\\n104 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cOnce we have selected all the models, we loop over each of them. First, we run the k-\\nfold analysis. Next, we run the model on the entire training and testing dataset.\\n\\nAll  the  algorithms  use  default  tuning  parameters.  We  will  calculate  the  mean  and\\nstandard deviation of the evaluation metric for each algorithm and collect the results\\nfor model comparison later:\\n\\nnames = []\\nkfold_results = []\\ntest_results = []\\ntrain_results = []\\nfor name, model in models:\\n    names.append(name)\\n    ## k-fold analysis:\\n    kfold = KFold(n_splits=num_folds, random_state=seed)\\n    #converted mean squared error to positive. The lower the better\\n    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, \\\\\\n      scoring=scoring)\\n    kfold_results.append(cv_results)\\n    # Full Training period\\n    res = model.fit(X_train, Y_train)\\n    train_result = mean_squared_error(res.predict(X_train), Y_train)\\n    train_results.append(train_result)\\n    # Test results\\n    test_result = mean_squared_error(res.predict(X_test), Y_test)\\n    test_results.append(test_result)\\n\\nLet’s compare the algorithms by looking at the cross validation results:\\n\\nCross validation results\\n\\nfig = pyplot.figure()\\nfig.suptitle(\\'Algorithm Comparison: Kfold results\\')\\nax = fig.add_subplot(111)\\npyplot.boxplot(kfold_results)\\nax.set_xticklabels(names)\\nfig.set_size_inches(15,8)\\npyplot.show()\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n105\\n\\n\\x0cOutput\\n\\nAlthough  the  results  of  a  couple  of  the  models  look  good,  we  see  that  the  linear\\nregression and the regularized regression including the lasso regression (LASSO) and\\nelastic  net  (EN)  seem  to  perform  best.  This  indicates  a  strong  linear  relationship\\nbetween  the  dependent  and  independent  variables.  Going  back  to  the  exploratory\\nanalysis, we saw a good correlation and linear relationship of the target variables with\\nthe different lagged MSFT variables.\\n\\nLet us look at the errors of the test set as well:\\n\\nTraining and test error\\n\\n# compare algorithms\\nfig = pyplot.figure()\\n\\nind = np.arange(len(names))  # the x locations for the groups\\nwidth = 0.35  # the width of the bars\\n\\nfig.suptitle(\\'Algorithm Comparison\\')\\nax = fig.add_subplot(111)\\npyplot.bar(ind - width/2, train_results,  width=width, label=\\'Train Error\\')\\npyplot.bar(ind + width/2, test_results, width=width, label=\\'Test Error\\')\\nfig.set_size_inches(15,8)\\npyplot.legend()\\nax.set_xticks(ind)\\nax.set_xticklabels(names)\\npyplot.show()\\n\\n106 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cOutput\\n\\nExamining the training and test error, we still see a stronger performance from the\\nlinear  models.  Some  of  the  algorithms,  such  as  the  decision  tree  regressor  (CART),\\noverfit on the training data and produced very high error on the test set. Ensemble\\nmodels  such  as  gradient  boosting  regression  (GBR)  and  random  forest  regression\\n(RFR) have low bias but high variance. We also see that the artificial neural network\\nalgorithm (shown as MLP in the chart) shows higher errors in both the training and\\ntest sets. This is perhaps due to the linear relationship of the variables not captured\\naccurately by ANN, improper hyperparameters, or insufficient training of the model.\\nOur original intuition from the cross validation results and the scatterplots also seem\\nto demonstrate a better performance of linear models.\\n\\nWe now look at some of the time series and deep learning models that can be used.\\nOnce we are done creating these, we will compare their performance against that of\\nthe supervised regression–based models. Due to the nature of time series models, we\\nare  not  able  to  run  a  k-fold  analysis.  We  can  still  compare  our  results  to  the  other\\nmodels based on the full training and testing results.\\n\\n5.3.2. Time series–based models: ARIMA and LSTM.     The  models  used  so  far  already\\nembed the time series component by using a time-delay approach, where the lagged\\nvariable is included as one of the independent variables. However, for the time ser‐\\nies–based  models  we  do  not  need  the  lagged  variables  of  MSFT  as  the  independent\\nvariables. Hence, as a first step we remove MSFT’s previous returns for these models.\\nWe use all other variables as the exogenous variables in these models.\\n\\nLet us first prepare the dataset for ARIMA models by having only the correlated var‐\\nriables as exogenous variables:\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n107\\n\\n\\x0cX_train_ARIMA=X_train.loc[:, [\\'GOOGL\\', \\'IBM\\', \\'DEXJPUS\\', \\'SP500\\', \\'DJIA\\', \\\\\\n\\'VIXCLS\\']]\\nX_test_ARIMA=X_test.loc[:, [\\'GOOGL\\', \\'IBM\\', \\'DEXJPUS\\', \\'SP500\\', \\'DJIA\\', \\\\\\n\\'VIXCLS\\']]\\ntr_len = len(X_train_ARIMA)\\nte_len = len(X_test_ARIMA)\\nto_len = len (X)\\n\\nWe now configure the ARIMA model with the order (1,0,0) and use the independent\\nvariables as the exogenous variables in the model. The version of the ARIMA model\\nwhere the exogenous variables are also used is known as the ARIMAX model, where\\n\"X\" represents exogenous variables:\\n\\nmodelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[1,0,0])\\nmodel_fit = modelARIMA.fit()\\n\\nNow we fit the ARIMA model:\\n\\nerror_Training_ARIMA = mean_squared_error(Y_train, model_fit.fittedvalues)\\npredicted = model_fit.predict(start = tr_len -1 ,end = to_len -1, \\\\\\n  exog = X_test_ARIMA)[1:]\\nerror_Test_ARIMA = mean_squared_error(Y_test,predicted)\\nerror_Test_ARIMA\\n\\nOutput\\n\\n0.0005931919240399084\\n\\nError of this ARIMA model is reasonable.\\n\\nNow let’s prepare the dataset for the LSTM model. We need the data in the form of\\narrays of all the input variables and the output variables.\\n\\nThe logic behind the LSTM is that data is taken from the previous day (the data of all\\nthe other features for that day—correlated assets and the lagged variables of MSFT)\\nand we try to predict the next day. Then we move the one-day window with one day\\nand again predict the next day. We iterate like this over the whole dataset (of course\\nin batches). The code below will create a dataset in which X is the set of independent\\nvariables at a given time (t) and Y is the target variable at the next time (t + 1):\\n\\nseq_len = 2 #Length of the seq for the LSTM\\n\\nY_train_LSTM, Y_test_LSTM = np.array(Y_train)[seq_len-1:], np.array(Y_test)\\nX_train_LSTM = np.zeros((X_train.shape[0]+1-seq_len, seq_len, X_train.shape[1]))\\nX_test_LSTM = np.zeros((X_test.shape[0], seq_len, X.shape[1]))\\nfor i in range(seq_len):\\n    X_train_LSTM[:, i, :] = np.array(X_train)[i:X_train.shape[0]+i+1-seq_len, :]\\n    X_test_LSTM[:, i, :] = np.array(X)\\\\\\n    [X_train.shape[0]+i-1:X.shape[0]+i+1-seq_len, :]\\n\\nIn  the  next  step,  we  create  the  LSTM  architecture.  As  we  can  see,  the  input  of  the\\nLSTM  is  in  X_train_LSTM,  which  goes  into  50  hidden  units  in  the  LSTM  layer  and\\nthen is transformed to a single output—the stock return value. The hyperparameters\\n\\n108 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0c(i.e., learning rate, optimizer, activation function, etc.) were discussed in Chapter 3 of\\nthe book:\\n\\n# LSTM Network\\ndef create_LSTMmodel(learn_rate = 0.01, momentum=0):\\n        # create model\\n    model = Sequential()\\n    model.add(LSTM(50, input_shape=(X_train_LSTM.shape[1],\\\\\\n      X_train_LSTM.shape[2])))\\n    #More cells can be added if needed\\n    model.add(Dense(1))\\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\\n    model.compile(loss=\\'mse\\', optimizer=\\'adam\\')\\n    return model\\nLSTMModel = create_LSTMmodel(learn_rate = 0.01, momentum=0)\\nLSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM, \\\\\\n  validation_data=(X_test_LSTM, Y_test_LSTM),\\\\\\n  epochs=330, batch_size=72, verbose=0, shuffle=False)\\n\\nNow we fit the LSTM model with the data and look at the change in the model per‐\\nformance metric over time simultaneously in the training set and the test set:\\n\\npyplot.plot(LSTMModel_fit.history[\\'loss\\'], label=\\'train\\', )\\npyplot.plot(LSTMModel_fit.history[\\'val_loss\\'], \\'--\\',label=\\'test\\',)\\npyplot.legend()\\npyplot.show()\\n\\nOutput\\n\\nerror_Training_LSTM = mean_squared_error(Y_train_LSTM,\\\\\\n  LSTMModel.predict(X_train_LSTM))\\npredicted = LSTMModel.predict(X_test_LSTM)\\nerror_Test_LSTM = mean_squared_error(Y_test,predicted)\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n109\\n\\n\\x0cNow, in order to compare the time series and the deep learning models, we append\\nthe result of these models to the results of the supervised regression–based models:\\n\\ntest_results.append(error_Test_ARIMA)\\ntest_results.append(error_Test_LSTM)\\n\\ntrain_results.append(error_Training_ARIMA)\\ntrain_results.append(error_Training_LSTM)\\n\\nnames.append(\"ARIMA\")\\nnames.append(\"LSTM\")\\n\\nOutput\\n\\nLooking at the chart, we find the time series–based ARIMA model comparable to the\\nlinear supervised regression models: linear regression (LR), lasso regression (LASSO),\\nand  elastic  net  (EN).  This  can  primarily  be  due  to  the  strong  linear  relationship  as\\ndiscussed before. The LSTM model performs decently; however, the ARIMA model\\noutperforms the LSTM model in the test set. Hence, we select the ARIMA model for\\nmodel tuning.\\n\\n6. Model tuning and grid search\\n\\nLet us perform the model tuning of the ARIMA model.\\n\\n110 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cModel Tuning for the Supervised Learning or Time Series Models\\n\\nThe  detailed  implementation  of  grid  search  for  all  the  supervised\\nlearning–based models, along with the ARIMA and LSTM models,\\nis  provided  in  the  Regression-Master  template  under  the  GitHub\\nrepository  for  this  book.  For  the  grid  search  of  the  ARIMA  and\\nLSTM models, refer to the “ARIMA and LSTM Grid Search” sec‐\\ntion of the Regression-Master template.\\n\\nThe ARIMA model is generally represented as ARIMA(p,d,q) model, where p is the\\norder of the autoregressive part, d is the degree of first differencing involved, and q is\\nthe  order  of  the  moving  average  part.  The  order  of  the  ARIMA  model  was  set  to\\n(1,0,0).  So  we  perform  a  grid  search  with  different  p,  d,  and  q  combinations  in  the\\nARIMA model’s order and select the combination that minimizes the fitting error:\\n\\ndef evaluate_arima_model(arima_order):\\n    #predicted = list()\\n    modelARIMA=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=arima_order)\\n    model_fit = modelARIMA.fit()\\n    error = mean_squared_error(Y_train, model_fit.fittedvalues)\\n    return error\\n\\n# evaluate combinations of p, d and q values for an ARIMA model\\ndef evaluate_models(p_values, d_values, q_values):\\n    best_score, best_cfg = float(\"inf\"), None\\n    for p in p_values:\\n        for d in d_values:\\n            for q in q_values:\\n                order = (p,d,q)\\n                try:\\n                    mse = evaluate_arima_model(order)\\n                    if mse < best_score:\\n                        best_score, best_cfg = mse, order\\n                    print(\\'ARIMA%s MSE=%.7f\\' % (order,mse))\\n                except:\\n                    continue\\n    print(\\'Best ARIMA%s MSE=%.7f\\' % (best_cfg, best_score))\\n\\n# evaluate parameters\\np_values = [0, 1, 2]\\nd_values = range(0, 2)\\nq_values = range(0, 2)\\nwarnings.filterwarnings(\"ignore\")\\nevaluate_models(p_values, d_values, q_values)\\n\\nOutput\\n\\nARIMA(0, 0, 0) MSE=0.0009879\\nARIMA(0, 0, 1) MSE=0.0009721\\nARIMA(1, 0, 0) MSE=0.0009696\\nARIMA(1, 0, 1) MSE=0.0009685\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n111\\n\\n\\x0cARIMA(2, 0, 0) MSE=0.0009684\\nARIMA(2, 0, 1) MSE=0.0009683\\nBest ARIMA(2, 0, 1) MSE=0.0009683\\n\\nWe see that the ARIMA model with the order (2,0,1) is the best performer out of all\\nthe  combinations  tested  in  the  grid  search,  although  there  isn’t  a  significant  differ‐\\nence in the mean squared error (MSE) with other combinations. This means that the\\nmodel with the autoregressive lag of two and moving average of one yields the best\\nresult. We should not forget the fact that there are other exogenous variables in the\\nmodel that influence the order of the best ARIMA model as well.\\n\\n7. Finalize the model\\n\\nIn the last step we will check the finalized model on the test set.\\n\\n7.1. Results on the test dataset.   \\n\\n# prepare model\\nmodelARIMA_tuned=ARIMA(endog=Y_train,exog=X_train_ARIMA,order=[2,0,1])\\nmodel_fit_tuned = modelARIMA_tuned.fit()\\n\\n# estimate accuracy on validation set\\npredicted_tuned = model_fit.predict(start = tr_len -1 ,\\\\\\n  end = to_len -1, exog = X_test_ARIMA)[1:]\\nprint(mean_squared_error(Y_test,predicted_tuned))\\n\\nOutput\\n\\n0.0005970582461404503\\n\\nThe MSE of the model on the test set looks good and is actually less than that of the\\ntraining set.\\n\\nIn the last step, we will visualize the output of the selected model and compare the\\nmodeled data against the actual data. In order to visualize the chart, we convert the\\nreturn time series to a price time series. We also assume the price at the beginning of\\nthe test set as one for the sake of simplicity. Let us look at the plot of actual versus\\npredicted data:\\n\\n# plotting the actual data versus predicted data\\npredicted_tuned.index = Y_test.index\\npyplot.plot(np.exp(Y_test).cumprod(), \\'r\\', label=\\'actual\\',)\\n\\n# plotting t, a separately\\npyplot.plot(np.exp(predicted_tuned).cumprod(), \\'b--\\', label=\\'predicted\\')\\npyplot.legend()\\npyplot.rcParams[\"figure.figsize\"] = (8,5)\\npyplot.show()\\n\\n112 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cLooking  at  the  chart,  we  clearly  see  the  trend  has  been  captured  perfectly  by  the\\nmodel. The predicted series is less volatile compared to the actual time series, and it\\naligns with the actual data for the first few months of the test set. A point to note is\\nthat  the  purpose  of  the  model  is  to  compute  the  next  day’s  return  given  the  data\\nobserved up to the present day, and not to predict the stock price several days in the\\nfuture given the current data. Hence, a deviation from the actual data is expected as\\nwe move away from the beginning of the test set. The model seems to perform well\\nfor the first few months, with deviation from the actual data increasing six to seven\\nmonths after the beginning of the test set.\\n\\nConclusion\\n\\nWe can conclude that simple models—linear regression, regularized regression (i.e.,\\nLasso and elastic net)—along with the time series models, such as ARIMA, are prom‐\\nising modeling approaches for stock price prediction problems. This approach helps\\nus deal with overfitting and underfitting, which are some of the key challenges in pre‐\\ndicting problems in finance.\\n\\nWe should also note that we can use a wider set of indicators, such as P/E ratio, trad‐\\ning volume, technical indicators, or news data, which might lead to better results. We\\nwill demonstrate this in some of the future case studies in the book.\\n\\nOverall, we created a supervised-regression and time series modeling framework that\\nallows  us  to  perform  stock  price  prediction  using  historical  data.  This  framework\\ngenerates results to analyze risk and profitability before risking any capital.\\n\\nCase Study 1: Stock Price Prediction \\n\\n| \\n\\n113\\n\\n\\x0cCase Study 2: Derivative Pricing\\nIn  computational  finance  and  risk  management,  several  numerical  methods  (e.g.,\\nfinite differences, fourier methods, and Monte Carlo simulation) are commonly used\\nfor the valuation of financial derivatives.\\n\\nThe Black-Scholes formula is probably one of the most widely cited and used models\\nin derivative pricing. Numerous variations and extensions of this formula are used to\\nprice  many  kinds  of  financial  derivatives.  However,  the  model  is  based  on  several\\nassumptions. It assumes a specific form of movement for the derivative price, namely\\na  Geometric  Brownian  Motion  (GBM).  It  also  assumes  a  conditional  payment  at\\nmaturity of the option and economic constraints, such as no-arbitrage. Several other\\nderivative  pricing  models  have  similarly  impractical  model  assumptions.  Finance\\npractitioners are well aware that these assumptions are violated in practice, and prices\\nfrom these models are further adjusted using practitioner judgment.\\n\\nAnother  aspect  of  the  many  traditional  derivative  pricing  models  is  model  calibra‐\\ntion, which is typically done not by historical asset prices but by means of derivative\\nprices (i.e., by matching the market prices of heavily traded options to the derivative\\nprices from the mathematical model). In the process of model calibration, thousands\\nof derivative prices need to be determined in order to fit the parameters of the model,\\nand  the  overall  process  is  time  consuming.  Efficient  numerical  computation  is\\nincreasingly  important  in  financial  risk  management,  especially  when  we  deal  with\\nreal-time  risk  management  (e.g.,  high  frequency  trading).  However,  due  to  the\\nrequirement of a highly efficient computation, certain high-quality asset models and\\nmethodologies are discarded during model calibration of traditional derivative pric‐\\ning models.\\n\\nMachine learning can potentially be used to tackle these drawbacks related to imprac‐\\ntical  model  assumptions  and  inefficient  model  calibration.  Machine  learning  algo‐\\nrithms have the ability to tackle more nuances with very few theoretical assumptions\\nand can be effectively used for derivative pricing, even in a world with frictions. With\\nthe advancements in hardware, we can train machine learning models on high per‐\\nformance CPUs, GPUs, and other specialized hardware to achieve a speed increase of\\nseveral orders of magnitude as compared to the traditional derivative pricing models.\\n\\nAdditionally,  market  data  is  plentiful,  so  it  is  possible  to  train  a  machine  learning\\nalgorithm to learn the function that is collectively generating derivative prices in the\\nmarket.  Machine  learning  models  can  capture  subtle  nonlinearities  in  the  data  that\\nare not obtainable through other statistical approaches.\\n\\nIn this case study, we look at derivative pricing from a machine learning standpoint\\nand use a supervised regression–based model to price an option from simulated data.\\nThe main idea here is to come up with a machine learning framework for derivative\\npricing. Achieving a machine learning model with high accuracy would mean that we\\n\\n114 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0ccan  leverage  the  efficient  numerical  calculation  of  machine  learning  for  derivative\\npricing with fewer underlying model assumptions.\\n\\nIn this case study, we will focus on:\\n\\n• Developing a machine learning–based framework for derivative pricing.\\n\\n• Comparison of linear and nonlinear supervised regression models in the context\\n\\nof derivative pricing.\\n\\nBlueprint for Developing a Machine Learning Model for\\nDerivative Pricing\\n\\n1. Problem definition\\n\\nIn  the  supervised  regression  framework  we  used  for  this  case  study,  the  predicted\\nvariable  is  the  price  of  the  option,  and  the  predictor  variables  are  the  market  data\\nused as inputs to the Black-Scholes option pricing model.\\n\\nThe variables selected to estimate the market price of the option are stock price, strike\\nprice,  time  to  expiration,  volatility,  interest  rate,  and  dividend  yield.  The  predicted\\nvariable for this case study was generated using random inputs and feeding them into\\nthe well-known Black-Scholes model.12\\n\\nThe  price  of  a  call  option  per  the  Black-Scholes  option  pricing  model  is  defined  in\\nEquation 5-1.\\n\\nEquation 5-1. Black-Scholes equation for call option\\n\\nSe –qτΦ(d1) – e –rτKΦ(d2)\\n\\nwith\\n\\nd1 =\\n\\nand\\n\\nln (S / K ) + (r – q + σ 2 / 2)τ\\n\\nσ\\n\\nτ\\n\\n12 The predicted variable, which is the option price, should ideally be directly obtained for the market. Given\\n\\nthis case study is more for demonstration purposes, we use model-generated option price for the sake of con‐\\nvenience.\\n\\nCase Study 2: Derivative Pricing \\n\\n| \\n\\n115\\n\\n\\x0cd2 =\\n\\nln (S / K ) + (r – q – σ 2 / 2)τ\\n\\nσ\\n\\nτ\\n\\n= d1 – σ τ\\n\\nwhere we have stock price S ; strike price K ; risk-free rate r; annual dividend yield q;\\ntime to maturity τ = T – t (represented as a unitless fraction of one year); and vola‐\\ntility σ.\\n\\nTo make the logic simpler, we define moneyness as M = K / S  and look at the prices\\nin terms of per unit of current stock price. We also set q as 0.\\n\\nThis simplifies the formula to the following:\\n\\ne –qτΦ( – ln (M ) + (r + σ 2 / 2)τ\\n\\nσ\\n\\nτ\\n\\n) – e –rτMΦ( – ln (M ) + (r – σ 2 / 2)τ\\n\\nσ\\n\\nτ\\n\\n)\\n\\nLooking at the equation above, the parameters that feed into the Black-Scholes option\\npricing model are moneyness, risk-free rate, volatility, and time to maturity.\\n\\nThe  parameter  that  plays  the  central  role  in  derivative  market  is  volatility,  as  it  is\\ndirectly related to the movement of the stock prices. With the increase in the volatil‐\\nity, the range of share price movements becomes much wider than that of a low vola‐\\ntility stock.\\n\\nIn the options market, there isn’t a single volatility used to price all the options. This\\nvolatility depends on the option moneyness and time to maturity. In general, the vol‐\\natility increases with higher time to maturity and with moneyness. This behavior is\\nreferred to as volatility smile/skew. We often derive the volatility from the price of the\\noptions existing in the market, and this volatility is referred to as “implied” volatility.\\nIn this exercise, we assume the structure of the volatility surface and use function in\\nEquation 5-2, where volatility depends on the option moneyness and time to matur‐\\nity to generate the option volatility surface.\\n\\nEquation 5-2. Equation for vloatility\\n\\nσ(M , τ) = σ0 + ατ + β(M – 1)2\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  loading  of  Python  packages  is  similar  to  case\\nstudy  1  in  this  chapter.  Please  refer  to  the  Jupyter  notebook  of  this  case  study  for\\nmore details.\\n\\n2.2. Defining functions and parameters.    To generate the dataset, we need to simulate the\\ninput parameters and then create the predicted variable.\\n\\n116 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cAs a first step we define the constant parameters. The constant parameters required\\nfor the volatility surface are defined below. These parameters are not expected to have\\na significant impact on the option price; therefore, these parameters are set to some\\nmeaningful values:\\n\\ntrue_alpha = 0.1\\ntrue_beta = 0.1\\ntrue_sigma0 = 0.2\\n\\nThe  risk-free  rate,  which  is  an  input  to  the  Black-Scholes  option  pricing  model,  is\\ndefined as follows:\\n\\nrisk_free_rate = 0.05\\n\\nVolatility and option pricing functions.     In  this  step  we  define  the  function  to  compute\\nthe volatility and price of a call option as per Equations 5-1 and 5-2:\\n\\ndef option_vol_from_surface(moneyness, time_to_maturity):\\n    return true_sigma0 + true_alpha * time_to_maturity +\\\\\\n     true_beta * np.square(moneyness - 1)\\n\\ndef call_option_price(moneyness, time_to_maturity, option_vol):\\n    d1=(np.log(1/moneyness)+(risk_free_rate+np.square(option_vol))*\\\\\\n    time_to_maturity)/ (option_vol*np.sqrt(time_to_maturity))\\n    d2=(np.log(1/moneyness)+(risk_free_rate-np.square(option_vol))*\\\\\\n    time_to_maturity)/(option_vol*np.sqrt(time_to_maturity))\\n    N_d1 = norm.cdf(d1)\\n    N_d2 = norm.cdf(d2)\\n\\n    return N_d1 - moneyness * np.exp(-risk_free_rate*time_to_maturity) * N_d2\\n\\n2.3. Data generation.     We  generate  the  input  and  output  variables  in  the  following\\nsteps:\\n\\n• Time to maturity (Ts) is generated using the np.random.random function, which\\n\\ngenerates a uniform random variable between zero and one.\\n\\n• Moneyness (Ks) is generated using the np.random.randn function, which gener‐\\nates a normally distributed random variable. The random number multiplied by\\n0.25 generates the deviation of strike from spot price,13 and the overall equation\\nensures that the moneyness is greater than zero.\\n\\n• Volatility (sigma) is generated as a function of time to maturity and moneyness\\n\\nusing Equation 5-2.\\n\\n• The  option  price  is  generated  using  Equation  5-1  for  the  Black-Scholes  option\\n\\nprice.\\n\\n13 When the spot price is equal to the strike price, at-the-money option.\\n\\nCase Study 2: Derivative Pricing \\n\\n| \\n\\n117\\n\\n\\x0cIn total we generate 10,000 data points (N):\\n\\nN = 10000\\n\\nKs = 1+0.25*np.random.randn(N)\\nTs = np.random.random(N)\\nSigmas = np.array([option_vol_from_surface(k,t) for k,t in zip(Ks,Ts)])\\nPs = np.array([call_option_price(k,t,sig) for k,t,sig in zip(Ks,Ts,Sigmas)])\\n\\nNow we create the variables for predicted and predictor variables:\\n\\nY = Ps\\nX = np.concatenate([Ks.reshape(-1,1), Ts.reshape(-1,1), Sigmas.reshape(-1,1)], \\\\\\naxis=1)\\n\\ndataset = pd.DataFrame(np.concatenate([Y.reshape(-1,1), X], axis=1),\\n                       columns=[\\'Price\\', \\'Moneyness\\', \\'Time\\', \\'Vol\\'])\\n\\n3. Exploratory data analysis\\n\\nLet’s have a look at the dataset we have.\\n\\n3.1. Descriptive statistics.   \\ndataset.head()\\n\\nOutput\\n\\nPrice\\n1.390e-01\\n\\nMoneyness\\n0.898\\n\\nTime\\n0.221\\n\\nVol\\n0.223\\n\\n3.814e-06\\n\\n1.223\\n\\n1.409e-01\\n\\n0.969\\n\\n1.984e-01\\n\\n0.950\\n\\n2.495e-01\\n\\n0.914\\n\\n0.052\\n\\n0.210\\n\\n0.391\\n\\n0.239\\n\\n0.628\\n\\n0.263\\n\\n0.810\\n\\n0.282\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\nThe dataset contains price—which is the price of the option and is the predicted vari‐\\nable—along with moneyness (the ratio of strike and spot price), time to maturity, and\\nvolatility, which are the features in the model.\\n\\n3.2. Data visualization.    In this step we look at scatterplot to understand the interaction\\nbetween different variables:14\\n\\npyplot.figure(figsize=(15,15))\\nscatter_matrix(dataset,figsize=(12,12))\\npyplot.show()\\n\\n14 Refer to the Jupyter notebook of this case study to go through other charts such as histogram plot and corre‐\\n\\nlation plot.\\n\\n118 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cOutput\\n\\nThe  scatterplot  reveals  very  interesting  dependencies  and  relationships  between  the\\nvariables. Let us look at the first row of the chart to see the relationship of price to\\ndifferent variables. We observe that as moneyness decreases (i.e., strike price decrea‐\\nses as compared to the stock price), there is an increase in the price, which is in line\\nwith the rationale described in the previous section. Looking at the price versus time\\nto maturity, we see an increase in the option price. The price versus volatility chart\\nalso shows an increase in the price with the volatility. However, option price seems to\\nexhibit a nonlinear relationship with most of the variables. This means that we expect\\nour nonlinear models to do a better job than our linear models.\\n\\nAnother interesting relationship is between volatility and strike. The more we deviate\\nfrom  the  moneyness  of  one,  the  higher  the  volatility  we  observe.  This  behavior  is\\nshown  due  to  the  volatility  function  we  defined  before  and  illustrates  the  volatility\\nsmile/skew.\\n\\nCase Study 2: Derivative Pricing \\n\\n| \\n\\n119\\n\\n\\x0c4. Data preparation and analysis\\n\\nWe  performed  most  of  the  data  preparation  steps  (i.e.,  getting  the  dependent  and\\nindependent variables) in the preceding sections. In this step we look at the feature\\nimportance.\\n\\n4.1. Univariate feature selection.    We start by looking at each feature individually and,\\nusing the single variable regression fit as the criteria, look at the most important vari‐\\nables:\\n\\nbestfeatures = SelectKBest(k=\\'all\\', score_func=f_regression)\\nfit = bestfeatures.fit(X,Y)\\ndfscores = pd.DataFrame(fit.scores_)\\ndfcolumns = pd.DataFrame([\\'Moneyness\\', \\'Time\\', \\'Vol\\'])\\n#concat two dataframes for better visualization\\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\\nfeatureScores.columns = [\\'Specs\\',\\'Score\\']  #naming the dataframe columns\\nfeatureScores.nlargest(10,\\'Score\\').set_index(\\'Specs\\')\\n\\nOutput\\n\\nMoneyness : 30282.309\\nVol : 2407.757\\nTime : 1597.452\\n\\nWe observe that the moneyness is the most important variable for the option price,\\nfollowed by volatility and time to maturity. Given there are only three predictor vari‐\\nables, we retain all the variables for modeling.\\n\\n5. Evaluate models\\n\\n5.1. Train-test split and evaluation metrics.     First,  we  separate  the  training  set  and  test\\nset:\\n\\nvalidation_size = 0.2\\n\\ntrain_size = int(len(X) * (1-validation_size))\\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]\\n\\nWe use the prebuilt sklearn models to run a k-fold analysis on our training data. We\\nthen train the model on the full training data and use it for prediction of the test data.\\nWe will evaluate algorithms using the mean squared error metric. The parameters for\\nthe k-fold analysis and evaluation metrics are defined as follows:\\n\\nnum_folds = 10\\nseed = 7\\nscoring = \\'neg_mean_squared_error\\'\\n\\n120 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0c5.2. Compare models and algorithms.    Now that we have completed the data loading and\\nhave  designed  the  test  harness,  we  need  to  choose  a  model  out  of  the  suite  of  the\\nsupervised regression models.\\n\\nLinear models and regression trees\\n\\nmodels = []\\nmodels.append((\\'LR\\', LinearRegression()))\\nmodels.append((\\'KNN\\', KNeighborsRegressor()))\\nmodels.append((\\'CART\\', DecisionTreeRegressor()))\\nmodels.append((\\'SVR\\', SVR()))\\n\\nArtificial neural network\\n\\nmodels.append((\\'MLP\\', MLPRegressor()))\\n\\nBoosting and bagging methods\\n\\n# Boosting methods\\nmodels.append((\\'ABR\\', AdaBoostRegressor()))\\nmodels.append((\\'GBR\\', GradientBoostingRegressor()))\\n# Bagging methods\\nmodels.append((\\'RFR\\', RandomForestRegressor()))\\nmodels.append((\\'ETR\\', ExtraTreesRegressor()))\\n\\nOnce we have selected all the models, we loop over each of them. First, we run the k-\\nfold analysis. Next, we run the model on the entire training and testing dataset.\\n\\nThe algorithms use default tuning parameters. We will calculate the mean and stan‐\\ndard deviation of error metric and save the results for use later.\\n\\nOutput\\n\\nCase Study 2: Derivative Pricing \\n\\n| \\n\\n121\\n\\n\\x0cThe  Python  code  for  the  k-fold  analysis  step  is  similar  to  that  used  in  case  study  1.\\nReaders can also refer to the Jupyter notebook of this case study in the code reposi‐\\ntory for more details. Let us look at the performance of the models in the training set.\\n\\nWe see clearly that the nonlinear models, including classification and regression tree\\n(CART), ensemble models, and artificial neural network (represented by MLP in the\\nchart above), perform a lot better that the linear algorithms. This is intuitive given the\\nnonlinear relationships we observed in the scatterplot.\\n\\nArtificial neural networks (ANN) have the natural ability to model any function with\\nfast experimentation and deployment times (definition, training, testing, inference).\\nANN can effectively be used in complex derivative pricing situations. Hence, out of\\nall the models with good performance, we choose ANN for further analysis.\\n\\n6. Model tuning and finalizing the model\\n\\nDetermining the proper number of nodes for the middle layer of an ANN is more of\\nan art than a science, as discussed in Chapter 3. Too many nodes in the middle layer,\\nand thus too many connections, produce a neural network that memorizes the input\\ndata and lacks the ability to generalize. Therefore, increasing the number of nodes in\\nthe middle layer will improve performance on the training set, while decreasing the\\nnumber of nodes in the middle layer will improve performance on a new dataset.\\n\\nAs discussed in Chapter 3, the ANN model has several other hyperparameters such as\\nlearning rate, momentum, activation function, number of epochs, and batch size. All\\nthese hyperparameters can be tuned during the grid search process. However, in this\\nstep, we stick to performing grid search on the number of hidden layers for the pur‐\\npose of simplicity. The approach to perform grid search on other hyperparameters is\\nthe same as described in the following code snippet:\\n\\n\\'\\'\\'\\nhidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\\n    The ith element represents the number of neurons in the ith\\n    hidden layer.\\n\\'\\'\\'\\nparam_grid={\\'hidden_layer_sizes\\': [(20,), (50,), (20,20), (20, 30, 20)]}\\nmodel = MLPRegressor()\\nkfold = KFold(n_splits=num_folds, random_state=seed)\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\\\\n  cv=kfold)\\ngrid_result = grid.fit(X_train, Y_train)\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\nmeans = grid_result.cv_results_[\\'mean_test_score\\']\\nstds = grid_result.cv_results_[\\'std_test_score\\']\\nparams = grid_result.cv_results_[\\'params\\']\\nfor mean, stdev, param in zip(means, stds, params):\\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\\n\\n122 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cOutput\\n\\nBest: -0.000024 using {\\'hidden_layer_sizes\\': (20, 30, 20)}\\n-0.000580 (0.000601) with: {\\'hidden_layer_sizes\\': (20,)}\\n-0.000078 (0.000041) with: {\\'hidden_layer_sizes\\': (50,)}\\n-0.000090 (0.000140) with: {\\'hidden_layer_sizes\\': (20, 20)}\\n-0.000024 (0.000011) with: {\\'hidden_layer_sizes\\': (20, 30, 20)}\\n\\nThe  best  model  has  three  layers,  with  20,  30,  and  20  nodes  in  each  hidden  layer,\\nrespectively. Hence, we prepare a model with this configuration and check its perfor‐\\nmance on the test set. This is a crucial step, because a greater number of layers may\\nlead to overfitting and have poor performance in the test set.\\n\\n# prepare model\\nmodel_tuned = MLPRegressor(hidden_layer_sizes=(20, 30, 20))\\nmodel_tuned.fit(X_train, Y_train)\\n\\n# estimate accuracy on validation set\\n# transform the validation dataset\\npredictions = model_tuned.predict(X_test)\\nprint(mean_squared_error(Y_test, predictions))\\n\\nOutput\\n\\n3.08127276609567e-05\\n\\nWe see that the root mean squared error (RMSE) is 3.08e–5, which is less than one\\ncent. Hence, the ANN model does an excellent job of fitting the Black-Scholes option\\npricing model. A greater number of layers and tuning of other hyperparameters may\\nenable the ANN model to capture the complex relationship and nonlinearity in the\\ndata even better. Overall, the results suggest that ANN may be used to train an option\\npricing model that matches market prices.\\n\\n7. Additional analysis: removing the volatility data\\n\\nAs an additional analysis, we make the process harder by trying to predict the price\\nwithout  the  volatility  data.  If  the  model  performance  is  good,  we  will  eliminate  the\\nneed to have a volatility function as described before. In this step, we further compare\\nthe  performance  of  the  linear  and  nonlinear  models.  In  the  following  code  snippet,\\nwe remove the volatility variable from the dataset of the predictor variable and define\\nthe training set and test set again:\\n\\nX = X[:, :2]\\nvalidation_size = 0.2\\ntrain_size = int(len(X) * (1-validation_size))\\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]\\n\\nNext, we run the suite of the models (except the regularized regression model) with\\nthe  new  dataset,  with  the  same  parameters  and  similar  Python  code  as  before.  The\\nperformance of all the models after removing the volatility data is as follows:\\n\\nCase Study 2: Derivative Pricing \\n\\n| \\n\\n123\\n\\n\\x0cLooking at the result, we have a similar conclusion as before and see a poor perfor‐\\nmance  of  the  linear  regression  and  good  performance  of  the  ensemble  and  ANN\\nmodels. The linear regression now does even a worse job than before. However, the\\nperformance of ANN and other ensemble models does not deviate much from their\\nprevious performance. This implies the information of the volatility is likely captured\\nin other variables, such as moneyness and time to maturity. Overall, it is good news\\nas it means that fewer variables might be needed to achieve the same performance.\\n\\nConclusion\\n\\nWe  know  that  derivative  pricing  is  a  nonlinear  problem.  As  expected,  our  linear\\nregression  model  did  not  do  as  well  as  our  nonlinear  models,  and  the  non-linear\\nmodels  have  a  very  good  overall  performance.  We  also  observed  that  removing  the\\nvolatility increases the difficulty of the prediction problem for the linear regression.\\nHowever, the nonlinear models such as ensemble models and ANN are still able to\\ndo well at the prediction process. This does indicate that one might be able to side‐\\nstep  the  development  of  an  option  volatility  surface  and  achieve  a  good  prediction\\nwith a smaller number of variables.\\n\\nWe  saw  that  an  artificial  neural  network  (ANN)  can  reproduce  the  Black-Scholes\\noption pricing formula for a call option to a high degree of accuracy, meaning we can\\nleverage  efficient  numerical  calculation  of  machine  learning  in  derivative  pricing\\nwithout  relying  on  the  impractical  assumptions  made  in  the  traditional  derivative\\npricing models. The ANN and the related machine learning architecture can easily be\\nextended to pricing derivatives in the real world, with no knowledge of the theory of\\nderivative  pricing.  The  use  of  machine  learning  techniques  can  lead  to  much  faster\\nderivative  pricing  compared  to  traditional  derivative  pricing  models.  The  price  we\\n\\n124 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cmight have to pay for this extra speed is some loss of accuracy. However, this reduced\\naccuracy is often well within reasonable limits and acceptable from a practical point\\nof  view.  New  technology  has  commoditized  the  use  of  ANN,  so  it  might  be  worth‐\\nwhile  for  banks,  hedge  funds,  and  financial  institutions  to  explore  these  models  for\\nderivative pricing.\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors\\nThe risk tolerance of an investor is one of the most important inputs to the portfolio\\nallocation and rebalancing steps of the portfolio management process. There is a wide\\nvariety  of  risk  profiling  tools  that  take  varied  approaches  to  understanding  the  risk\\ntolerance of an investor. Most of these approaches include qualitative judgment and\\ninvolve  significant  manual  effort.  In  most  of  the  cases,  the  risk  tolerance  of  an\\ninvestor is decided based on a risk tolerance questionnaire.\\n\\nSeveral  studies  have  shown  that  these  risk  tolerance  questionnaires  are  prone  to\\nerror, as investors suffer from behavioral biases and are poor judges of their own risk\\nperception, especially during stressed markets. Also, given that these questionnaires\\nmust be manually completed by investors, they eliminate the possibility of automat‐\\ning the entire investment management process.\\n\\nSo can machine learning provide a better understanding of an investor’s risk profile\\nthan  a  risk  tolerance  questionnaire  can?  Can  machine  learning  contribute  to  auto‐\\nmating the entire portfolio management process by cutting the client out of the loop?\\nCould  an  algorithm  be  written  to  develop  a  personality  profile  for  the  client  that\\nwould  be  a  better  representation  of  how  they  would  deal  with  different  market\\nscenarios?\\n\\nThe goal of this case study is to answer these questions. We first build a supervised\\nregression–based model to predict the risk tolerance of an investor. We then build a\\nrobo-advisor  dashboard  in  Python  and  implement  the  risk  tolerance  prediction\\nmodel in the dashboard. The overall purpose is to demonstrate the automation of the\\nmanual steps in the portfolio management process with the help of machine learning.\\nThis can prove to be immensely useful, specifically for robo-advisors.\\n\\nA  dashboard  is  one  of  the  key  features  of  a  robo-advisor  as  it  provides  access  to\\nimportant  information  and  allows  users  to  interact  with  their  accounts  free  of  any\\nhuman dependency, making the portfolio management process highly efficient.\\n\\nFigure 5-6 provides a quick glance at the robo-advisor dashboard built for this case\\nstudy.  The  dashboard  performs  end-to-end  asset  allocation  for  an  investor,  embed‐\\nding the machine learning–based risk tolerance model constructed in this case study.\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors \\n\\n| \\n\\n125\\n\\n\\x0cFigure 5-6. Robo-advisor dashboard\\n\\nThis dashboard has been built in Python and is described in detail in an additional\\nstep in this case study. Although it has been built in the context of robo-advisors, it\\ncan be extended to other areas in finance and can embed the machine learning mod‐\\nels discussed in other case studies, providing finance decision makers with a graphi‐\\ncal interface for analyzing and interpreting model results.\\n\\nIn this case study, we will focus on:\\n\\n• Feature elimination and feature importance/intuition.\\n\\n• Using  machine  learning  to  automate  manual  processes  involved  in  portfolio\\n\\nmanagement process.\\n\\n• Using machine learning to quantify and model the behavioral bias of investors/\\n\\nindividuals.\\n\\n• Embedding  machine  learning  models  into  user  interfaces  or  dashboards  using\\n\\nPython.\\n\\n126 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cBlueprint for Modeling Investor Risk Tolerance and Enabling\\na Machine Learning–Based Robo-Advisor\\n\\n1. Problem definition\\n\\nIn the supervised regression framework used for this case study, the predicted vari‐\\nable  is  the  “true”  risk  tolerance  of  an  individual,15  and  the  predictor  variables  are\\ndemographic, financial, and behavioral attributes of an individual.\\n\\nThe  data  used  for  this  case  study  is  from  the  Survey  of  Consumer  Finances  (SCF),\\nwhich  is  conducted  by  the  Federal  Reserve  Board.  The  survey  includes  responses\\nabout household demographics, net worth, financial, and nonfinancial assets for the\\nsame set of individuals in 2007 (precrisis) and 2009 (postcrisis). This enables us to see\\nhow each household’s allocation changed after the 2008 global financial crisis. Refer\\nto the data dictionary for more information on this survey.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.    The details on loading the standard Python packages\\nwere  presented  in  the  previous  case  studies.  Refer  to  the  Jupyter  notebook  for  this\\ncase study for more details.\\n\\n2.2. Loading the data.     In  this  step  we  load  the  data  from  the  Survey  of  Consumer\\nFinances and look at the data shape:\\n\\n# load dataset\\ndataset = pd.read_excel(\\'SCFP2009panel.xlsx\\')\\n\\nLet us look at the size of the data:\\n\\ndataset.shape\\n\\nOutput\\n\\n(19285, 515)\\n\\nAs we can see, the dataset has a total of 19,285 observations with 515 columns. The\\nnumber of columns represents the number of features.\\n\\n15 Given that the primary purpose of the model is to be used in the portfolio management context, the individ‐\\n\\nual is also referred to as investor in the case study.\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors \\n\\n| \\n\\n127\\n\\n\\x0c3. Data preparation and feature selection\\n\\nIn this step we prepare the predicted and predictor variables to be used for modeling.\\n\\n3.1. Preparing the predicted variable.    In the first step, we prepare the predicted variable,\\nwhich is the true risk tolerance.\\n\\nThe steps to compute the true risk tolerance are as follows:\\n\\n1. Compute the risky assets and the risk-free assets for all the individuals in the sur‐\\n\\nvey data. Risky and risk-free assets are defined as follows:\\n\\nRisky assets\\n\\nInvestments in mutual funds, stocks, and bonds.\\n\\nRisk-free assets\\n\\nChecking and savings balances, certificates of deposit, and other cash balan‐\\nces and equivalents.\\n\\n2. Take the ratio of risky assets to total assets (where total assets is the sum of risky\\nand risk-free assets) of an individual and consider that as a measure of the indi‐\\nvidual’s  risk  tolerance.16  From  the  SCF,  we  have  the  data  of  risky  and  risk-free\\nassets for the individuals in 2007 and 2009. We use this data and normalize the\\nrisky assets with the price of a stock index (S&P500) in 2007 versus 2009 to get\\nrisk tolerance.\\n\\n3. Identify  the  “intelligent”  investors.  Some  literature  describes  an  intelligent\\ninvestor as one who does not change their risk tolerance during changes in the\\nmarket.  So  we  consider  the  investors  who  changed  their  risk  tolerance  by  less\\nthan 10% between 2007 and 2009 as the intelligent investors. Of course, this is a\\nqualitative judgment, and there can be several other ways of defining an intelli‐\\ngent investor. However, as mentioned before, beyond coming up with a precise\\ndefinition of true risk tolerance, the purpose of this case study is to demonstrate\\nthe usage of machine learning and provide a machine learning–based framework\\nin portfolio management that can be further leveraged for more detailed analysis.\\n\\nLet us compute the predicted variable. First, we get the risky and risk-free assets and\\ncompute the risk tolerance for 2007 and 2009 in the following code snippet:\\n\\n# Compute the risky assets and risk-free assets for 2007\\ndataset[\\'RiskFree07\\']= dataset[\\'LIQ07\\'] + dataset[\\'CDS07\\'] + dataset[\\'SAVBND07\\']\\\\\\n + dataset[\\'CASHLI07\\']\\ndataset[\\'Risky07\\'] = dataset[\\'NMMF07\\'] + dataset[\\'STOCKS07\\'] + dataset[\\'BOND07\\']\\n\\n16 There potentially can be several ways of computing the risk tolerance. In this case study, we use the intuitive\\n\\nways to measure the risk tolerance of an individual.\\n\\n128 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0c# Compute the risky assets and risk-free assets for 2009\\ndataset[\\'RiskFree09\\']= dataset[\\'LIQ09\\'] + dataset[\\'CDS09\\'] + dataset[\\'SAVBND09\\']\\\\\\n+ dataset[\\'CASHLI09\\']\\ndataset[\\'Risky09\\'] = dataset[\\'NMMF09\\'] + dataset[\\'STOCKS09\\'] + dataset[\\'BOND09\\']\\n\\n# Compute the risk tolerance for 2007\\ndataset[\\'RT07\\'] = dataset[\\'Risky07\\']/(dataset[\\'Risky07\\']+dataset[\\'RiskFree07\\'])\\n\\n#Average stock index for normalizing the risky assets in 2009\\nAverage_SP500_2007=1478\\nAverage_SP500_2009=948\\n\\n# Compute the risk tolerance for 2009\\ndataset[\\'RT09\\'] = dataset[\\'Risky09\\']/(dataset[\\'Risky09\\']+dataset[\\'RiskFree09\\'])*\\\\\\n                (Average_SP500_2009/Average_SP500_2007)\\n\\nLet us look at the details of the data:\\n\\ndataset.head()\\n\\nOutput\\n\\nThe data above displays some of the columns out of the 521 columns of the dataset.\\n\\nLet us compute the percentage change in risk tolerance between 2007 and 2009:\\n\\ndataset[\\'PercentageChange\\'] = np.abs(dataset[\\'RT09\\']/dataset[\\'RT07\\']-1)\\n\\nNext, we drop the rows containing “NA” or “NaN”:\\n\\n# Drop the rows containing NA\\ndataset=dataset.dropna(axis=0)\\n\\ndataset=dataset[~dataset.isin([np.nan, np.inf, -np.inf]).any(1)]\\n\\nLet us investigate the risk tolerance behavior of individuals in 2007 versus 2009. First\\nwe look at the risk tolerance in 2007:\\n\\nsns.distplot(dataset[\\'RT07\\'], hist=True, kde=False,\\n             bins=int(180/5), color = \\'blue\\',\\n             hist_kws={\\'edgecolor\\':\\'black\\'})\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors \\n\\n| \\n\\n129\\n\\n\\x0cOutput\\n\\nLooking at the risk tolerance in 2007, we see that a significant number of individuals\\nhad a risk tolerance close to one, meaning investments were skewed more toward the\\nrisky assets. Now let us look at the risk tolerance in 2009:\\n\\nsns.distplot(dataset[\\'RT09\\'], hist=True, kde=False,\\n             bins=int(180/5), color = \\'blue\\',\\n             hist_kws={\\'edgecolor\\':\\'black\\'})\\n\\nOutput\\n\\nClearly, the behavior of the individuals reversed after the crisis. Overall risk tolerance\\ndecreased, which is shown by the outsized proportion of households having risk tol‐\\nerance  close  to  zero  in  2009.  Most  of  the  investments  of  these  individuals  were  in\\nrisk-free assets.\\n\\n130 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cIn  the  next  step,  we  pick  the  intelligent  investors  whose  change  in  risk  tolerance\\nbetween 2007 and 2009 was less than 10%, as described in “3.1. Preparing the predic‐\\nted variable” on page 128:\\n\\ndataset3 = dataset[dataset[\\'PercentageChange\\']<=.1]\\n\\nWe  assign  the  true  risk  tolerance  as  the  average  risk  tolerance  of  these  intelligent\\ninvestors between 2007 and 2009:\\n\\ndataset3[\\'TrueRiskTolerance\\'] = (dataset3[\\'RT07\\'] + dataset3[\\'RT09\\'])/2\\n\\nThis is the predicted variable for this case study.\\n\\nLet us drop other labels that might not be needed for the prediction:\\n\\ndataset3.drop(labels=[\\'RT07\\', \\'RT09\\'], axis=1, inplace=True)\\ndataset3.drop(labels=[\\'PercentageChange\\'], axis=1, inplace=True)\\n\\n3.2. Feature selection—limit the feature space.     In  this  section,  we  will  explore  ways  to\\ncondense the feature space.\\n\\n3.2.1. Feature elimination.     To  filter  the  features  further,  we  check  the  description  in\\nthe data dictionary and keep only the features that are relevant.\\n\\nLooking at the entire data, we have more than 500 features in the dataset. However,\\nacademic literature and industry practice indicate risk tolerance is heavily influenced\\nby  investor  demographic,  financial,  and  behavioral  attributes,  such  as  age,  current\\nincome, net worth, and willingness to take risk. All these attributes were available in\\nthe dataset and are summarized in the following section. These attributes are used as\\nfeatures to predict investors’ risk tolerance.\\n\\nIn  the  dataset,  each  of  the  columns  contains  a  numeric  value  corresponding  to  the\\nvalue of the attribute. The details are as follows:\\n\\nAGE\\n\\nThere are six age categories, where 1 represents age less than 35 and 6 represents\\nage more than 75.\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors \\n\\n| \\n\\n131\\n\\n\\x0cEDUC\\n\\nThere  are  four  education  categories,  where  1  represents  no  high  school  and  4\\nrepresents college degree.\\n\\nMARRIED\\n\\nThere are two categories to represent marital status, where 1 represents married\\nand 2 represents unmarried.\\n\\nOCCU\\n\\nThis  represents  occupation  category.  A  value  of  1  represents  managerial  status\\nand 4 represents unemployed.\\n\\nKIDS\\n\\nNumber of children.\\n\\nWSAVED\\n\\nThis  represents  the  individual’s  spending  versus  income,  split  into  three  cate‐\\ngories. For example, 1 represents spending exceeded income.\\n\\nNWCAT\\n\\nThis represents net worth category. There are five categories, where 1 represents\\nnet worth less than the 25th percentile and 5 represents net worth more than the\\n90th percentile.\\n\\nINCCL\\n\\nThis  represents  income  category.  There  are  five  categories,  where  1  represents\\nincome less than $10,000 and 5 represents income more than $100,000.\\n\\nRISK\\n\\nThis represents the willingness to take risk on a scale of 1 to 4, where 1 represents\\nthe highest level of willingness to take risk.\\n\\nWe keep only the intuitive features as of 2007 and remove all the intermediate fea‐\\ntures and features related to 2009, as the variables of 2007 are the only ones required\\nfor predicting the risk tolerance:\\n\\nkeep_list2 = [\\'AGE07\\',\\'EDCL07\\',\\'MARRIED07\\',\\'KIDS07\\',\\'OCCAT107\\',\\'INCOME07\\',\\\\\\n\\'RISK07\\',\\'NETWORTH07\\',\\'TrueRiskTolerance\\']\\n\\ndrop_list2 = [col for col in dataset3.columns if col not in keep_list2]\\n\\ndataset3.drop(labels=drop_list2, axis=1, inplace=True)\\n\\n132 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cNow let us look at the correlation among the features:\\n\\n# correlation\\ncorrelation = dataset3.corr()\\nplt.figure(figsize=(15,15))\\nplt.title(\\'Correlation Matrix\\')\\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap=\\'cubehelix\\')\\n\\nOutput\\n\\nLooking  at  the  correlation  chart  (full-size  version  available  on  GitHub),  net  worth\\nand  income  are  positively  correlated  with  risk  tolerance.  With  a  greater  number  of\\nkids and marriage, risk tolerance decreases. As the willingness to take risks decreases,\\nthe  risk  tolerance  decreases.  With  age  there  is  a  positive  relationship  of  the  risk\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors \\n\\n| \\n\\n133\\n\\n\\x0ctolerance.  As  per  Hui  Wang  and  Sherman  Hanna’s  paper  “Does  Risk  Tolerance\\nDecrease with Age?,” risk tolerance increases as people age (i.e., the proportion of net\\nwealth invested in risky assets increases as people age) when other variables are held\\nconstant.\\n\\nSo in summary, the relationship of these variables with risk tolerance seems intuitive.\\n\\n4. Evaluate models\\n\\n4.1. Train-test split.    Let us split the data into training and test set:\\n\\nY= dataset3[\"TrueRiskTolerance\"]\\nX = dataset3.loc[:, dataset3.columns != \\'TrueRiskTolerance\\']\\nvalidation_size = 0.2\\nseed = 3\\nX_train, X_validation, Y_train, Y_validation = \\\\\\ntrain_test_split(X, Y, test_size=validation_size, random_state=seed)\\n\\n4.2. Test options and evaluation metrics.     We  use  R2  as  the  evaluation  metric  and  select\\n10 as the number of folds for cross validation.17\\n\\nnum_folds = 10\\nscoring = \\'r2\\'\\n\\n4.3. Compare models and algorithms.    Next,  we  select  the  suite  of  the  regression  model\\nand perform the k-folds cross validation.\\n\\nRegression Models\\n\\n# spot-check the algorithms\\nmodels = []\\nmodels.append((\\'LR\\', LinearRegression()))\\nmodels.append((\\'LASSO\\', Lasso()))\\nmodels.append((\\'EN\\', ElasticNet()))\\nmodels.append((\\'KNN\\', KNeighborsRegressor()))\\nmodels.append((\\'CART\\', DecisionTreeRegressor()))\\nmodels.append((\\'SVR\\', SVR()))\\n#Ensemble Models\\n# Boosting methods\\nmodels.append((\\'ABR\\', AdaBoostRegressor()))\\nmodels.append((\\'GBR\\', GradientBoostingRegressor()))\\n# Bagging methods\\nmodels.append((\\'RFR\\', RandomForestRegressor()))\\nmodels.append((\\'ETR\\', ExtraTreesRegressor()))\\n\\n17 We could have chosen RMSE as the evaluation metric; however, R2 was chosen as the evaluation metric given\\n\\nthat we already used RMSE as the evaluation metric in the previous case studies.\\n\\n134 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cThe Python code for the k-fold analysis step is similar to that of previous case studies.\\nReaders can also refer to the Jupyter notebook of this case study in the code reposi‐\\ntory for more details. Let us look at the performance of the models in the training set.\\n\\nThe nonlinear models perform better than the linear models, which means that there\\nis a nonlinear relationship between the risk tolerance and the variables used to pre‐\\ndict it. Given random forest regression is one of the best methods, we use it for fur‐\\nther grid search.\\n\\n5. Model tuning and grid search\\n\\nAs  discussed  in  Chapter  4,  random  forest  has  many  hyperparameters  that  can  be\\ntweaked while performing the grid search. However, we will confine our grid search\\nto  number  of  estimators  (n_estimators)  as  it  is  one  of  the  most  important  hyper‐\\nparameters.  It  represents  the  number  of  trees  in  the  random  forest  model.  Ideally,\\nthis should be increased until no further improvement is seen in the model:\\n\\n# 8. Grid search : RandomForestRegressor\\n\\'\\'\\'\\nn_estimators : integer, optional (default=10)\\n    The number of trees in the forest.\\n\\'\\'\\'\\nparam_grid = {\\'n_estimators\\': [50,100,150,200,250,300,350,400]}\\nmodel = RandomForestRegressor()\\nkfold = KFold(n_splits=num_folds, random_state=seed)\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\\\\n  cv=kfold)\\ngrid_result = grid.fit(X_train, Y_train)\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\nmeans = grid_result.cv_results_[\\'mean_test_score\\']\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors \\n\\n| \\n\\n135\\n\\n\\x0cstds = grid_result.cv_results_[\\'std_test_score\\']\\nparams = grid_result.cv_results_[\\'params\\']\\n\\nOutput\\n\\nBest: 0.738632 using {\\'n_estimators\\': 250}\\n\\nRandom forest with number of estimators as 250 is the best model after grid search.\\n\\n6. Finalize the model\\n\\nLet us look at the results on the test dataset and check the feature importance.\\n\\n6.1. Results on the test dataset.    We prepare the random forest model with the number\\nof estimators as 250:\\n\\nmodel = RandomForestRegressor(n_estimators = 250)\\nmodel.fit(X_train, Y_train)\\n\\nLet us look at the performance in the training set:\\n\\nfrom sklearn.metrics import r2_score\\npredictions_train = model.predict(X_train)\\nprint(r2_score(Y_train, predictions_train))\\n\\nOutput\\n\\n0.9640632406817223\\n\\nThe R2 of the training set is 96%, which is a good result. Now let us look at the perfor‐\\nmance in the test set:\\n\\npredictions = model.predict(X_validation)\\nprint(mean_squared_error(Y_validation, predictions))\\nprint(r2_score(Y_validation, predictions))\\n\\nOutput\\n\\n0.007781840953471237\\n0.7614494526639909\\n\\nFrom the mean squared error and R2 of 76% shown above for the test set, the random\\nforest model does an excellent job of fitting the risk tolerance.\\n\\n6.2. Feature importance and features intuition\\n\\nLet  us  look  into  the  feature  importance  of  the  variables  within  the  random  forest\\nmodel:\\n\\nimport pandas as pd\\nimport numpy as np\\nmodel = RandomForestRegressor(n_estimators= 200,n_jobs=-1)\\nmodel.fit(X_train,Y_train)\\n#use inbuilt class feature_importances of tree based classifiers\\n#plot graph of feature importances for better visualization\\n\\n136 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\\nfeat_importances.nlargest(10).plot(kind=\\'barh\\')\\nplt.show()\\n\\nOutput\\n\\nIn  the  chart,  the  x-axis  represents  the  magnitude  of  the  importance  of  a  feature.\\nHence,  income  and  net  worth,  followed  by  age  and  willingness  to  take  risk,  are  the\\nkey variables in determining risk tolerance.\\n\\n6.3. Save model for later use.     In  this  step  we  save  the  model  for  later  use.  The  saved\\nmodel can be used directly for prediction given the set of input variables. The model\\nis  saved  as  finalized_model.sav  using  the  dump  module  of  the  pickle  package.  This\\nsaved model can be loaded using the load module.\\n\\nLet’s save the model as the first step:\\n\\n# Save Model Using Pickle\\nfrom pickle import dump\\nfrom pickle import load\\n\\n# save the model to disk\\nfilename = \\'finalized_model.sav\\'\\ndump(model, open(filename, \\'wb\\'))\\n\\nNow let’s load the saved model and use it for prediction:\\n\\n# load the model from disk\\nloaded_model = load(open(filename, \\'rb\\'))\\n# estimate accuracy on validation set\\npredictions = loaded_model.predict(X_validation)\\nresult = mean_squared_error(Y_validation, predictions)\\nprint(r2_score(Y_validation, predictions))\\nprint(result)\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors \\n\\n| \\n\\n137\\n\\n\\x0cOutput\\n\\n0.7683894847939692\\n0.007555447734714956\\n\\n7. Additional step: robo-advisor dashboard\\n\\nWe mentioned the robo-advisor dashboard in the beginning of this case study. The\\nrobo-advisor dashboard performs an automation of the portfolio management pro‐\\ncess and aims to overcome the problem of traditional risk tolerance profiling.\\n\\nPython Code for Robo-Advisor Dashboard\\n\\nThis  robo-advisor  dashboard  is  built  in  Python  using  the  plotly\\ndash package. Dash is a productive Python framework for building\\nweb applications with good user interfaces. The code for the robo-\\nadvisor  dashboard  is  added  to  the  code  repository  for  this  book.\\nThe code is in a Jupyter notebook called “Sample Robo-advisor”. A\\ndetailed  description  of  the  code  is  outside  the  scope  of  this  case\\nstudy. However, the codebase can be leveraged for creation of any\\nnew machine learning–enabled dashboard.\\n\\nThe dashboard has two panels:\\n\\n• Inputs for investor characteristics\\n\\n• Asset allocation and portfolio performance\\n\\nInput for investor characteristics.     Figure  5-7  shows  the  input  panel  for  the  investor\\ncharacteristics.  This  panel  takes  all  the  input  regarding  the  investor’s  demographic,\\nfinancial,  and  behavioral  attributes.  These  inputs  are  for  the  predicted  variables  we\\nused  in  the  risk  tolerance  model  created  in  the  preceding  steps.  The  interface  is\\ndesigned to input the categorical and continuous variables in the correct format.\\n\\nOnce the inputs are submitted, we leverage the model saved in “6.3. Save model for\\nlater use” on page 137. This model takes all the inputs and produces the risk tolerance\\nof  an  investor  (refer  to  the  predict_riskTolerance  function  of  the  “Sample  Robo-\\nadvisor” Jupyter notebook in the code repository for this book for more details). The\\nrisk tolerance prediction model is embedded in this dashboard and is triggered once\\nthe “Calculate Risk Tolerance” button is pressed after submitting the inputs.\\n\\n138 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cFigure 5-7. Robo-advisor input panel\\n\\n7.2 Asset allocation and portfolio performance.     Figure  5-8  shows  the  “Asset  Allocation\\nand Portfolio Performance” panel, which performs the following functionalities:\\n\\n• Once the risk tolerance is computed using the model, it is displayed on the top of\\n\\nthis panel.\\n\\n• In the next step, we pick the assets for our portfolio from the dropdown.\\n\\n• Once the list of assets are submitted, the traditional mean-variance portfolio allo‐\\ncation  model  is  used  to  allocate  the  portfolio  among  the  assets  selected.  Risk\\n\\nCase Study 3: Investor Risk Tolerance and Robo-Advisors \\n\\n| \\n\\n139\\n\\n\\x0ctolerance is one of the key inputs for this process. (Refer to the get_asset_allo\\ncation  function  of  the  “Sample  Robo-advisor”  Jupyter  notebook  in  the  code\\nrepository for this book for more details.)\\n\\n• The  dashboard  also  shows  the  historical  performance  of  the  allocated  portfolio\\n\\nfor an initial investment of $100.\\n\\nFigure 5-8. Robo-advisor asset allocation and portfolio performance panel\\n\\nAlthough the dashboard is a basic version of the robo-advisor dashboard, it performs\\nend-to-end asset allocation for an investor and provides the portfolio view and his‐\\ntorical performance of the portfolio over a selected period. There are several potential\\nenhancements  to  this  prototype  in  terms  of  the  interface  and  underlying  models\\nused. The dashboard can be enhanced to include additional instruments and incor‐\\nporate additional features such as real-time portfolio monitoring, portfolio rebalanc‐\\ning,  and  investment  advisory.  In  terms  of  the  underlying  models  used  for  asset\\nallocation,  we  have  used  the  traditional  mean-variance  optimization  method,  but  it\\ncan be further enhanced to use the allocation algorithms based on machine learning\\ntechniques  such  as  eigen-portfolio,  hierarchical  risk  parity,  or  reinforcement  learn‐\\ning–based models, described in Chapters 7, 8 and 9, respectively. The risk tolerance\\nmodel can be further enhanced by using additional features or using the actual data\\nof the investors rather than using data from the Survey of Consumer Finances.\\n\\n140 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cConclusion\\n\\nIn this case study, we introduced the regression-based algorithm applied to compute\\nan  investor’s  risk  tolerance,  followed  by  a  demonstration  of  the  model  in  a  robo-\\nadvisor setup. We showed that machine learning models might be able to objectively\\nanalyze the behavior of different investors in a changing market and attribute these\\nchanges  to  variables  involved  in  determining  risk  appetite.  With  an  increase  in  the\\nvolume of investors’ data and the availability of rich machine learning infrastructure,\\nsuch models might prove to be more useful than existing manual processes.\\n\\nWe saw that there is a nonlinear relationship between the variables and the risk toler‐\\nance. We analyzed the feature importance and found that results of the case study are\\nquite intuitive. Income and net worth, followed by age and willingness to take risk,\\nare the key variables to deciding risk tolerance. These variables have been considered\\nkey variables to model risk tolerance across academic and industry literature.\\n\\nThrough  the  robo-advisor  dashboard  powered  by  machine  learning,  we  demon‐\\nstrated an effective combination of data science and machine learning implementa‐\\ntion in wealth management. Robo-advisors and investment managers could leverage\\nsuch  models  and  platforms  to  enhance  the  portfolio  management  process  with  the\\nhelp of machine learning.\\n\\nCase Study 4: Yield Curve Prediction\\nA  yield  curve  is  a  line  that  plots  yields  (interest  rates)  of  bonds  having  equal  credit\\nquality but differing maturity dates. This yield curve is used as a benchmark for other\\ndebt in the market, such as mortgage rates or bank lending rates. The most frequently\\nreported yield curve compares the 3-months, 2-years, 5-years, 10-years, and 30-years\\nU.S. Treasury debt.\\n\\nThe  yield  curve  is  the  centerpiece  in  a  fixed  income  market.  Fixed  income  markets\\nare important sources of finance for governments, national and supranational insti‐\\ntutions, banks, and private and public corporations. In addition, yield curves are very\\nimportant to investors in pension funds and insurance companies.\\n\\nThe  yield  curve  is  a  key  representation  of  the  state  of  the  bond  market.  Investors\\nwatch the bond market closely as it is a strong predictor of future economic activity\\nand levels of inflation, which affect prices of goods, financial assets, and real estate.\\nThe slope of the yield curve is an important indicator of short-term interest rates and\\nis followed closely by investors.\\n\\nHence, an accurate yield curve forecasting is of critical importance in financial appli‐\\ncations. Several statistical techniques and tools commonly used in econometrics and\\nfinance have been applied to model the yield curve.\\n\\nCase Study 4: Yield Curve Prediction \\n\\n| \\n\\n141\\n\\n\\x0cIn this case study we will use supervised learning–based models to predict the yield\\ncurve.  This  case  study  is  inspired  by  the  paper  Artificial  Neural  Networks  in  Fixed\\nIncome Markets for Yield Curve Forecasting by Manuel Nunes et al. (2018).\\n\\nIn this case study, we will focus on:\\n\\n• Simultaneous  modeling  (producing  multiple  outputs  at  the  same  time)  of  the\\n\\ninterest rates.\\n\\n• Comparison of neural network versus linear regression models.\\n\\n• Modeling a time series in a supervised regression–based framework.\\n\\n• Understanding the variable intuition and feature selection.\\n\\nOverall,  the  case  study  is  similar  to  the  stock  price  prediction  case  study  presented\\nearlier in this chapter, with the following differences:\\n\\n• We predict multiple outputs simultaneously, rather than a single output.\\n\\n• The predicted variable in this case study is not the return variable.\\n\\n• Given  that  we  already  covered  time  series  models  in  case  study  1,  we  focus  on\\n\\nartificial neural networks for prediction in this case study.\\n\\nBlueprint for Using Supervised Learning Models to Predict\\nthe Yield Curve\\n\\n1. Problem definition\\n\\nIn  the  supervised  regression  framework  used  for  this  case  study,  three  tenors  (1M,\\n5Y,  and  30Y)  of  the  yield  curve  are  the  predicted  variables.  These  tenors  represent\\nshort-term, medium-term, and long-term tenors of the yield curve.\\n\\nWe  need  to  understand  what  affects  the  movement  of  the  yield  curve  and  hence\\nincorporate as much information into our model as we can. As a high-level overview,\\nother  than  the  historical  price  of  the  yield  curve  itself,  we  look  at  other  correlated\\nvariables  that  can  influence  the  yield  curve.  The  independent  or  predictor  variables\\nwe consider are:\\n\\n• Previous  value  of  the  treasury  curve  for  different  tenors.  The  tenors  used  are  1-\\n\\nmonth, 3-month, 1-year, 2-year, 5-year, 7-year, 10-year, and 30-year yields.\\n\\n142 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0c• Percentage  of  the  federal  debt  held  by  the  public,  foreign  governments,  and  the\\n\\nfederal reserve.\\n\\n• Corporate spread on Baa-rated debt relative to the 10-year treasury rate.\\n\\nThe federal debt and corporate spread are correlated variables and can be potentially\\nuseful in modeling the yield curve. The dataset used for this case study is extracted\\nfrom Yahoo Finance and FRED. We will use the daily data of the last 10 years, from\\n2010 onward.\\n\\nBy the end of this case study, readers will be familiar with a general machine learning\\napproach to yield curve modeling, from gathering and cleaning data to building and\\ntuning different models.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  loading  of  Python  packages  is  similar  to  other\\ncase studies in this chapter. Refer to the Jupyter notebook of this case study for more\\ndetails.\\n\\n2.2. Loading the data.    The following steps demonstrate the loading of data using Pan‐\\ndas’s DataReader function:\\n\\n# Get the data by webscraping using pandas datareader\\ntsy_tickers = [\\'DGS1MO\\', \\'DGS3MO\\', \\'DGS1\\', \\'DGS2\\', \\'DGS5\\', \\'DGS7\\', \\'DGS10\\',\\n               \\'DGS30\\',\\n               \\'TREAST\\', # Treasury securities held by the Federal Reserve ($MM)\\n               \\'FYGFDPUN\\', # Federal Debt Held by the Public ($MM)\\n               \\'FDHBFIN\\', # Federal Debt Held by International Investors ($BN)\\n               \\'GFDEBTN\\', # Federal Debt: Total Public Debt ($BN)\\n               \\'BAA10Y\\', # Baa Corporate Bond Yield Relative to Yield on 10-Year\\n              ]\\ntsy_data = web.DataReader(tsy_tickers, \\'fred\\').dropna(how=\\'all\\').ffill()\\ntsy_data[\\'FDHBFIN\\'] = tsy_data[\\'FDHBFIN\\'] * 1000\\ntsy_data[\\'GOV_PCT\\'] = tsy_data[\\'TREAST\\'] / tsy_data[\\'GFDEBTN\\']\\ntsy_data[\\'HOM_PCT\\'] = tsy_data[\\'FYGFDPUN\\'] / tsy_data[\\'GFDEBTN\\']\\ntsy_data[\\'FOR_PCT\\'] = tsy_data[\\'FDHBFIN\\'] / tsy_data[\\'GFDEBTN\\']\\n\\nNext,  we  define  our  dependent  (Y)  and  independent  (X)  variables.  The  predicted\\nvariables  are  the  rate  for  three  tenors  of  the  yield  curve  (i.e.,  1M,  5Y,  and  30Y)  as\\nmentioned before. The number of trading days in a week is assumed to be five, and\\nwe compute the lagged version of the variables mentioned in the problem definition\\nsection as independent variables using five trading day lag.\\n\\nThe lagged five-day variables embed the time series component by using a time-delay\\napproach, where the lagged variable is included as one of the independent variables.\\nThis  step  reframes  the  time  series  data  into  a  supervised  regression–based  model\\nframework.\\n\\nCase Study 4: Yield Curve Prediction \\n\\n| \\n\\n143\\n\\n\\x0c3. Exploratory data analysis.    We will look at descriptive statistics and data visualization\\nin this section.\\n\\n3.1. Descriptive statistics.    Let us look at the shape and the columns in the dataset:\\n\\ndataset.shape\\n\\nOutput\\n\\n(505, 15)\\n\\nThe data contains around 500 observations with 15 columns.\\n\\n3.2. Data visualization.    Let us first plot the predicted variables and see their behavior:\\n\\nY.plot(style=[\\'-\\',\\'--\\',\\':\\'])\\n\\nOutput\\n\\nIn the plot, we see that the deviation among the short-term, medium-term, and long-\\nterm rates was higher in 2010 and has been decreasing since then. There was a drop\\nin  the  long-term  and  medium-term  rates  during  2011,  and  they  also  have  been\\ndeclining since then. The order of the rates has been in line with the tenors. However,\\nfor a few months in recent years, the 5Y rate has been lower than the 1M rate. In the\\ntime series of all the tenors, we can see that the mean varies with time, resulting in an\\nupward trend. Thus these series are nonstationary time series.\\n\\n144 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cIn some cases, the linear regression for such nonstationary dependent variables might\\nnot be valid. However, we are using the lagged variables, which are also nonstation‐\\nary as independent variables. So we are effectively modeling a nonstationary time ser‐\\nies against another nonstationary time series, which might still be valid.\\n\\nNext, we look at the scatterplots (a correlation plot is skipped for this case study as it\\nhas a similar interpretation to that of a scatterplot). We can visualize the relationship\\nbetween all the variables in the regression using the scatter matrix shown below:\\n\\n# Scatterplot Matrix\\npyplot.figure(figsize=(15,15))\\nscatter_matrix(dataset,figsize=(15,16))\\npyplot.show()\\n\\nOutput\\n\\nCase Study 4: Yield Curve Prediction \\n\\n| \\n\\n145\\n\\n\\x0cLooking at the scatterplot (full-size version available on GitHub), we see a significant\\nlinear  relationship  of  the  predicted  variables  with  their  lags  and  other  tenors  of  the\\nyield  curve.  There  is  also  a  linear  relationship,  with  negative  slope  between  1M,  5Y\\nrates versus corporate spread and changes in foreign government purchases. The 30Y\\nrate  shows  a  linear  relationship  with  these  variables,  although  the  slope  is  negative.\\nOverall, we see a lot of linear relationships, and we expect the linear models to per‐\\nform well.\\n\\n4. Data preparation and analysis\\n\\nWe  performed  most  of  the  data  preparation  steps  (i.e.,  getting  the  dependent  and\\nindependent variables) in the preceding steps, and so we’ll skip this step.\\n\\n5. Evaluate models\\n\\nIn this step we evaluate the models. The Python code for this step is similar to dthat\\nin case study 1, and some of the repetitive code is skipped. Readers can also refer to\\nthe Jupyter notebook of this case study in the code repository for this book for more\\ndetails.\\n\\n5.1. Train-test split and evaluation metrics.    We will use 80% of the dataset for modeling\\nand  use  20%  for  testing.  We  will  evaluate  algorithms  using  the  mean  squared  error\\nmetric. All the algorithms use default tuning parameters.\\n\\n5.2. Compare models and algorithms.    In this case study, the primary purpose is to com‐\\npare the linear models with the artificial neural network in yield curve modeling. So\\nwe  stick  to  the  linear  regression  (LR),  regularized  regression  (LASSO  and  EN),  and\\nartificial neural network (shown as MLP). We also include a few other models such as\\nKNN and CART, as these models are simpler with good interpretation, and if there is\\na  nonlinear  relationship  between  the  variables,  the  CART  and  KNN  models  will  be\\nable to capture it and provide a good comparison benchmark for ANN.\\n\\nLooking at the training and test error, we see a good performance of the linear regres‐\\nsion model. We see that lasso and elastic net perform poorly. These are regularized\\nregression  models,  and  they  reduce  the  number  of  variables  in  case  they  are  not\\nimportant. A decrease in the number of variables might have caused a loss of infor‐\\nmation leading to poor model performance. KNN and CART are good, but looking\\nclosely, we see that the test errors are higher than the training error. We also see that\\nthe  performance  of  the  artificial  neural  network  (MLP)  algorithm  is  comparable  to\\nthe  linear  regression  model.  Despite  its  simplicity,  the  linear  regression  is  a  tough\\nbenchmark  to  beat  for  one-step-ahead  forecasting  when  there  is  a  significant  linear\\nrelationship between the variables.\\n\\n146 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cOutput\\n\\n6. Model tuning and grid search.\\n\\nSimilar to case study 2 of this chapter, we perform a grid search of the ANN model\\nwith different combinations of hidden layers. Several other hyperparameters such as\\nlearning rate, momentum, activation function, number of epochs, and batch size can\\nbe tuned during the grid search process, similar to the steps mentioned below.\\n\\n\\'\\'\\'\\nhidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\\n    The ith element represents the number of neurons in the ith\\n    hidden layer.\\n\\'\\'\\'\\nparam_grid={\\'hidden_layer_sizes\\': [(20,), (50,), (20,20), (20, 30, 20)]}\\nmodel = MLPRegressor()\\nkfold = KFold(n_splits=num_folds, random_state=seed)\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\\\\n  cv=kfold)\\ngrid_result = grid.fit(X_train, Y_train)\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\nmeans = grid_result.cv_results_[\\'mean_test_score\\']\\nstds = grid_result.cv_results_[\\'std_test_score\\']\\nparams = grid_result.cv_results_[\\'params\\']\\nfor mean, stdev, param in zip(means, stds, params):\\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\\n\\nOutput\\n\\nBest: -0.018006 using {\\'hidden_layer_sizes\\': (20, 30, 20)}\\n-0.036433 (0.019326) with: {\\'hidden_layer_sizes\\': (20,)}\\n\\nCase Study 4: Yield Curve Prediction \\n\\n| \\n\\n147\\n\\n\\x0c-0.020793 (0.007075) with: {\\'hidden_layer_sizes\\': (50,)}\\n-0.026638 (0.010154) with: {\\'hidden_layer_sizes\\': (20, 20)}\\n-0.018006 (0.005637) with: {\\'hidden_layer_sizes\\': (20, 30, 20)}\\n\\nThe best model is the model with three layers, with 20, 30, and 20 nodes in each hid‐\\nden layer, respectively. Hence, we prepare a model with this configuration and check\\nits  performance  on  the  test  set.  This  is  a  crucial  step,  as  a  greater  number  of  layers\\nmay lead to overfitting and have poor performance in the test set.\\n\\nPrediction comparison.     In  the  last  step  we  look  at  the  prediction  plot  of  actual  data\\nversus  the  prediction  from  both  linear  regression  and  ANN  models.  Refer  to  the\\nJupyter notebook of this case study for the Python code of this section.\\n\\n148 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cLooking at the charts above, we see that the predictions of the linear regression and\\nANN are comparable. For 1M tenor, the fitting with ANN is slightly poor compared\\nto the regression. However, for 5Y and 30Y tenors the ANN performs as well as the\\nregression model.\\n\\nConclusion\\n\\nIn this case study, we applied supervised regression to the prediction of several tenors\\nof yield curve. The linear regression model, despite its simplicity, is a tough bench‐\\nmark to beat for such one-step-ahead forecasting, given the dominant characteristic\\nof  the  last  available  value  of  the  variable  to  predict.  The  ANN  results  in  this  case\\nstudy are comparable to the linear regression models. An additional benefit of ANN\\nis that it is more flexible to changing market conditions. Also, ANN models can be\\nenhanced  by  performing  grid  search  on  several  other  hyperparameters  and  the\\noption of incorporating recurrent neural networks, such as LSTM.\\n\\nOverall, we built a machine learning–based model using ANN with an encouraging\\noutcome, in the context of fixed income instruments. This allows us to perform pre‐\\ndictions  using  historical  data  to  generate  results  and  analyze  risk  and  profitability\\nbefore risking any actual capital in the fixed income market.\\n\\nChapter Summary\\nIn “Case Study 1: Stock Price Prediction” on page 95, we covered a machine learning\\nand  time  series–based  framework  for  stock  price  prediction.  We  demonstrated  the\\nsignificance of visualization and compared time series against the machine learning\\n\\nChapter Summary \\n\\n| \\n\\n149\\n\\n\\x0cmodels.  In  “Case  Study  2:  Derivative  Pricing”  on  page  114,  we  explored  the  use  of\\nmachine  learning  for  a  traditional  derivative  pricing  problem  and  demonstrated  a\\nhigh  model  performance.  In  “Case  Study  3:  Investor  Risk  Tolerance  and  Robo-\\nAdvisors”  on  page  125,  we  demonstrated  how  supervised  learning  models  can  be\\nused  to  model  the  risk  tolerance  of  investors,  which  can  lead  to  automation  of  the\\nportfolio management process. “Case Study 4: Yield Curve Prediction” on page 141\\nwas  similar  to  the  stock  price  prediction  case  study,  providing  another  example  of\\ncomparison of linear and nonlinear models in the context of fixed income markets.\\n\\nWe saw that time series and linear supervised learning models worked well for asset\\nprice  prediction  problems  (i.e.,  case  studies  1  and  4),  where  the  predicted  variable\\nhad a significant linear relationship with its lagged component. However, in deriva‐\\ntive  pricing  and  risk  tolerance  prediction,  where  there  are  nonlinear  relationships,\\nensemble and ANN models performed better. Readers who are interested in imple‐\\nmenting a case study using supervised regression or time series models are encour‐\\naged  to  understand  the  nuances  in  the  variable  relationships  and  model  intuition\\nbefore proceeding to model selection.\\n\\nOverall, the concepts in Python, machine learning, time series, and finance presented\\nin this chapter through the case studies can used as a blueprint for any other super‐\\nvised regression–based problem in finance.\\n\\nExercises\\n\\n• Using the concepts and framework of machine learning and time series models\\nspecified in case study 1, develop a predictive model for another asset class—cur‐\\nrency pair (EUR/USD, for example) or bitcoin.\\n\\n• In case study 1, add some technical indicators, such as trend or momentum, and\\ncheck the enhancement in the model performance. Some of the ideas of the tech‐\\nnical indicators can be borrowed from “Case Study 3: Bitcoin Trading Strategy”\\non page 179 in Chapter 6.\\n\\n• Using the concepts in “Case Study 2: Derivative Pricing” on page 114, develop a\\n\\nmachine learning–based model to price American options.\\n\\n• Incorporate  multivariate  time  series  modeling  using  a  variant  of  the  ARIMA\\nmodel, such as VARMAX, for rates prediction in the yield curve prediction case\\nstudy and compare the performance against the machine learning–based models.\\n\\n• Enhance  the  robo-advisor  dashboard  presented  in  “Case  Study  3:  Investor  Risk\\nTolerance  and  Robo-Advisors”  on  page  125  to  incorporate  instruments  other\\nthan equities.\\n\\n150 \\n\\n| \\n\\nChapter 5: Supervised Learning: Regression (Including Time Series Models)\\n\\n\\x0cCHAPTER 6\\nSupervised Learning: Classification\\n\\nHere are some of the key questions that financial analysts attempt to solve:\\n\\n• Is a borrower going to repay their loan or default on it?\\n\\n• Will the instrument price go up or down?\\n\\n• Is this credit card transaction a fraud or not?\\n\\nAll of these problem statements, in which the goal is to predict the categorical class\\nlabels, are inherently suitable for classification-based machine learning.\\n\\nClassification-based algorithms have been used across many areas within finance that\\nrequire predicting a qualitative response. These include fraud detection, default pre‐\\ndiction, credit scoring, directional forecasting of asset price movement, and buy/sell\\nrecommendations. There are many other use cases of classification-based supervised\\nlearning in portfolio management and algorithmic trading.\\n\\nIn  this  chapter  we  cover  three  such  classification-based  case  studies  that  span  a\\ndiverse set of areas, including fraud detection, loan default probability, and formulat‐\\ning a trading strategy.\\n\\nIn “Case Study 1: Fraud Detection” on page 153, we use a classification-based algorithm\\nto predict whether a transaction is fraudulent. The focus of this case study is also to\\ndeal  with  an  unbalanced  dataset,  given  that  the  fraud  dataset  is  highly  unbalanced\\nwith a small number of fraudulent observations.\\n\\nIn “Case Study 2: Loan Default Probability” on page 166, we use a classification-based\\nalgorithm  to  predict  whether  a  loan  will  default.  The  case  study  focuses  on  various\\ntechniques  and  concepts  of  data  processing,  feature  selection,  and  exploratory\\nanalysis.\\n\\n151\\n\\n\\x0cIn “Case Study 3: Bitcoin Trading Strategy” on page 179, we use classification-based\\nalgorithms  to  predict  whether  the  current  trading  signal  of  bitcoin  is  to  buy  or  sell\\ndepending on the relationship between the short-term and long-term price. We pre‐\\ndict the trend of bitcoin’s price using technical indicators. The prediction model can\\neasily  be  transformed  into  a  trading  bot  that  can  perform  buy,  sell,  or  hold  actions\\nwithout human intervention.\\n\\nIn addition to focusing on different problem statements in finance, these case studies\\nwill help you understand:\\n\\n• How to develop new features such as technical indicators for an investment strat‐\\n\\negy using feature engineering, and how to improve model performance.\\n\\n• How to use data preparation and data transformation, and how to perform fea‐\\n\\nture reduction and use feature importance.\\n\\n• How to use data visualization and exploratory data analysis for feature reduction\\n\\nand to improve model performance.\\n\\n• How to use algorithm tuning and grid search across various classification-based\\n\\nmodels to improve model performance.\\n\\n• How to handle unbalanced data.\\n\\n• How to use the appropriate evaluation metrics for classification.\\n\\nThis Chapter’s Code Repository\\n\\nA  Python-based  master  template  for  supervised  classification\\nmodel, along with the Jupyter notebook for the case studies presen‐\\nted in this chapter, is included in the folder Chapter 6 - Sup. Learn‐\\ning - Classification models in the code repository for this book. All\\nof  the  case  studies  presented  in  this  chapter  use  the  standardized\\nseven-step model development process presented in Chapter 2.1\\n\\nFor  any  new  classification-based  problem,  the  master  template\\nfrom  the  code  repository  can  be  modified  with  the  elements  spe‐\\ncific  to  the  problem.  The  templates  are  designed  to  run  on  cloud\\ninfrastructure  (e.g.,  Kaggle,  Google  Colab,  or  AWS).  In  order  to\\nrun the template on the local machine, all the packages used within\\nthe template must be installed successfully.\\n\\n1 There may be reordering or renaming of the steps or substeps based on the appropriateness and intuitiveness\\n\\nof the steps/substeps.\\n\\n152 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cCase Study 1: Fraud Detection\\nFraud  is  one  of  the  most  significant  issues  the  finance  sector  faces.  It  is  incredibly\\ncostly. According to one study, it is estimated that the typical organization loses 5%\\nof its annual revenue to fraud each year. When applied to the 2017 estimated Gross\\nWorld Product of $79.6 trillion, this translates to potential global losses of up to $4\\ntrillion.\\n\\nFraud detection is a task inherently suitable for machine learning, as machine learn‐\\ning–based models can scan through huge transactional datasets, detect unusual activ‐\\nity,  and  identify  all  cases  that  might  be  prone  to  fraud.  Also,  the  computations  of\\nthese models are faster compared to traditional rule-based approaches. By collecting\\ndata from various sources and then mapping them to trigger points, machine learn‐\\ning solutions are able to discover the rate of defaulting or fraud propensity for each\\npotential customer and transaction, providing key alerts and insights for the financial\\ninstitutions.\\n\\nIn this case study, we will use various classification-based models to detect whether a\\ntransaction is a normal payment or a fraud.\\n\\nThe focuses of this case study are:\\n\\n• Handling unbalanced data by downsampling/upsampling the data.\\n\\n• Selecting  the  right  evaluation  metric,  given  that  one  of  the  main  goals  is  to\\nreduce  false  negatives  (cases  in  which  fraudulent  transactions  incorrectly  go\\nunnoticed).\\n\\nBlueprint for Using Classification Models to Determine\\nWhether a Transaction Is Fraudulent\\n\\n1. Problem definition\\n\\nIn  the  classification  framework  defined  for  this  case  study,  the  response  (or  target)\\nvariable  has  the  column  name  “Class.”  This  column  has  a  value  of  1  in  the  case  of\\nfraud and a value of 0 otherwise.\\n\\nThe  dataset  used  is  obtained  from  Kaggle.  This  dataset  holds  transactions  by  Euro‐\\npean cardholders that occurred over two days in September 2013, with 492 cases of\\nfraud out of 284,807 transactions.\\n\\nCase Study 1: Fraud Detection \\n\\n| \\n\\n153\\n\\n\\x0cThe  dataset  has  been  anonymized  for  privacy  reasons.  Given  that  certain  feature\\nnames are not provided (i.e., they are called V1, V2, V3, etc.), the visualization and\\nfeature importance will not give much insight into the behavior of the model.\\n\\nBy the end of this case study, readers will be familiar with a general approach to fraud\\nmodeling, from gathering and cleaning data to building and tuning a classifier.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  list  of  the  libraries  used  for  data  loading,  data\\nanalysis, data preparation, model evaluation, and model tuning are shown below. The\\npackages used for different purposes have been separated in the Python code below.\\nThe details of most of these packages and functions have been provided in Chapter 2\\nand Chapter 4:\\n\\nPackages for data loading, data analysis, and data preparation\\n\\nimport numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nfrom matplotlib import pyplot\\n\\nfrom pandas import read_csv, set_option\\nfrom pandas.plotting import scatter_matrix\\nfrom sklearn.preprocessing import StandardScaler\\n\\nPackages for model evaluation and classification models\\n\\nfrom sklearn.model_selection import train_test_split, KFold,\\\\\\n cross_val_score, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.svm import SVC\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier,\\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\\nfrom sklearn.metrics import classification_report, confusion_matrix,\\\\\\n  accuracy_score\\n\\nPackages for deep learning models\\n\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\n\\n154 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cPackages for saving the model\\n\\nfrom pickle import dump\\nfrom pickle import load\\n\\n3. Exploratory data analysis\\n\\nThe following sections walk through some high-level data inspection.\\n\\n3.1. Descriptive statistics.    The first thing we must do is gather a basic sense of our data.\\nRemember,  except  for  the  transaction  and  amount,  we  do  not  know  the  names  of\\nother columns. The only thing we know is that the values of those columns have been\\nscaled. Let’s look at the shape and columns of the data:\\n\\n# shape\\ndataset.shape\\n\\nOutput\\n\\n(284807, 31)\\n\\n#peek at data\\nset_option(\\'display.width\\', 100)\\ndataset.head(5)\\n\\nOutput\\n\\n5 rows × 31 columns\\n\\nAs shown, the variable names are nondescript (V1, V2, etc.). Also, the data type for\\nthe entire dataset is float, except Class, which is of type integer.\\n\\nHow many are fraud and how many are not fraud? Let us check:\\n\\nclass_names = {0:\\'Not Fraud\\', 1:\\'Fraud\\'}\\nprint(dataset.Class.value_counts().rename(index = class_names))\\n\\nOutput\\n\\nNot Fraud    284315\\nFraud           492\\nName: Class, dtype: int64\\n\\nNotice the stark imbalance of the data labels. Most of the transactions are nonfraud.\\nIf we use this dataset as the base for our modeling, most models will not place enough\\nemphasis  on  the  fraud  signals;  the  nonfraud  data  points  will  drown  out  any  weight\\n\\nCase Study 1: Fraud Detection \\n\\n| \\n\\n155\\n\\n\\x0cthe fraud signals provide. As is, we may encounter difficulties modeling the predic‐\\ntion  of  fraud,  with  this  imbalance  leading  the  models  to  simply  assume  all  transac‐\\ntions are nonfraud. This would be an unacceptable result. We will explore some ways\\nof dealing with this issue in the subsequent sections.\\n\\n3.2. Data visualization.    Since the feature descriptions are not provided, visualizing the\\ndata will not lead to much insight. This step will be skipped in this case study.\\n\\n4. Data preparation\\n\\nThis data is from Kaggle and is already in a cleaned format without any empty rows\\nor columns. Data cleaning or categorization is unnecessary.\\n\\n5. Evaluate models\\n\\nNow we are ready to split the data and evaluate the models.\\n\\n5.1. Train-test split and evaluation metrics.    As  described  in  Chapter  2,  it  is  a  good  idea\\nto partition the original dataset into training and test sets. The test set is a sample of\\nthe data that we hold back from our analysis and modeling. We use it at the end of\\nour project to confirm the accuracy of our final model. It is the final test that gives us\\nconfidence in our estimates of accuracy on unseen data. We will use 80% of the data‐\\nset for model training and 20% for testing:\\n\\nY= dataset[\"Class\"]\\nX = dataset.loc[:, dataset.columns != \\'Class\\']\\nvalidation_size = 0.2\\nseed = 7\\nX_train, X_validation, Y_train, Y_validation =\\\\\\ntrain_test_split(X, Y, test_size=validation_size, random_state=seed)\\n\\n5.2. Checking models.    In this step, we will evaluate different machine learning models.\\nTo optimize the various hyperparameters of the models, we use ten-fold cross valida‐\\ntion and recalculate the results ten times to account for the inherent randomness in\\nsome of the models and the CV process. All of these models, including cross valida‐\\ntion, are described in Chapter 4.\\n\\nLet us design our test harness. We will evaluate algorithms using the accuracy metric.\\nThis is a gross metric that will give us a quick idea of how correct a given model is. It\\nis useful on binary classification problems.\\n\\n# test options for classification\\nnum_folds = 10\\nscoring = \\'accuracy\\'\\n\\nLet’s create a baseline of performance for this problem and spot-check a number of\\ndifferent algorithms. The selected algorithms include:\\n\\n156 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cLinear algorithms\\n\\nLogistic regression (LR) and linear discriminant analysis (LDA).\\n\\nNonlinear algorithms\\n\\nClassification and regression trees (CART) and K-nearest neighbors (KNN).\\n\\nThere  are  good  reasons  for  selecting  these  models.  These  models  are  simpler  and\\nfaster models with good interpretation for problems with large datasets. CART and\\nKNN will be able to discern any nonlinear relationships between the variables. The\\nkey problem here is using an unbalanced sample. Unless we resolve that, more com‐\\nplex models, such as ensemble and ANNs, will have poor prediction. We will focus\\non addressing this later in the case study and then will evaluate the performance of\\nthese types of models.\\n\\n# spot-check basic Classification algorithms\\nmodels = []\\nmodels.append((\\'LR\\', LogisticRegression()))\\nmodels.append((\\'LDA\\', LinearDiscriminantAnalysis()))\\nmodels.append((\\'KNN\\', KNeighborsClassifier()))\\nmodels.append((\\'CART\\', DecisionTreeClassifier()))\\n\\nAll the algorithms use default tuning parameters. We will display the mean and stan‐\\ndard deviation of accuracy for each algorithm as we calculate and collect the results\\nfor use later.\\n\\nresults = []\\nnames = []\\nfor name, model in models:\\n    kfold = KFold(n_splits=num_folds, random_state=seed)\\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, \\\\\\n      scoring=scoring)\\n    results.append(cv_results)\\n    names.append(name)\\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\\n    print(msg)\\n\\nOutput\\n\\nLR: 0.998942 (0.000229)\\nLDA: 0.999364 (0.000136)\\nKNN: 0.998310 (0.000290)\\nCART: 0.999175 (0.000193)\\n\\n# compare algorithms\\nfig = pyplot.figure()\\nfig.suptitle(\\'Algorithm Comparison\\')\\nax = fig.add_subplot(111)\\npyplot.boxplot(results)\\nax.set_xticklabels(names)\\nfig.set_size_inches(8,4)\\npyplot.show()\\n\\nCase Study 1: Fraud Detection \\n\\n| \\n\\n157\\n\\n\\x0cThe accuracy of the overall result is quite high. But let us check how well it predicts\\nthe fraud cases. Choosing one of the model CART from the results above and looking\\nat the result on the test set:\\n\\n# prepare model\\nmodel = DecisionTreeClassifier()\\nmodel.fit(X_train, Y_train)\\n\\n# estimate accuracy on validation set\\npredictions = model.predict(X_validation)\\nprint(accuracy_score(Y_validation, predictions))\\nprint(classification_report(Y_validation, predictions))\\n\\nOutput\\n\\n0.9992275552122467\\n              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00     56862\\n           1       0.77      0.79      0.78       100\\n\\n    accuracy                           1.00     56962\\n   macro avg       0.89      0.89      0.89     56962\\nweighted avg       1.00      1.00      1.00     56962\\n\\nAnd producing the confusion matrix yields:\\n\\ndf_cm = pd.DataFrame(confusion_matrix(Y_validation, predictions), \\\\\\ncolumns=np.unique(Y_validation), index = np.unique(Y_validation))\\ndf_cm.index.name = \\'Actual\\'\\ndf_cm.columns.name = \\'Predicted\\'\\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})\\n\\n158 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cOverall accuracy is strong, but the confusion metrics tell a different story. Despite the\\nhigh accuracy level, 21 out of 100 instances of fraud are missed and incorrectly pre‐\\ndicted as nonfraud. The false negative rate is substantial.\\n\\nThe intention of a fraud detection model is to minimize these false negatives. To do\\nso, the first step would be to choose the right evaluation metric.\\n\\nIn  Chapter  4,  we  covered  the  evaluation  metrics,  such  as  accuracy,  precision,  and\\nrecall, for a classification-based problem. Accuracy is the number of correct predic‐\\ntions made as a ratio of all predictions made. Precision is the number of items cor‐\\nrectly  identified  as  positive  out  of  total  items  identified  as  positive  by  the  model.\\nRecall  is  the  total  number  of  items  correctly  identified  as  positive  out  of  total  true\\npositives.\\n\\nFor this type of problem, we should focus on recall, the ratio of true positives to the\\nsum of true positives and false negatives. So if false negatives are high, then the value\\nof recall will be low.\\n\\nIn the next step, we perform model tuning, select the model using the recall metric,\\nand perform under-sampling.\\n\\n6. Model tuning\\n\\nThe  purpose  of  the  model  tuning  step  is  to  perform  the  grid  search  on  the  model\\nselected  in  the  previous  step.  However,  since  we  encountered  poor  model  perfor‐\\nmance in the previous section due to the unbalanced dataset, we will focus our atten‐\\ntion  on  that.  We  will  analyze  the  impact  of  choosing  the  correct  evaluation  metric\\nand see the impact of using an adjusted, balanced sample.\\n\\nCase Study 1: Fraud Detection \\n\\n| \\n\\n159\\n\\n\\x0c6.1. Model tuning by choosing the correct evaluation metric.    As mentioned in the preced‐\\ning  step,  if  false  negatives  are  high,  then  the  value  of  recall  will  be  low.  Models  are\\nranked according to this metric:\\n\\nscoring = \\'recall\\'\\n\\nLet us spot-check some basic classification algorithms for recall:\\n\\nmodels = []\\nmodels.append((\\'LR\\', LogisticRegression()))\\nmodels.append((\\'LDA\\', LinearDiscriminantAnalysis()))\\nmodels.append((\\'KNN\\', KNeighborsClassifier()))\\nmodels.append((\\'CART\\', DecisionTreeClassifier()))\\n\\nRunning cross validation:\\n\\nresults = []\\nnames = []\\nfor name, model in models:\\n    kfold = KFold(n_splits=num_folds, random_state=seed)\\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, \\\\\\n      scoring=scoring)\\n    results.append(cv_results)\\n    names.append(name)\\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\\n    print(msg)\\n\\nOutput\\n\\nLR: 0.595470 (0.089743)\\nLDA: 0.758283 (0.045450)\\nKNN: 0.023882 (0.019671)\\nCART: 0.735192 (0.073650)\\n\\nWe see that the LDA model has the best recall of the four models. We continue by\\nevaluating the test set using the trained LDA:\\n\\n# prepare model\\nmodel = LinearDiscriminantAnalysis()\\nmodel.fit(X_train, Y_train)\\n# estimate accuracy on validation set\\n\\npredictions = model.predict(X_validation)\\nprint(accuracy_score(Y_validation, predictions))\\n\\nOutput\\n\\n0.9995435553526912\\n\\n160 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cLDA performs better, missing only 18 out of 100 cases of fraud. Additionally, we find\\nfewer false positives as well. However, there is still improvement to be made.\\n\\n6.2. Model tuning—balancing the sample by random under-sampling.     The  current  data\\nexhibits  a  significant  class  imbalance,  where  there  are  very  few  data  points  labeled\\n“fraud.”  The  issue  of  such  class  imbalance  can  result  in  a  serious  bias  toward  the\\nmajority class, reducing the classification performance and increasing the number of\\nfalse negatives.\\n\\nOne of the remedies to handle such situations is to under-sample the data. A simple\\ntechnique is to under-sample the majority class randomly and uniformly. This might\\nlead to a loss of information, but it may yield strong results by modeling the minority\\nclass well.\\n\\nNext,  we  will  implement  random  under-sampling,  which  consists  of  removing  data\\nto  have  a  more  balanced  dataset.  This  will  help  ensure  that  our  models  avoid\\noverfitting.\\n\\nThe steps to implement random under-sampling are:\\n\\n1. First, we determine the severity of the class imbalance by using value_counts()\\non  the  class  column.  We  determine  how  many  instances  are  considered  fraud\\ntransactions (fraud = 1).\\n\\n2. We  bring  the  nonfraud  transaction  observation  count  to  the  same  amount  as\\nfraud transactions. Assuming we want a 50/50 ratio, this will be equivalent to 492\\ncases of fraud and 492 cases of nonfraud transactions.\\n\\n3. We  now  have  a  subsample  of  our  dataframe  with  a  50/50  ratio  with  regards  to\\nour classes. We train the models on this subsample. Then we perform this itera‐\\ntion again to shuffle the nonfraud observations in the training sample. We keep\\n\\nCase Study 1: Fraud Detection \\n\\n| \\n\\n161\\n\\n\\x0ctrack of the model performance to see whether our models can maintain a cer‐\\ntain accuracy every time we repeat this process:\\n\\ndf = pd.concat([X_train, Y_train], axis=1)\\n# amount of fraud classes 492 rows.\\nfraud_df = df.loc[df[\\'Class\\'] == 1]\\nnon_fraud_df = df.loc[df[\\'Class\\'] == 0][:492]\\n\\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\\n\\n# Shuffle dataframe rows\\ndf_new = normal_distributed_df.sample(frac=1, random_state=42)\\n# split out validation dataset for the end\\nY_train_new= df_new[\"Class\"]\\nX_train_new = df_new.loc[:, dataset.columns != \\'Class\\']\\n\\nLet us look at the distribution of the classes in the dataset:\\n\\nprint(\\'Distribution of the Classes in the subsample dataset\\')\\nprint(df_new[\\'Class\\'].value_counts()/len(df_new))\\nsns.countplot(\\'Class\\', data=df_new)\\npyplot.title(\\'Equally Distributed Classes\\', fontsize=14)\\npyplot.show()\\n\\nOutput\\n\\nDistribution of the Classes in the subsample dataset\\n1    0.5\\n0    0.5\\nName: Class, dtype: float64\\n\\n162 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cThe data is now balanced, with close to 1,000 observations. We will train all the mod‐\\nels again, including an ANN. Now that the data is balanced, we will focus on accuracy\\nas our main evaluation metric, since it considers both false positives and false nega‐\\ntives. Recall can always be produced if needed:\\n\\n#setting the evaluation metric\\nscoring=\\'accuracy\\'\\n# spot-check the algorithms\\nmodels = []\\nmodels.append((\\'LR\\', LogisticRegression()))\\nmodels.append((\\'LDA\\', LinearDiscriminantAnalysis()))\\nmodels.append((\\'KNN\\', KNeighborsClassifier()))\\nmodels.append((\\'CART\\', DecisionTreeClassifier()))\\nmodels.append((\\'NB\\', GaussianNB()))\\nmodels.append((\\'SVM\\', SVC()))\\n#Neural Network\\nmodels.append((\\'NN\\', MLPClassifier()))\\n# Ensemble Models\\n# Boosting methods\\nmodels.append((\\'AB\\', AdaBoostClassifier()))\\nmodels.append((\\'GBM\\', GradientBoostingClassifier()))\\n# Bagging methods\\nmodels.append((\\'RF\\', RandomForestClassifier()))\\nmodels.append((\\'ET\\', ExtraTreesClassifier()))\\n\\nThe steps to define and compile an ANN-based deep learning model in Keras, along\\nwith all the terms (neurons, activation, momentum, etc.) mentioned in the following\\ncode,  have  been  described  in  Chapter  3.  This  code  can  be  leveraged  for  any  deep\\nlearning–based classification model.\\n\\nKeras-based deep learning model:\\n\\n# Function to create model, required for KerasClassifier\\ndef create_model(neurons=12, activation=\\'relu\\', learn_rate = 0.01, momentum=0):\\n    # create model\\n    model = Sequential()\\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], \\\\\\n      activation=activation))\\n    model.add(Dense(32,activation=activation))\\n    model.add(Dense(1, activation=\\'sigmoid\\'))\\n    # Compile model\\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\\n    model.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\', \\\\\\n    metrics=[\\'accuracy\\'])\\n    return model\\nmodels.append((\\'DNN\\', KerasClassifier(build_fn=create_model,\\\\\\nepochs=50, batch_size=10, verbose=0)))\\n\\nRunning the cross validation on the new set of models results in the following:\\n\\nCase Study 1: Fraud Detection \\n\\n| \\n\\n163\\n\\n\\x0cAlthough  a  couple  of  models,  including  random  forest  (RF)  and  logistic  regression\\n(LR), perform well, GBM slightly edges out the other models. We select this for fur‐\\nther  analysis.  Note  that  the  result  of  the  deep  learning  model  using  Keras  (i.e.,\\n“DNN”) is poor.\\n\\nA grid search is performed for the GBM model by varying the number of estimators\\nand maximum depth. The details of the GBM model and the parameters to tune for\\nthis model are described in Chapter 4.\\n\\n# Grid Search: GradientBoosting Tuning\\nn_estimators = [20,180,1000]\\nmax_depth= [2, 3,5]\\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\\nmodel = GradientBoostingClassifier()\\nkfold = KFold(n_splits=num_folds, random_state=seed)\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\\\\n  cv=kfold)\\ngrid_result = grid.fit(X_train_new, Y_train_new)\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\n\\nOutput\\n\\nBest: 0.936992 using {\\'max_depth\\': 5, \\'n_estimators\\': 1000}\\n\\nIn the next step, the final model is prepared, and the result on the test set is checked:\\n\\n# prepare model\\nmodel = GradientBoostingClassifier(max_depth= 5, n_estimators = 1000)\\nmodel.fit(X_train_new, Y_train_new)\\n# estimate accuracy on Original validation set\\npredictions = model.predict(X_validation)\\nprint(accuracy_score(Y_validation, predictions))\\n\\n164 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cOutput\\n\\n0.9668199852533268\\n\\nThe accuracy of the model is high. Let’s look at the confusion matrix:\\n\\nOutput\\n\\nThe results on the test set are impressive, with a high accuracy and, importantly, no\\nfalse negatives. However, we see that an outcome of using our under-sampled data is\\na propensity for false positives—cases in which nonfraud transactions are misclassi‐\\nfied as fraudulent. This is a trade-off the financial institution would have to consider.\\nThere  is  an  inherent  cost  balance  between  the  operational  overhead,  and  possible\\ncustomer  experience  impact,  from  processing  false  positives  and  the  financial  loss\\nresulting from missing fraud cases through false negatives.\\n\\nConclusion\\n\\nIn  this  case  study,  we  performed  fraud  detection  on  credit  card  transactions.  We\\nillustrated how different classification machine learning models stack up against each\\nother and demonstrated that choosing the right metric can make an important differ‐\\nence  in  model  evaluation.  Under-sampling  was  shown  to  lead  to  a  significant\\nimprovement, as all fraud cases in the test set were correctly identified after applying\\nunder-sampling. This came with a trade-off, though. The reduction in false negatives\\ncame with an increase in false positives.\\n\\nOverall,  by  using  different  machine  learning  models,  choosing  the  right  evaluation\\nmetrics, and handling unbalanced data, we demonstrated how the implementation of\\na simple classification-based model can produce robust results for fraud detection.\\n\\nCase Study 1: Fraud Detection \\n\\n| \\n\\n165\\n\\n\\x0cCase Study 2: Loan Default Probability\\nLending is one of the most important activities of the finance industry. Lenders pro‐\\nvide loans to borrowers in exchange for the promise of repayment with interest. That\\nmeans  the  lender  makes  a  profit  only  if  the  borrower  pays  off  the  loan.  Hence,  the\\ntwo most critical questions in the lending industry are:\\n\\n1. How risky is the borrower?\\n\\n2. Given the borrower’s risk, should we lend to them?\\n\\nDefault  prediction  could  be  described  as  a  perfect  job  for  machine  learning,  as  the\\nalgorithms can be trained on millions of examples of consumer data. Algorithms can\\nperform automated tasks such as matching data records, identifying exceptions, and\\ncalculating  whether  an  applicant  qualifies  for  a  loan.  The  underlying  trends  can  be\\nassessed with algorithms and continuously analyzed to detect trends that might influ‐\\nence lending and underwriting risk in the future.\\n\\nThe goal of this case study is to build a machine learning model to predict the proba‐\\nbility that a loan will default.\\n\\nIn most real-life cases, including loan default modeling, we are unable to work with\\nclean, complete data. Some of the potential problems we are bound to encounter are\\nmissing  values,  incomplete  categorical  data,  and  irrelevant  features.  Although  data\\ncleaning may not be mentioned often, it is critical for the success of machine learning\\napplications. The algorithms that we use can be powerful, but without the relevant or\\nappropriate data, the system may fail to yield ideal results. So one of the focus areas of\\nthis  case  study  will  be  data  preparation  and  cleaning.  Various  techniques  and  con‐\\ncepts of data processing, feature selection, and exploratory analysis are used for data\\ncleaning and organizing the feature space.\\n\\nIn this case study, we will focus on:\\n\\n• Data preparation, data cleaning, and handling a large number of features.\\n\\n• Data discretization and handling categorical data.\\n\\n• Feature selection and data transformation.\\n\\n166 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cBlueprint for Creating a Machine Learning Model for\\nPredicting Loan Default Probability\\n\\n1. Problem definition\\n\\nIn the classification framework for this case study, the predicted variable is charge-off,\\na  debt  that  a  creditor  has  given  up  trying  to  collect  on  after  a  borrower  has  missed\\npayments  for  several  months.  The  predicted  variable  takes  a  value  of  1  in  case  of\\ncharge-off and a value of 0 otherwise.\\n\\nWe will analyze data for loans from 2007 to 2017Q3 from Lending Club, available on\\nKaggle.  Lending  Club  is  a  US  peer-to-peer  lending  company.  It  operates  an  online\\nlending platform that enables borrowers to obtain a loan and investors to purchase\\nnotes  backed  by  payments  made  on  these  loans.  The  dataset  contains  more  than\\n887,000  observations  with  150  variables  containing  complete  loan  data  for  all  loans\\nissued over the specified time period. The features include income, age, credit scores,\\nhome  ownership,  borrower  location,  collections,  and  many  others.  We  will  investi‐\\ngate these 150 predictor variables for feature selection.\\n\\nBy the end of this case study, readers will be familiar with a general approach to loan\\ndefault  modeling,  from  gathering  and  cleaning  data  to  building  and  tuning  a\\nclassifier.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  standard  Python  packages  are  loaded  in  this\\nstep. The details have been presented in the previous case studies. Please refer to the\\nJupyter notebook for this case study for more details.\\n\\n2.2. Loading the data.     The  loan  data  for  the  time  period  from  2007  to  2017Q3  is\\nloaded:\\n\\n# load dataset\\ndataset = pd.read_csv(\\'LoansData.csv.gz\\', compression=\\'gzip\\', \\\\\\nlow_memory=True)\\n\\n3. Data preparation and feature selection\\n\\nIn the first step, let us look at the size of the data:\\n\\ndataset.shape\\n\\nCase Study 2: Loan Default Probability \\n\\n| \\n\\n167\\n\\n\\x0cOutput\\n\\n(1646801, 150)\\n\\nGiven that there are 150 features for each loan, we will first focus on limiting the fea‐\\nture space, followed by the exploratory analysis.\\n\\n3.1. Preparing the predicted variable.    Here, we look at the details of the predicted vari‐\\nable and prepare it. The predicted variable will be derived from the loan_status col‐\\numn. Let’s check the value distributions:2\\n\\ndataset[\\'loan_status\\'].value_counts(dropna=False)\\n\\nOutput\\n\\nCurrent                                                788950\\nFully Paid                                             646902\\nCharged Off                                            168084\\nLate (31-120 days)                                      23763\\nIn Grace Period                                         10474\\nLate (16-30 days)                                        5786\\nDoes not meet the credit policy. Status:Fully Paid       1988\\nDoes not meet the credit policy. Status:Charged Off       761\\nDefault                                                    70\\nNaN                                                        23\\nName: loan_status, dtype: int64\\n\\nFrom the data definition documentation:\\n\\nFully Paid\\n\\nLoans that have been fully repaid.\\n\\nDefault\\n\\nLoans that have not been current for 121 days or more.\\n\\nCharged Off\\n\\nLoans for which there is no longer a reasonable expectation of further payments.\\n\\nA  large  proportion  of  observations  show  a  status  of  Current,  and  we  do  not  know\\nwhether those will be Charged Off, Fully Paid, or Default in the future. The obser‐\\nvations for Default are tiny in number compared to Fully Paid or Charged Off and\\nare  not  considered.  The  remaining  categories  of  loan  status  are  not  of  prime\\nimportance  for  this  analysis.  So,  in  order  to  convert  this  to  a  binary  classification\\nproblem and to analyze in detail the effect of important variables on the loan status,\\nwe will consider only two major categories—Charged Off and Fully Paid:\\n\\n2 The predicted variable is further used for correlation-based feature reduction.\\n\\n168 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cdataset = dataset.loc[dataset[\\'loan_status\\'].isin([\\'Fully Paid\\', \\'Charged Off\\'])]\\ndataset[\\'loan_status\\'].value_counts(normalize=True, dropna=False)\\n\\nOutput\\n\\nFully Paid     0.793758\\nCharged Off    0.206242\\nName: loan_status, dtype: float64\\n\\nAbout 79% of the remaining loans have been fully paid and 21% have been charged\\noff, so we have a somewhat unbalanced classification problem, but it is not as unbal‐\\nanced as the dataset of fraud detection we saw in the previous case study.\\n\\nIn the next step, we create a new binary column in the dataset, where we categorize\\nFully Paid as 0 and Charged Off as 1. This column represents the predicted variable\\nfor this classification problem. A value of 1 in this column indicates the borrower has\\ndefaulted:\\n\\ndataset[\\'charged_off\\'] = (dataset[\\'loan_status\\'] == \\'Charged Off\\').apply(np.uint8)\\ndataset.drop(\\'loan_status\\', axis=1, inplace=True)\\n\\n3.2. Feature selection—limit the feature space.    The full dataset has 150 features for each\\nloan, but not all features contribute to the prediction variable. Removing features of\\nlow importance can improve accuracy and reduce both model complexity and over‐\\nfitting. Training time can also be reduced for very large datasets. We’ll eliminate fea‐\\ntures in the following steps using three different approaches:\\n\\n• Eliminating features that have more than 30% missing values.\\n\\n• Eliminating features that are unintuitive based on subjective judgment.\\n\\n• Eliminating features with low correlation with the predicted variable.\\n\\n3.2.1. Feature elimination based on significant missing values.     First,  we  calculate  the  per‐\\ncentage of missing data for each feature:\\n\\nmissing_fractions = dataset.isnull().mean().sort_values(ascending=False)\\n\\n#Drop the missing fraction\\ndrop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\\ndataset.drop(labels=drop_list, axis=1, inplace=True)\\ndataset.shape\\n\\nOutput\\n\\n(814986, 92)\\n\\nThis dataset has 92 columns remaining once some of the columns with a significant\\nnumber of missing values are dropped.\\n\\n3.2.2. Feature elimination based on intuitiveness.    To filter the features further we check\\nthe description in the data dictionary and keep the features that intuitively contribute\\n\\nCase Study 2: Loan Default Probability \\n\\n| \\n\\n169\\n\\n\\x0cto the prediction of default. We keep features that contain the relevant credit detail of\\nthe  borrower,  including  annual  income,  FICO  score,  and  debt-to-income  ratio.  We\\nalso  keep  those  features  that  are  available  to  investors  when  considering  an  invest‐\\nment  in  the  loan.  These  include  features  in  the  loan  application  and  any  features\\nadded by Lending Club when the loan listing was accepted, such as loan grade and\\ninterest rate.\\n\\nThe list of the features retained are shown in the following code snippet:\\n\\nkeep_list = [\\'charged_off\\',\\'funded_amnt\\',\\'addr_state\\', \\'annual_inc\\', \\\\\\n\\'application_type\\',\\'dti\\', \\'earliest_cr_line\\', \\'emp_length\\',\\\\\\n\\'emp_title\\', \\'fico_range_high\\',\\\\\\n\\'fico_range_low\\', \\'grade\\', \\'home_ownership\\', \\'id\\', \\'initial_list_status\\', \\\\\\n\\'installment\\', \\'int_rate\\', \\'loan_amnt\\', \\'loan_status\\',\\\\\\n\\'mort_acc\\', \\'open_acc\\', \\'pub_rec\\', \\'pub_rec_bankruptcies\\', \\\\\\n\\'purpose\\', \\'revol_bal\\', \\'revol_util\\', \\\\\\n\\'sub_grade\\', \\'term\\', \\'title\\', \\'total_acc\\',\\\\\\n\\'verification_status\\', \\'zip_code\\',\\'last_pymnt_amnt\\',\\\\\\n\\'num_actv_rev_tl\\', \\'mo_sin_rcnt_rev_tl_op\\',\\\\\\n\\'mo_sin_old_rev_tl_op\\',\"bc_util\",\"bc_open_to_buy\",\\\\\\n\"avg_cur_bal\",\"acc_open_past_24mths\" ]\\n\\ndrop_list = [col for col in dataset.columns if col not in keep_list]\\ndataset.drop(labels=drop_list, axis=1, inplace=True)\\ndataset.shape\\n\\nOutput\\n\\n(814986, 39)\\n\\nAfter removing the features in this step, 39 columns remain.\\n\\n3.2.3. Feature elimination based on the correlation.    The next step is to check the correla‐\\ntion  with  the  predicted  variable.  Correlation  gives  us  the  interdependence  between\\nthe predicted variable and the feature. We select features with a moderate-to-strong\\nrelationship  with  the  target  variable  and  drop  those  that  have  a  correlation  of  less\\nthan 3% with the predicted variable:\\n\\ncorrelation = dataset.corr()\\ncorrelation_chargeOff = abs(correlation[\\'charged_off\\'])\\ndrop_list_corr = sorted(list(correlation_chargeOff\\\\\\n  [correlation_chargeOff < 0.03].index))\\nprint(drop_list_corr)\\n\\nOutput\\n\\n[\\'pub_rec\\', \\'pub_rec_bankruptcies\\', \\'revol_bal\\', \\'total_acc\\']\\n\\nThe columns with low correlation are dropped from the dataset, and we are left with\\nonly 35 columns:\\n\\ndataset.drop(labels=drop_list_corr, axis=1, inplace=True)\\n\\n170 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0c4. Feature selection and exploratory analysis\\n\\nIn this step, we perform the exploratory data analysis of the feature selection. Given\\nthat many features had to be eliminated, it is preferable that we perform the explora‐\\ntory data analysis after feature selection to better visualize the relevant features. We\\nwill  also  continue  the  feature  elimination  by  visually  screening  and  dropping  those\\nfeatures deemed irrelevant.\\n\\n4.1. Feature analysis and exploration.     In  the  following  sections,  we  take  a  deeper  dive\\ninto the dataset features.\\n\\n4.1.1. Analyzing the categorical features.    Let us look at the some of the categorical fea‐\\ntures in the dataset.\\n\\nFirst, let’s look at the id, emp_title, title, and zip_code features:\\n\\ndataset[[\\'id\\',\\'emp_title\\',\\'title\\',\\'zip_code\\']].describe()\\n\\nOutput\\n\\nid\\n814986\\n\\nemp_title\\n766415\\n\\ncount\\n\\nunique\\n\\n814986\\n\\n280473\\n\\ntitle\\n807068\\n\\n60298\\n\\nzip_code\\n814986\\n\\n925\\n\\ntop\\n\\nfreq\\n\\n14680062\\n\\nTeacher\\n\\nDebt consolidation 945xx\\n\\n1\\n\\n11351\\n\\n371874\\n\\n9517\\n\\nIDs are all unique and irrelevant for modeling. There are too many unique values for\\nemployment titles and titles. Occupation and job title may provide some information\\nfor default modeling; however, we assume much of this information is embedded in\\nthe  verified  income  of  the  customer.  Moreover,  additional  cleaning  steps  on  these\\nfeatures,  such  as  standardizing  or  grouping  the  titles,  would  be  necessary  to  extract\\nany marginal information. This work is outside the scope of this case study but could\\nbe explored in subsequent iterations of the model.\\n\\nGeography could play a role in credit determination, and zip codes provide a granu‐\\nlar view of this dimension. Again, additional work would be necessary to prepare this\\nfeature for modeling and was deemed outside the scope of this case study.\\n\\ndataset.drop([\\'id\\',\\'emp_title\\',\\'title\\',\\'zip_code\\'], axis=1, inplace=True)\\n\\nLet’s look at the term feature.\\n\\nTerm refers to the number of payments on the loan. Values are in months and can be\\neither 36 or 60. The 60-month loans are more likely to charge off.\\n\\nCase Study 2: Loan Default Probability \\n\\n| \\n\\n171\\n\\n\\x0cLet’s convert term to integers and group by the term for further analysis:\\n\\ndataset[\\'term\\'] = dataset[\\'term\\'].apply(lambda s: np.int8(s.split()[0]))\\ndataset.groupby(\\'term\\')[\\'charged_off\\'].value_counts(normalize=True).loc[:,1]\\n\\nOutput\\n\\nterm\\n36    0.165710\\n60    0.333793\\nName: charged_off, dtype: float64\\n\\nLoans with five-year periods are more than twice as likely to charge-off as loans with\\nthree-year periods. This feature seems to be important for the prediction.\\n\\nLet’s look at the emp_length feature:\\n\\ndataset[\\'emp_length\\'].replace(to_replace=\\'10+ years\\', value=\\'10 years\\',\\\\\\n  inplace=True)\\n\\ndataset[\\'emp_length\\'].replace(\\'< 1 year\\', \\'0 years\\', inplace=True)\\n\\ndef emp_length_to_int(s):\\n    if pd.isnull(s):\\n        return s\\n    else:\\n        return np.int8(s.split()[0])\\n\\ndataset[\\'emp_length\\'] = dataset[\\'emp_length\\'].apply(emp_length_to_int)\\ncharge_off_rates = dataset.groupby(\\'emp_length\\')[\\'charged_off\\'].value_counts\\\\\\n  (normalize=True).loc[:,1]\\nsns.barplot(x=charge_off_rates.index, y=charge_off_rates.values)\\n\\nOutput\\n\\n172 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cLoan  status  does  not  appear  to  vary  much  with  employment  length  (on  average);\\nhence this feature is dropped:\\n\\ndataset.drop([\\'emp_length\\'], axis=1, inplace=True)\\n\\nLet’s look at the sub_grade feature:\\n\\ncharge_off_rates = dataset.groupby(\\'sub_grade\\')[\\'charged_off\\'].value_counts\\\\\\n(normalize=True).loc[:,1]\\nsns.barplot(x=charge_off_rates.index, y=charge_off_rates.values)\\n\\nOutput\\n\\nAs shown in the chart, there’s a clear trend of higher probability of charge-off as the\\nsub-grade worsens, and so it is considered to be a key feature.\\n\\n4.1.2. Analyzing the continuous features.    Let’s look at the annual_inc feature:\\n\\ndataset[[\\'annual_inc\\']].describe()\\n\\nOutput\\n\\nannual_inc\\n8.149860e+05\\n\\ncount\\n\\nmean 7.523039e+04\\n\\nstd\\n\\nmin\\n\\n6.524373e+04\\n\\n0.000000e+00\\n\\n25% 4.500000e+04\\n\\n50% 6.500000e+04\\n\\n75% 9.000000e+04\\n\\nmax\\n\\n9.550000e+06\\n\\nCase Study 2: Loan Default Probability \\n\\n| \\n\\n173\\n\\n\\x0cAnnual income ranges from $0 to $9,550,000, with a median of $65,000. Because of\\nthe large range of incomes, we use a log transform of the annual income variable:\\n\\ndataset[\\'log_annual_inc\\'] = dataset[\\'annual_inc\\'].apply(lambda x: np.log10(x+1))\\ndataset.drop(\\'annual_inc\\', axis=1, inplace=True)\\n\\nLet’s look at the FICO score (fico_range_low, fico_range_high) feature:\\n\\ndataset[[\\'fico_range_low\\',\\'fico_range_high\\']].corr()\\n\\nOutput\\n\\nfico_range_low fico_range_high\\n\\nfico_range_low 1.0\\n\\nfico_range_high 1.0\\n\\n1.0\\n\\n1.0\\n\\nGiven  that  the  correlation  between  FICO  low  and  high  is  1,  it  is  preferred  that  we\\nkeep only one feature, which we take as the average of FICO scores:\\n\\ndataset[\\'fico_score\\'] = 0.5*dataset[\\'fico_range_low\\'] +\\\\\\n 0.5*dataset[\\'fico_range_high\\']\\n\\ndataset.drop([\\'fico_range_high\\', \\'fico_range_low\\'], axis=1, inplace=True)\\n\\n4.2. Encoding categorical data.    In order to use a feature in the classification models, we\\nneed to convert the categorical data (i.e., text features) to its numeric representation.\\nThis process is called encoding. There can be different ways of encoding. However,\\nfor  this  case  study  we  will  use  a  label  encoder,  which  encodes  labels  with  a  value\\nbetween 0 and n, where n is the number of distinct labels. The LabelEncoder func‐\\ntion  from  sklearn  is  used  in  the  following  step,  and  all  the  categorical  columns  are\\nencoded at once:\\n\\nfrom sklearn.preprocessing import LabelEncoder\\n# Categorical boolean mask\\ncategorical_feature_mask = dataset.dtypes==object\\n# filter categorical columns using mask and turn it into a list\\ncategorical_cols = dataset.columns[categorical_feature_mask].tolist()\\n\\nLet us look at the categorical columns:\\n\\ncategorical_cols\\n\\nOutput\\n\\n[\\'grade\\',\\n \\'sub_grade\\',\\n \\'home_ownership\\',\\n \\'verification_status\\',\\n \\'purpose\\',\\n \\'addr_state\\',\\n \\'initial_list_status\\',\\n \\'application_type\\']\\n\\n174 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0c4.3. Sampling data.    Given that the loan data is skewed, it is sampled to have an equal\\nnumber of charge-off and no charge-off observations. Sampling leads to a more bal‐\\nanced dataset and avoids overfitting:3\\n\\nloanstatus_0 = dataset[dataset[\"charged_off\"]==0]\\nloanstatus_1 = dataset[dataset[\"charged_off\"]==1]\\nsubset_of_loanstatus_0 = loanstatus_0.sample(n=5500)\\nsubset_of_loanstatus_1 = loanstatus_1.sample(n=5500)\\ndataset = pd.concat([subset_of_loanstatus_1, subset_of_loanstatus_0])\\ndataset = dataset.sample(frac=1).reset_index(drop=True)\\nprint(\"Current shape of dataset :\",dataset.shape)\\n\\nAlthough  sampling  may  have  its  advantages,  there  might  be  some  disadvantages  as\\nwell.  Sampling  may  exclude  some  data  that  might  not  be  homogeneous  to  the  data\\nthat  is  taken.  This  affects  the  level  of  accuracy  in  the  results.  Also,  selection  of  the\\nproper size of samples is a difficult job. Hence, sampling should be performed with\\ncaution and should generally be avoided in the case of a relatively balanced dataset.\\n\\n5. Evaluate algorithms and models\\n\\n5.1. Train-test split.    Splitting out the validation dataset for the model evaluation is the\\nnext step:\\n\\nY= dataset[\"charged_off\"]\\nX = dataset.loc[:, dataset.columns != \\'charged_off\\']\\nvalidation_size = 0.2\\nseed = 7\\nX_train, X_validation, Y_train, Y_validation = \\\\\\ntrain_test_split(X, Y, test_size=validation_size, random_state=seed)\\n\\n5.2. Test options and evaluation metrics.     In  this  step,  the  test  options  and  evaluation\\nmetrics are selected. The roc_auc evaluation metric is selected for this classification.\\nThe  details  of  this  metric  were  provided  in  Chapter  4.  This  metric  represents  a\\nmodel’s ability to discriminate between positive and negative classes. An roc_auc of\\n1.0 represents a model that made all predictions perfectly, and a value of 0.5 repre‐\\nsents a model that is as good as random.\\n\\nnum_folds = 10\\nscoring = \\'roc_auc\\'\\n\\nThe model cannot afford to have a high amount of false negatives as that leads to a\\nnegative impact on the investors and the credibility of the company. So we can use\\nrecall as we did in the fraud detection use case.\\n\\n3 Sampling is covered in detail in “Case Study 1: Fraud Detection” on page 153.\\n\\nCase Study 2: Loan Default Probability \\n\\n| \\n\\n175\\n\\n\\x0c5.3. Compare models and algorithms.     Let  us  spot-check  the  classification  algorithms.\\nWe include ANN and ensemble models in the list of models to be checked:\\n\\nmodels = []\\nmodels.append((\\'LR\\', LogisticRegression()))\\nmodels.append((\\'LDA\\', LinearDiscriminantAnalysis()))\\nmodels.append((\\'KNN\\', KNeighborsClassifier()))\\nmodels.append((\\'CART\\', DecisionTreeClassifier()))\\nmodels.append((\\'NB\\', GaussianNB()))\\n# Neural Network\\nmodels.append((\\'NN\\', MLPClassifier()))\\n# Ensemble Models\\n# Boosting methods\\nmodels.append((\\'AB\\', AdaBoostClassifier()))\\nmodels.append((\\'GBM\\', GradientBoostingClassifier()))\\n# Bagging methods\\nmodels.append((\\'RF\\', RandomForestClassifier()))\\nmodels.append((\\'ET\\', ExtraTreesClassifier()))\\n\\nAfter performing the k-fold cross validation on the models shown above, the overall\\nperformance is as follows:\\n\\nThe gradient boosting method (GBM) model performs best, and we select it for grid\\nsearch  in  the  next  step.  The  details  of  GBM  along  with  the  model  parameters  are\\ndescribed in Chapter 4.\\n\\n176 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0c6. Model tuning and grid search\\n\\nWe tune the number of estimator and maximum depth hyperparameters, which were\\ndiscussed in Chapter 4:\\n\\n# Grid Search: GradientBoosting Tuning\\nn_estimators = [20,180]\\nmax_depth= [3,5]\\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\\nmodel = GradientBoostingClassifier()\\nkfold = KFold(n_splits=num_folds, random_state=seed)\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, \\\\\\n  cv=kfold)\\ngrid_result = grid.fit(X_train, Y_train)\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\n\\nOutput\\n\\nBest: 0.952950 using {\\'max_depth\\': 5, \\'n_estimators\\': 180}\\n\\nA GBM model with  max_depth of 5 and number of estimators of 150 results in the\\nbest model.\\n\\n7. Finalize the model\\n\\nNow, we perform the final steps for selecting a model.\\n\\n7.1. Results on the test dataset.     Let  us  prepare  the  GBM  model  with  the  parameters\\nfound during the grid search step and check the results on the test dataset:\\n\\nmodel = GradientBoostingClassifier(max_depth= 5, n_estimators= 180)\\nmodel.fit(X_train, Y_train)\\n\\n# estimate accuracy on validation set\\npredictions = model.predict(X_validation)\\nprint(accuracy_score(Y_validation, predictions))\\n\\nOutput\\n\\n0.889090909090909\\n\\nCase Study 2: Loan Default Probability \\n\\n| \\n\\n177\\n\\n\\x0cThe  accuracy  of  the  model  is  a  reasonable  89%  on  the  test  set.  Let  us  examine  the\\nconfusion matrix:\\n\\nLooking at the confusion matrix and the overall result of the test set, both the rate of\\nfalse positives and the rate of false negatives are lower; the overall model performance\\nlooks good and is in line with the training set results.\\n\\n7.2. Variable intuition/feature importance.     In  this  step,  we  compute  and  display  the\\nvariable importance of our trained model:\\n\\nprint(model.feature_importances_) #use inbuilt class feature_importances\\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\\n#plot graph of feature importances for better visualization\\nfeat_importances.nlargest(10).plot(kind=\\'barh\\')\\npyplot.show()\\n\\nOutput\\n\\nThe results of the model importance are intuitive. The last payment amount seems to\\nbe the most important feature, followed by loan term and sub-grade.\\n\\n178 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cConclusion\\n\\nIn  this  case  study,  we  introduced  the  classification-based  tree  algorithm  applied  to\\nloan default prediction. We showed that data preparation is one of the most impor‐\\ntant steps. We addressed this by performing feature elimination using different tech‐\\nniques, such as feature intuition, correlation analysis, visualization, and data quality\\nchecks of the features. We illustrated that there can be different ways of handling and\\nanalyzing the categorical data and converting categorical data into a usable format for\\nthe models.\\n\\nWe emphasized that performing data processing and establishing an understanding\\nof  variable  importance  is  key  in  the  model  development  process.  A  focus  on  these\\nsteps led to the implementation of a simple classification-based model that produced\\nrobust results for default prediction.\\n\\nCase Study 3: Bitcoin Trading Strategy\\nFirst released as open source in 2009 by the pseudonymous Satoshi Nakamoto, bit‐\\ncoin is the longest-running and most well-known cryptocurrency.\\n\\nA  major  drawback  of  cryptocurrency  trading  is  the  volatility  of  the  market.  Since\\ncryptocurrency markets trade 24/7, tracking cryptocurrency positions against quickly\\nchanging market dynamics can rapidly become an impossible task to manage. This is\\nwhere automated trading algorithms and trading bots can assist.\\n\\nVarious  machine  learning  algorithms  can  be  used  to  generate  trading  signals  in  an\\nattempt  to  predict  the  market’s  movement.  One  could  use  machine  learning  algo‐\\nrithms  to  classify  the  next  day’s  movement  into  three  categories:  market  will  rise\\n(take  a  long  position),  market  will  fall  (take  a  short  position),  or  market  will  move\\nsideways (take no position). Since we know the market direction, we can decide the\\noptimum entry and exit points.\\n\\nMachine learning has one key aspect called feature engineering. It means that we can\\ncreate new, intuitive features and feed them to a machine learning algorithm in order\\nto achieve better results. We can introduce different technical indicators as features\\nto help predict future prices of an asset. These technical indicators are derived from\\nmarket variables such as price or volume and have additional information or signals\\nembedded  in  them.  There  are  many  different  categories  of  technical  indicators,\\nincluding trend, volume, volatility, and momentum indicators.\\n\\nIn this case study, we will use various classification-based models to predict whether\\nthe current position signal is buy or sell. We will create additional trend and momen‐\\ntum indicators from market prices to leverage as additional features in the prediction.\\n\\nCase Study 3: Bitcoin Trading Strategy \\n\\n| \\n\\n179\\n\\n\\x0cIn this case study, we will focus on:\\n\\n• Building  a  trading  strategy  using  classification  (classification  of  long/short  sig‐\\n\\nnals).\\n\\n• Feature engineering and constructing technical indicators of trend, momentum,\\n\\nand mean reversion.\\n\\n• Building a framework for backtesting results of a trading strategy.\\n\\n• Choosing the right evaluation metric to assess a trading strategy.\\n\\nBlueprint for Using Classification-Based Models to Predict\\nWhether to Buy or Sell in the Bitcoin Market\\n\\n1. Problem definition\\n\\nThe problem of predicting a buy or sell signal for a trading strategy is defined in the\\nclassification framework, where the predicted variable has a value of 1 for buy and 0\\nfor  sell.  This  signal  is  decided  through  the  comparison  of  the  short-term  and  long-\\nterm price trends.\\n\\nThe data used is from one of the largest bitcoin exchanges in terms of average daily\\nvolume, Bitstamp. The data covers prices from January 2012 to May 2017. Different\\ntrend and momentum indicators are created from the data and are added as features\\nto enhance the performance of the prediction model.\\n\\nBy  the  end  of  this  case  study,  readers  will  be  familiar  with  a  general  approach  to\\nbuilding a trading strategy, from cleaning data and feature engineering to model tun‐\\ning and developing a backtesting framework.\\n\\n2. Getting started—loading the data and Python packages\\n\\nLet’s load the packages and the data.\\n\\n2.1. Loading the Python packages.     The  standard  Python  packages  are  loaded  in  this\\nstep. The details have been presented in the previous case studies. Refer to the Jupyter\\nnotebook for this case study for more details.\\n\\n2.2. Loading the data.    The bitcoin data fetched from the Bitstamp website is loaded in\\nthis step:\\n\\n180 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0c# load dataset\\ndataset = pd.read_csv(\\'BitstampData.csv\\')\\n\\n3. Exploratory data analysis\\n\\nIn this step, we will take a detailed look at this data.\\n\\n3.1. Descriptive statistics.    First, let us look at the shape of the data:\\n\\ndataset.shape\\n\\nOutput\\n\\n(2841377, 8)\\n\\n# peek at data\\nset_option(\\'display.width\\', 100)\\ndataset.tail(2)\\n\\nOutput\\n\\nTimestamp\\n1496188560\\n\\nOpen\\n2190.49\\n\\nHigh\\n2190.49\\n\\nLow\\n2181.37\\n\\nClose\\n2181.37\\n\\n2841372\\n\\nVolume_(BTC) Volume_(Currency) Weighted_Price\\n1.700166\\n\\n2190.247337\\n\\n3723.784755\\n\\n2841373\\n\\n1496188620\\n\\n2190.50\\n\\n2197.52\\n\\n2186.17\\n\\n2195.63\\n\\n6.561029\\n\\n14402.811961\\n\\n2195.206304\\n\\nThe  dataset  has  minute-by-minute  data  of  OHLC  (Open,  High,  Low,  Close)  and\\ntraded  volume  of  bitcoin.  The  dataset  is  large,  with  approximately  2.8  million  total\\nobservations.\\n\\n4. Data preparation\\n\\nIn this part, we will clean the data to prepare for modeling.\\n\\n4.1. Data cleaning.    We clean the data by filling the NaNs with the last available values:\\n\\ndataset[dataset.columns.values] = dataset[dataset.columns.values].ffill()\\n\\nThe Timestamp column is not useful for modeling and is dropped from the dataset:\\n\\ndataset=dataset.drop(columns=[\\'Timestamp\\'])\\n\\n4.2. Preparing the data for classification.    As a first step, we will create the target variable\\nfor our model. This is the column that will indicate whether the trading signal is buy\\nor  sell.  We  define  the  short-term  price  as  the  10-day  rolling  average  and  the  long-\\nterm price as the 60-day rolling average. We attach a label of 1 (0) if the short-term\\nprice is higher (lower) than the long-term price:\\n\\n# Create short simple moving average over the short window\\ndataset[\\'short_mavg\\'] = dataset[\\'Close\\'].rolling(window=10, min_periods=1,\\\\\\ncenter=False).mean()\\n\\nCase Study 3: Bitcoin Trading Strategy \\n\\n| \\n\\n181\\n\\n\\x0c# Create long simple moving average over the long window\\ndataset[\\'long_mavg\\'] = dataset[\\'Close\\'].rolling(window=60, min_periods=1,\\\\\\ncenter=False).mean()\\n\\n# Create signals\\ndataset[\\'signal\\'] = np.where(dataset[\\'short_mavg\\'] >\\ndataset[\\'long_mavg\\'], 1.0, 0.0)\\n\\n4.3. Feature engineering.     We  begin  feature  engineering  by  analyzing  the  features  we\\nexpect may influence the performance of the prediction model. Based on a concep‐\\ntual understanding of key factors that drive investment strategies, the task at hand is\\nto  identify  and  construct  new  features  that  may  capture  the  risks  or  characteristics\\nembodied by these return drivers. For this case study, we will explore the efficacy of\\nspecific momentum technical indicators.\\n\\nThe current data of the bitcoin consists of date, open, high, low, close, and volume.\\nUsing this data, we calculate the following momentum indicators:\\n\\nMoving average\\n\\nA  moving  average  provides  an  indication  of  a  price  trend  by  cutting  down  the\\namount of noise in the series.\\n\\nStochastic oscillator %K\\n\\nA stochastic oscillator is a momentum indicator that compares the closing price\\nof a security to a range of its previous prices over a certain period of time. %K\\nand %D are slow and fast indicators. The fast indicator is more sensitive than the\\nslow indicator to changes in the price of the underlying security and will likely\\nresult in many transaction signals.\\n\\nRelative strength index (RSI)\\n\\nThis  is  a  momentum  indicator  that  measures  the  magnitude  of  recent  price\\nchanges to evaluate overbought or oversold conditions in the price of a stock or\\nother asset. The RSI ranges from 0 to 100. An asset is deemed to be overbought\\nonce  the  RSI  approaches  70,  meaning  that  the  asset  may  be  getting  overvalued\\nand is a good candidate for a pullback. Likewise, if the RSI approaches 30, it is an\\nindication that the asset may be getting oversold and is therefore likely to become\\nundervalued.\\n\\nRate of change (ROC)\\n\\nThis is a momentum oscillator that measures the percentage change between the\\ncurrent  price  and  the  n  period  past  prices.  Assets  with  higher  ROC  values  are\\nconsidered more likely to be overbought; those with lower ROC, more likely to\\nbe oversold.\\n\\n182 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cMomentum (MOM)\\n\\nThis is the rate of acceleration of a security’s price or volume—that is, the speed\\nat which the price is changing.\\n\\nThe  following  steps  show  how  to  generate  some  useful  features  for  prediction.  The\\nfeatures for trend and momentum can be leveraged for other trading strategy models:\\n\\n#calculation of exponential moving average\\ndef EMA(df, n):\\n    EMA = pd.Series(df[\\'Close\\'].ewm(span=n, min_periods=n).mean(), name=\\'EMA_\\'\\\\\\n     + str(n))\\n    return EMA\\ndataset[\\'EMA10\\'] = EMA(dataset, 10)\\ndataset[\\'EMA30\\'] = EMA(dataset, 30)\\ndataset[\\'EMA200\\'] = EMA(dataset, 200)\\ndataset.head()\\n\\n#calculation of rate of change\\ndef ROC(df, n):\\n    M = df.diff(n - 1)\\n    N = df.shift(n - 1)\\n    ROC = pd.Series(((M / N) * 100), name = \\'ROC_\\' + str(n))\\n    return ROC\\ndataset[\\'ROC10\\'] = ROC(dataset[\\'Close\\'], 10)\\ndataset[\\'ROC30\\'] = ROC(dataset[\\'Close\\'], 30)\\n\\n#calculation of price momentum\\ndef MOM(df, n):\\n    MOM = pd.Series(df.diff(n), name=\\'Momentum_\\' + str(n))\\n    return MOM\\ndataset[\\'MOM10\\'] = MOM(dataset[\\'Close\\'], 10)\\ndataset[\\'MOM30\\'] = MOM(dataset[\\'Close\\'], 30)\\n\\n#calculation of relative strength index\\ndef RSI(series, period):\\n delta = series.diff().dropna()\\n u = delta * 0\\n d = u.copy()\\n u[delta > 0] = delta[delta > 0]\\n d[delta < 0] = -delta[delta < 0]\\n u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\\n u = u.drop(u.index[:(period-1)])\\n d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\\n d = d.drop(d.index[:(period-1)])\\n rs = u.ewm(com=period-1, adjust=False).mean() / \\\\\\n d.ewm(com=period-1, adjust=False).mean()\\n return 100 - 100 / (1 + rs)\\ndataset[\\'RSI10\\'] = RSI(dataset[\\'Close\\'], 10)\\ndataset[\\'RSI30\\'] = RSI(dataset[\\'Close\\'], 30)\\ndataset[\\'RSI200\\'] = RSI(dataset[\\'Close\\'], 200)\\n\\n#calculation of stochastic osillator.\\n\\nCase Study 3: Bitcoin Trading Strategy \\n\\n| \\n\\n183\\n\\n\\x0cdef STOK(close, low, high, n):\\n STOK = ((close - low.rolling(n).min()) / (high.rolling(n).max() - \\\\\\n low.rolling(n).min())) * 100\\n return STOK\\n\\ndef STOD(close, low, high, n):\\n STOK = ((close - low.rolling(n).min()) / (high.rolling(n).max() - \\\\\\n low.rolling(n).min())) * 100\\n STOD = STOK.rolling(3).mean()\\n return STOD\\n\\ndataset[\\'%K10\\'] = STOK(dataset[\\'Close\\'], dataset[\\'Low\\'], dataset[\\'High\\'], 10)\\ndataset[\\'%D10\\'] = STOD(dataset[\\'Close\\'], dataset[\\'Low\\'], dataset[\\'High\\'], 10)\\ndataset[\\'%K30\\'] = STOK(dataset[\\'Close\\'], dataset[\\'Low\\'], dataset[\\'High\\'], 30)\\ndataset[\\'%D30\\'] = STOD(dataset[\\'Close\\'], dataset[\\'Low\\'], dataset[\\'High\\'], 30)\\ndataset[\\'%K200\\'] = STOK(dataset[\\'Close\\'], dataset[\\'Low\\'], dataset[\\'High\\'], 200)\\ndataset[\\'%D200\\'] = STOD(dataset[\\'Close\\'], dataset[\\'Low\\'], dataset[\\'High\\'], 200)\\n\\n#calculation of moving average\\ndef MA(df, n):\\n    MA = pd.Series(df[\\'Close\\'].rolling(n, min_periods=n).mean(), name=\\'MA_\\'\\\\\\n     + str(n))\\n    return MA\\ndataset[\\'MA21\\'] = MA(dataset, 10)\\ndataset[\\'MA63\\'] = MA(dataset, 30)\\ndataset[\\'MA252\\'] = MA(dataset, 200)\\n\\nWith our features completed, we’ll prepare them for use.\\n\\n4.4. Data visualization.     In  this  step,  we  visualize  different  properties  of  the  features\\nand the predicted variable:\\n\\ndataset[[\\'Weighted_Price\\']].plot(grid=True)\\nplt.show()\\n\\nOutput\\n\\n184 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cThe chart illustrates a sharp rise in the price of bitcoin, increasing from close to $0 to\\naround $2,500 in 2017. Also, high price volatility is readily visible.\\n\\nLet us look at the distribution of the predicted variable:\\n\\nfig = plt.figure()\\nplot = dataset.groupby([\\'signal\\']).size().plot(kind=\\'barh\\', color=\\'red\\')\\nplt.show()\\n\\nOutput\\n\\nThe predicted variable is 1 more than 52% of the time, meaning there are more buy\\nsignals  than  sell  signals.  The  predicted  variable  is  relatively  balanced,  especially  as\\ncompared to the fraud dataset we saw in the first case study.\\n\\n5. Evaluate algorithms and models\\n\\nIn this step, we will evaluate different algorithms.\\n\\n5.1. Train-test split.    We first split the dataset into training (80%) and test (20%) sets.\\nFor this case study we use 100,000 observations for a faster calculation. The next steps\\nwould be same in the event we want to use the entire dataset:\\n\\n# split out validation dataset for the end\\nsubset_dataset= dataset.iloc[-100000:]\\nY= subset_dataset[\"signal\"]\\nX = subset_dataset.loc[:, dataset.columns != \\'signal\\']\\nvalidation_size = 0.2\\nseed = 1\\nX_train, X_validation, Y_train, Y_validation =\\\\\\ntrain_test_split(X, Y, test_size=validation_size, random_state=1)\\n\\nCase Study 3: Bitcoin Trading Strategy \\n\\n| \\n\\n185\\n\\n\\x0c5.2. Test options and evaluation metrics.    Accuracy can be used as the evaluation metric\\nsince there is not a significant class imbalance in the data:\\n\\n# test options for classification\\nnum_folds = 10\\nscoring = \\'accuracy\\'\\n\\n5.3. Compare models and algorithms.    In order to know which algorithm is best for our\\nstrategy, we evaluate the linear, nonlinear, and ensemble models.\\n\\n5.3.1. Models.    Checking the classification algorithms:\\n\\nmodels = []\\nmodels.append((\\'LR\\', LogisticRegression(n_jobs=-1)))\\nmodels.append((\\'LDA\\', LinearDiscriminantAnalysis()))\\nmodels.append((\\'KNN\\', KNeighborsClassifier()))\\nmodels.append((\\'CART\\', DecisionTreeClassifier()))\\nmodels.append((\\'NB\\', GaussianNB()))\\n#Neural Network\\nmodels.append((\\'NN\\', MLPClassifier()))\\n# Ensemble Models\\n# Boosting methods\\nmodels.append((\\'AB\\', AdaBoostClassifier()))\\nmodels.append((\\'GBM\\', GradientBoostingClassifier()))\\n# Bagging methods\\nmodels.append((\\'RF\\', RandomForestClassifier(n_jobs=-1)))\\n\\nAfter  performing  the  k-fold  cross  validation,  the  comparison  of  the  models  is  as\\nfollows:\\n\\n186 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cAlthough some of the models show promising results, we prefer an ensemble model\\ngiven the huge size of the dataset, the large number of features, and an expected non‐\\nlinear  relationship  between  the  predicted  variable  and  the  features.  Random  forest\\nhas the best performance among the ensemble models.\\n\\n6. Model tuning and grid search\\n\\nA  grid  search  is  performed  for  the  random  forest  model  by  varying  the  number  of\\nestimators  and  maximum  depth.  The  details  of  the  random  forest  model  and  the\\nparameters to tune are discussed in Chapter 4:\\n\\nn_estimators = [20,80]\\nmax_depth= [5,10]\\ncriterion = [\"gini\",\"entropy\"]\\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth, \\\\\\n  criterion = criterion )\\nmodel = RandomForestClassifier(n_jobs=-1)\\nkfold = KFold(n_splits=num_folds, random_state=seed)\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, \\\\\\n  scoring=scoring, cv=kfold)\\ngrid_result = grid.fit(X_train, Y_train)\\nprint(\"Best: %f using %s\" % (grid_result.best_score_,\\\\\\n  grid_result.best_params_))\\n\\nOutput\\n\\nBest: 0.903438 using {\\'criterion\\': \\'gini\\', \\'max_depth\\': 10, \\'n_estimators\\': 80}\\n\\n7. Finalize the model\\n\\nLet us finalize the model with the best parameters found during the tuning step and\\nperform the variable intuition.\\n\\n7.1. Results on the test dataset.    In this step, we evaluate the selected model on the test\\nset:\\n\\n# prepare model\\nmodel = RandomForestClassifier(criterion=\\'gini\\', n_estimators=80,max_depth=10)\\n\\n#model = LogisticRegression()\\nmodel.fit(X_train, Y_train)\\n\\n# estimate accuracy on validation set\\npredictions = model.predict(X_validation)\\nprint(accuracy_score(Y_validation, predictions))\\n\\nOutput\\n\\n0.9075\\n\\nCase Study 3: Bitcoin Trading Strategy \\n\\n| \\n\\n187\\n\\n\\x0cThe selected model performs quite well, with an accuracy of 90.75%. Let us look at\\nthe confusion matrix:\\n\\nThe  overall  model  performance  is  reasonable  and  is  in  line  with  the  training  set\\nresults.\\n\\n7.2. Variable intuition/feature importance.    Let us look into the feature importance of the\\nmodel:\\n\\nImportance = pd.DataFrame({\\'Importance\\':model.feature_importances_*100},\\\\\\n index=X.columns)\\nImportance.sort_values(\\'Importance\\', axis=0, ascending=True).plot(kind=\\'barh\\', \\\\\\ncolor=\\'r\\' )\\nplt.xlabel(\\'Variable Importance\\')\\n\\nOutput\\n\\nThe result of the variable importance looks intuitive, and the momentum indicators\\nof RSI and MOM over the last 30 days seem to be the two most important features.\\n\\n188 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cThe  feature  importance  chart  corroborates  the  fact  that  introducing  new  features\\nleads to an improvement in the model performance.\\n\\n7.3. Backtesting results.     In  this  additional  step,  we  perform  a  backtest  on  the  model\\nwe’ve  developed.  We  create  a  column  for  strategy  returns  by  multiplying  the  daily\\nreturns  by  the  position  that  was  held  at  the  close  of  business  the  previous  day  and\\ncompare it against the actual returns.\\n\\nBacktesting Trading Strategies\\n\\nA  backtesting  approach  similar  to  the  one  presented  in  this  case\\nstudy can be used to quickly backtest any trading strategy.\\n\\nbacktestdata = pd.DataFrame(index=X_validation.index)\\nbacktestdata[\\'signal_pred\\'] = predictions\\nbacktestdata[\\'signal_actual\\'] = Y_validation\\nbacktestdata[\\'Market Returns\\'] = X_validation[\\'Close\\'].pct_change()\\nbacktestdata[\\'Actual Returns\\'] = backtestdata[\\'Market Returns\\'] *\\\\\\nbacktestdata[\\'signal_actual\\'].shift(1)\\nbacktestdata[\\'Strategy Returns\\'] = backtestdata[\\'Market Returns\\'] * \\\\\\nbacktestdata[\\'signal_pred\\'].shift(1)\\nbacktestdata=backtestdata.reset_index()\\nbacktestdata.head()\\nbacktestdata[[\\'Strategy Returns\\',\\'Actual Returns\\']].cumsum().hist()\\nbacktestdata[[\\'Strategy Returns\\',\\'Actual Returns\\']].cumsum().plot()\\n\\nOutput\\n\\nCase Study 3: Bitcoin Trading Strategy \\n\\n| \\n\\n189\\n\\n\\x0cLooking  at  the  backtesting  results,  we  do  not  deviate  significantly  from  the  actual\\nmarket  return.  Indeed,  the  achieved  momentum  trading  strategy  made  us  better  at\\npredicting the price direction to buy or sell in order to make profits. However, as our\\naccuracy is not 100% (but more than 96%), we made relatively few losses compared\\nto the actual returns.\\n\\nConclusion\\n\\nThis case study demonstrated that framing the problem is a key step when tackling a\\nfinance  problem  with  machine  learning.  In  doing  so,  it  was  detemined  that  trans‐\\nforming the labels according to an investment objective and performing feature engi‐\\nneering  were  required  for  this  trading  strategy.  We  demonstrated  the  efficiency  of\\nusing intuitive features related to the trend and momentum of the price movement.\\nThis  helped  increase  the  predictive  power  of  the  model.  Finally,  we  introduced  a\\nbacktesting framework, which allowed us to simulate a trading strategy using histori‐\\ncal data. This enabled us to generate results and analyze risk and profitability before\\nrisking any actual capital.\\n\\nChapter Summary\\nIn “Case Study 1: Fraud Detection” on page 153, we explored the issue of an unbal‐\\nanced  dataset  and  the  importance  of  having  the  right  evaluation  metric.  In  “Case\\nStudy 2: Loan Default Probability” on page 166, various techniques and concepts of\\ndata  processing,  feature  selection,  and  exploratory  analysis  were  covered.  In  “Case\\nStudy 3: Bitcoin Trading Strategy” on page 179, we looked at ways to create technical\\n\\n190 \\n\\n| \\n\\nChapter 6: Supervised Learning: Classification\\n\\n\\x0cindicators as features in order to use them for model enhancement. We also prepared\\na backtesting framework for a trading strategy.\\n\\nOverall,  the  concepts  in  Python,  machine  learning,  and  finance  presented  in  this\\nchapter can used as a blueprint for any other classification-based problem in finance.\\n\\nExercises\\n\\n• Predict whether a stock price will go up or down using the features related to the\\nstock  or  macroeconomic  variables  (use  the  ideas  from  the  bitcoin-based  case\\nstudy presented in this chapter).\\n\\n• Create a model to detect money laundering using the features of a transaction. A\\n\\nsample dataset for this exercise can be obtained from Kaggle.\\n\\n• Perform  a  credit  rating  analysis  of  corporations  using  the  features  related  to\\n\\ncreditworthiness.\\n\\nExercises \\n\\n| \\n\\n191\\n\\n\\x0c\\x0cPART III\\nUnsupervised Learning\\n\\n\\x0c\\x0cCHAPTER 7\\nUnsupervised Learning:\\nDimensionality Reduction\\n\\nIn previous chapters, we used supervised learning techniques to build machine learn‐\\ning models using data where the answer was already known (i.e., the class labels were\\navailable  in  our  input  data).  Now  we  will  explore  unsupervised  learning,  where  we\\ndraw inferences from datasets consisting of input data when the answer is unknown.\\nUnsupervised  learning  algorithms  attempt  to  infer  patterns  from  the  data  without\\nany  knowledge  of  the  output  the  data  is  meant  to  yield.  Without  requiring  labeled\\ndata, which can be time-consuming and impractical to create or acquire, this family\\nof models allows for easy use of larger datasets for analysis and model development.\\n\\nDimensionality  reduction  is  a  key  technique  within  unsupervised  learning.  It  com‐\\npresses the data by finding a smaller, different set of variables that capture what mat‐\\nters  most  in  the  original  features,  while  minimizing  the  loss  of  information.\\nDimensionality reduction helps mitigate problems associated with high dimensional‐\\nity and permits the visualization of salient aspects of higher-dimensional data that is\\notherwise difficult to explore.\\n\\nIn  the  context  of  finance,  where  datasets  are  often  large  and  contain  many  dimen‐\\nsions,  dimensionality  reduction  techniques  prove  to  be  quite  practical  and  useful.\\nDimensionality reduction enables us to reduce noise and redundancy in the dataset\\nand find an approximate version of the dataset using fewer features. With fewer vari‐\\nables to consider, exploration and visualization of a dataset becomes more straight‐\\nforward.  Dimensionality  reduction  techniques  also  enhance  supervised  learning–\\nbased models by reducing the number of features or by finding new ones. Practition‐\\ners  use  these  dimensionality  reduction  techniques  to  allocate  funds  across  asset\\nclasses and individual investments, identify trading strategies and signals, implement\\nportfolio hedging and risk management, and develop instrument pricing models.\\n\\n195\\n\\n\\x0cIn this chapter, we will discuss fundamental dimensionality reduction techniques and\\nwalk  through  three  case  studies  in  the  areas  of  portfolio  management,  interest  rate\\nmodeling,  and  trading  strategy  development.  The  case  studies  are  designed  to  not\\nonly  cover  diverse  topics  from  a  finance  standpoint  but  also  highlight  multiple\\nmachine  learning  and  data  science  concepts.  The  standardized  template  containing\\nthe detailed implementation of modeling steps in Python and machine learning and\\nfinance concepts can be used as a blueprint for any other dimensionality reduction–\\nbased problem in finance.\\n\\nIn “Case Study 1: Portfolio Management: Finding an Eigen Portfolio” on page 202, we\\nuse a dimensionality reduction algorithm to allocate capital into different asset classes\\nto  maximize  risk-adjusted  returns.  We  also  introduce  a  backtesting  framework  to\\nassess the performance of the portfolio we constructed.\\n\\nIn “Case Study 2: Yield Curve Construction and Interest Rate Modeling” on page 217,\\nwe use dimensionality reduction techniques to generate the typical movements of a\\nyield curve. This will illustrate how dimensionality reduction techniques can be used\\nfor  reducing  the  dimension  of  market  variables  across  a  number  of  asset  classes  to\\npromote faster and effective portfolio management, trading, hedging, and risk man‐\\nagement.\\n\\nIn “Case Study 3: Bitcoin Trading: Enhancing Speed and Accuracy” on page 227, we\\nuse  dimensionality  reduction  techniques  for  algorithmic  trading.  This  case  study\\ndemonstrates data exploration in low dimension.\\n\\nIn  addition  to  the  points  mentioned  above,  readers  will  understand  the  following\\npoints by the end of this chapter:\\n\\n• Basic concepts of models and techniques used for dimensionality reduction and\\n\\nhow to implement them in Python.\\n\\n• Concepts  of  eigenvalues  and  eigenvectors  of  Principal  Component  Analysis\\n(PCA),  selecting  the  right  number  of  principal  components,  and  extracting  the\\nfactor weights of principal components.\\n\\n• Usage of dimensionality reduction techniques such as singular value decomposi‐\\ntion  (SVD)  and  t-SNE  to  summarize  high-dimensional  data  for  effective  data\\nexploration and visualization.\\n\\n• How to reconstruct the original data using the reduced principal components.\\n\\n• How to enhance the speed and accuracy of supervised learning algorithms using\\n\\ndimensionality reduction.\\n\\n196 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0c• A backtesting framework for the portfolio performance computing and analyz‐\\ning  portfolio  performance  metrics  such  as  the  Sharpe  ratio  and  the  annualized\\nreturn of the portfolio.\\n\\nThis Chapter’s Code Repository\\n\\nA  Python-based  master  template  for  dimensionality  reduction,\\nalong  with  the  Jupyter  notebook  for  all  the  case  studies  in  this\\nchapter,  is  included  in  the  folder  Chapter  7  -  Unsup.  Learning  -\\nDimensionality Reduction in the code repository for this book. To\\nwork  through  any  dimensionality  reduction–modeling  machine\\nlearning  problems  in  Python  involving  the  dimensionality  reduc‐\\ntion models (such as PCA, SVD, Kernel PCA, or t-SNE) presented\\nin  this  chapter,  readers  need  to  modify  the  template  slightly  to\\nalign with their problem statement. All the case studies presented\\nin  this  chapter  use  the  standard  Python  master  template  with  the\\nstandardized model development steps presented in Chapter 3. For\\nthe dimensionality reduction case studies, steps 6 (i.e., model tun‐\\ning)  and  7  (i.e.,  finalizing  the  model)  are  relatively  lighter  com‐\\npared to the supervised learning models, so these steps have been\\nmerged  with  step  5.  For  situations  in  which  steps  are  irrelevant,\\nthey have been skipped or combined with others to make the flow\\nof the case study more intuitive.\\n\\nDimensionality Reduction Techniques\\nDimensionality  reduction  represents  the  information  in  a  given  dataset  more  effi‐\\nciently  by  using  fewer  features.  These  techniques  project  data  onto  a  lower  dimen‐\\nsional  space  by  either  discarding  variation  in  the  data  that  is  not  informative  or\\nidentifying a lower dimensional subspace on or near where the data resides.\\n\\nThere are many types of dimensionality reduction techniques. In this chapter, we will\\ncover these most frequently used techniques for dimensionality reduction:\\n\\n• Principal component analysis (PCA)\\n\\n• Kernel principal component analysis (KPCA)\\n\\n• t-distributed stochastic neighbor embedding (t-SNE)\\n\\nAfter  application  of  these  dimensionality  reduction  techniques,  the  low-dimension\\nfeature  subspace  can  be  a  linear  or  nonlinear  function  of  the  corresponding  high-\\ndimensional  feature  subspace.  Hence,  on  a  broad  level  these  dimensionality  reduc‐\\ntion algorithms can be classified as linear and nonlinear. Linear algorithms, such as\\nPCA, force the new variables to be linear combinations of the original features.\\n\\nDimensionality Reduction Techniques \\n\\n| \\n\\n197\\n\\n\\x0cNonlinear algorithms such KPCA and t-SNE can capture more complex structures in\\nthe data. However, given the infinite number of options, the algorithms still need to\\nmake assumptions to arrive at a solution.\\n\\nPrincipal Component Analysis\\nThe idea of principal component analysis (PCA) is to reduce the dimensionality of a\\ndataset with a large number of variables, while retaining as much variance in the data\\nas possible. PCA allows us to understand whether there is a different representation\\nof the data that can explain a majority of the original data points.\\n\\nPCA finds a set of new variables that, through a linear combination, yield the original\\nvariables.  The  new  variables  are  called  principal  components  (PCs).  These  principal\\ncomponents  are  orthogonal  (or  independent)  and  can  represent  the  original  data.\\nThe number of components is a hyperparameter of the PCA algorithm that sets the\\ntarget dimensionality.\\n\\nThe PCA algorithm works by projecting the original data onto the principal compo‐\\nnent  space.  It  then  identifies  a  sequence  of  principal  components,  each  of  which\\naligns with the direction of maximum variance in the data (after accounting for var‐\\niation  captured  by  previously  computed  components).  The  sequential  optimization\\nalso ensures that new components are not correlated with existing components. Thus\\nthe resulting set constitutes an orthogonal basis for a vector space.\\n\\nThe decline in the amount of variance of the original data explained by each principal\\ncomponent reflects the extent of correlation among the original features. The number\\nof components that capture, for example, 95% of the original variation relative to the\\ntotal number of features provides an insight into the linearly independent informa‐\\ntion of the original data. In order to understand how PCA works, let’s consider the\\ndistribution of data shown in Figure 7-1.\\n\\nFigure 7-1. PCA-1\\n\\nPCA finds a new quadrant system (y’ and x’ axes) that is obtained from the original\\nthrough  translation  and  rotation.  It  will  move  the  center  of  the  coordinate  system\\nfrom the original point (0, 0) to the center of the distribution of data points. It will\\nthen  move  the  x-axis  into  the  principal  axis  of  variation,  which  is  the  one  with  the\\n\\n198 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cmost variation relative to data points (i.e., the direction of maximum spread). Then it\\nmoves the other axis orthogonally to the principal one, into a less important direction\\nof variation.\\n\\nFigure 7-2 shows an example of PCA in which two dimensions explain nearly all the\\nvariance of the underlying data.\\n\\nFigure 7-2. PCA-2\\n\\nThese new directions that contain the maximum variance are called principal compo‐\\nnents and are orthogonal to each other by design.\\n\\nThere are two approaches to finding the principal components: Eigen decomposition\\nand singular value decomposition (SVD).\\n\\nEigen decomposition\\n\\nThe steps of Eigen decomposition are as follows:\\n\\n1. First, a covariance matrix is created for the features.\\n\\n2. Once  the  covariance  matrix  is  computed,  the  eigenvectors  of  the  covariance\\n\\nmatrix are calculated.1 These are the directions of maximum variance.\\n\\n3. The  eigenvalues  are  then  created.  They  define  the  magnitude  of  the  principal\\n\\ncomponents.\\n\\nSo,  for  n  dimensions,  there  will  be  an  n  ×  n  variance-covariance  matrix,  and  as  a\\nresult, we will have an eigenvector of n values and n eigenvalues.\\n\\nlibrary  offers  a  powerful \\n\\nPython’s  sklearn \\nimplementation  of  PCA.  The\\nsklearn.decomposition.PCA  function  computes  the  desired  number  of  principal\\ncomponents  and  projects  the  data  into  the  component  space.  The  following  code\\nsnippet illustrates how to create two principal components from a dataset.\\n\\n1 Eigenvectors and eigenvalues are concepts of linear algebra.\\n\\nDimensionality Reduction Techniques \\n\\n| \\n\\n199\\n\\n\\x0cImplementation\\n\\n# Import PCA Algorithm\\nfrom sklearn.decomposition import PCA\\n# Initialize the algorithm and set the number of PC\\'s\\npca = PCA(n_components=2)\\n# Fit the model to data\\npca.fit(data)\\n# Get list of PC\\'s\\npca.components_\\n# Transform the model to data\\npca.transform(data)\\n# Get the eigenvalues\\npca.explained_variance_ratio\\n\\nThere  are  additional  items,  such  as  factor  loading,  that  can  be  obtained  using  the\\nfunctions in the sklearn library. Their use will be demonstrated in the case studies.\\n\\nSingular value decomposition\\n\\nSingular value decomposition (SVD) is factorization of a matrix into three matrices\\nand is applicable to a more general case of m × n rectangular matrices.\\n\\nIf A is an m × n matrix, then SVD can express the matrix as:\\n\\nA = UΣV T\\n\\nwhere  A  is  an  m  ×  n  matrix,  U  is  an  (m  ×  m)  orthogonal  matrix,  Σ  is  an  (m  ×  n)\\nnonnegative rectangular diagonal matrix, and V is an (n × n) orthogonal matrix. SVD\\nof a given matrix tells us exactly how we can decompose the matrix. Σ is a diagonal\\nmatrix with m diagonal values called singular values. Their magnitude indicates how\\nsignificant they are to preserving the information of the original data. V contains the\\nprincipal components as column vectors.\\n\\nAs shown above, both Eigen decomposition and SVD tell us that using PCA is effec‐\\ntively looking at the initial data from a different angle. Both will always give the same\\nanswer; however, SVD can be much more efficient than Eigen decomposition, as it is\\nable to handle sparse matrices (those which contain very few nonzero elements). In\\naddition, SVD yields better numerical stability, especially when some of the features\\nare strongly correlated.\\n\\nTruncated  SVD  is  a  variant  of  SVD  that  computes  only  the  largest  singular  values,\\nwhere the number of computes is a user-specified parameter. This method is differ‐\\nent  from  regular  SVD  in  that  it  produces  a  factorization  where  the  number  of  col‐\\numns is equal to the specified truncation. For example, given an n × n matrix, SVD\\nwill produce matrices with n columns, whereas truncated SVD will produce matrices\\nwith a specified number of columns that may be less than n.\\n\\n200 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cImplementation\\n\\nfrom sklearn.decomposition import TruncatedSVD\\nsvd = TruncatedSVD(ncomps=20).fit(X)\\n\\nIn  terms  of  the  weaknesses  of  the  PCA  technique,  although  it  is  very  effective  in\\nreducing the number of dimensions, the resulting principal components may be less\\ninterpretable than the original features. Additionally, the results may be sensitive to\\nthe selected number of principal components. For example, too few principal compo‐\\nnents may miss some information compared to the original list of features. Also, PCA\\nmay not work well if the data is strongly nonlinear.\\n\\nKernel Principal Component Analysis\\nA main limitation of PCA is that it only applies linear transformations. Kernel princi‐\\npal  component  analysis  (KPCA)  extends  PCA  to  handle  nonlinearity.  It  first  maps\\nthe original data to some nonlinear feature space (usually one of higher dimension).\\nThen it applies PCA to extract the principal components in that space.\\n\\nA simple example of when KPCA is applicable is shown in Figure 7-3. Linear trans‐\\nformations  are  suitable  for  the  blue  and  red  data  points  on  the  left-hand  plot.\\nHowever, if all dots are arranged as per the graph on the right, the result is not line‐\\narly separable. We would then need to apply KPCA to separate the components.\\n\\nFigure 7-3. Kernel PCA\\n\\nImplementation\\n\\nfrom sklearn.decomposition import KernelPCA\\nkpca = KernelPCA(n_components=4, kernel=\\'rbf\\').fit_transform(X)\\n\\nIn the Python code, we specify kernel=\\'rbf\\', which is the radial basis function ker‐\\nnel.  This  is  commonly  used  as  a  kernel  in  machine  learning  techniques,  such  as  in\\nSVMs (see Chapter 4).\\n\\nUsing  KPCA,  component  separation  becomes  easier  in  a  higher  dimensional  space,\\nas  mapping  into  a  higher  dimensional  space  often  provides  greater  classification\\npower.\\n\\nDimensionality Reduction Techniques \\n\\n| \\n\\n201\\n\\n\\x0ct-distributed Stochastic Neighbor Embedding\\nt-distributed  stochastic  neighbor  embedding  (t-SNE)  is  a  dimensionality  reduction\\nalgorithm  that  reduces  the  dimensions  by  modeling  the  probability  distribution  of\\nneighbors around each point. Here, the term neighbors refers to the set of points clos‐\\nest to a given point. The algorithm emphasizes keeping similar points together in low\\ndimensions as opposed to maintaining the distance between points that are apart in\\nhigh dimensions.\\n\\nThe algorithm starts by calculating the probability of similarity of data points in cor‐\\nresponding high and low dimensional space. The similarity of points is calculated as\\nthe  conditional  probability  that  a  point  A  would  choose  point  B  as  its  neighbor  if\\nneighbors were picked in proportion to their probability density under a normal dis‐\\ntribution centered at A. The algorithm then tries to minimize the difference between\\nthese  conditional  probabilities  (or  similarities)  in  the  high  and  low  dimensional\\nspaces for a perfect representation of data points in the low dimensional space.\\n\\nImplementation\\n\\nfrom sklearn.manifold import TSNE\\nX_tsne = TSNE().fit_transform(X)\\n\\nAn  implementation  of  t-SNE  is  shown  in  the  third  case  study  presented  in  this\\nchapter.\\n\\nCase Study 1: Portfolio Management: Finding an Eigen\\nPortfolio\\nA primary objective of portfolio management is to allocate capital into different asset\\nclasses  to  maximize  risk-adjusted  returns.  Mean-variance  portfolio  optimization  is\\nthe  most  commonly  used  technique  for  asset  allocation.  This  method  requires  an\\nestimated covariance matrix and expected returns of the assets considered. However,\\nthe erratic nature of financial returns leads to estimation errors in these inputs, espe‐\\ncially  when  the  sample  size  of  returns  is  insufficient  compared  to  the  number  of\\nassets being allocated. These errors greatly jeopardize the optimality of the resulting\\nportfolios, leading to poor and unstable outcomes.\\n\\nDimensionality reduction is a technique we can use to address this issue. Using PCA,\\nwe  can  take  an  n  ×  n  covariance  matrix  of  our  assets  and  create  a  set  of  n  linearly\\nuncorrelated principal portfolios (sometimes referred to in literature as an eigen port‐\\nfolio) made up of our assets and their corresponding variances. The principal compo‐\\nnents of the covariance matrix capture most of the covariation among the assets and\\nare mutually uncorrelated. Moreover, we can use standardized principal components\\nas  the  portfolio  weights,  with  the  statistical  guarantee  that  the  returns  from  these\\nprincipal portfolios are linearly uncorrelated.\\n\\n202 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cBy the end of this case study, readers will be familiar with a general approach to find‐\\ning  an  eigen  portfolio  for  asset  allocation,  from  understanding  concepts  of  PCA  to\\nbacktesting different principal components.\\n\\nThis case study will focus on:\\n\\n• Understanding  eigenvalues  and  eigenvectors  of  PCA  and  deriving  portfolio\\n\\nweights using the principal components.\\n\\n• Developing a backtesting framework to evaluate portfolio performance.\\n\\n• Understanding how to work through a dimensionality reduction modeling prob‐\\n\\nlem from end to end.\\n\\nBlueprint for Using Dimensionality Reduction for Asset\\nAllocation\\n\\n1. Problem definition\\n\\nOur goal in this case study is to maximize the risk-adjusted returns of an equity port‐\\nfolio using PCA on a dataset of stock returns.\\n\\nThe dataset used for this case study is the Dow Jones Industrial Average (DJIA) index\\nand its respective 30 stocks. The return data used will be from the year 2000 onwards\\nand can be downloaded from Yahoo Finance.\\n\\nWe will also compare the performance of our hypothetical portfolios against a bench‐\\nmark and backtest the model to evaluate the effectiveness of the approach.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  list  of  the  libraries  used  for  data  loading,  data\\nanalysis, data preparation, model evaluation, and model tuning are shown below. The\\ndetails of most of these packages and functions can be found in Chapters 2 and 4.\\n\\nPackages for dimensionality reduction\\n\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.decomposition import TruncatedSVD\\nfrom numpy.linalg import inv, eig, svd\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.decomposition import KernelPCA\\n\\nCase Study 1: Portfolio Management: Finding an Eigen Portfolio \\n\\n| \\n\\n203\\n\\n\\x0cPackages for data processing and visualization\\n\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom pandas import read_csv, set_option\\nfrom pandas.plotting import scatter_matrix\\nimport seaborn as sns\\nfrom sklearn.preprocessing import StandardScaler\\n\\n2.2. Loading the data.    We import the dataframe containing the adjusted closing prices\\nfor all the companies in the DJIA index:\\n\\n# load dataset\\ndataset = read_csv(\\'Dow_adjcloses.csv\\', index_col=0)\\n\\n3. Exploratory data analysis\\n\\nNext, we inspect the dataset.\\n\\n3.1. Descriptive statistics.    Let’s look at the shape of the data:\\n\\ndataset.shape\\n\\nOutput\\n\\n(4804, 30)\\n\\nThe  data  is  comprised  of  30  columns  and  4,804  rows  containing  the  daily  closing\\nprices of the 30 stocks in the index since 2000.\\n\\n3.2. Data visualization.    The first thing we must do is gather a basic sense of our data.\\nLet us take a look at the return correlations:\\n\\ncorrelation = dataset.corr()\\nplt.figure(figsize=(15, 15))\\nplt.title(\\'Correlation Matrix\\')\\nsns.heatmap(correlation, vmax=1, square=True,annot=True, cmap=\\'cubehelix\\')\\n\\nThere is a significant positive correlation between the daily returns. The plot (full-size\\nversion  available  on  GitHub)  also  indicates  that  the  information  embedded  in  the\\ndata  may  be  represented  by  fewer  variables  (i.e.,  something  smaller  than  the  30\\ndimensions  we  have  now).  We  will  perform  another  detailed  look  at  the  data  after\\nimplementing dimensionality reduction.\\n\\n204 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cOutput\\n\\n4. Data preparation\\n\\nWe prepare the data for modeling in the following sections.\\n\\n4.1. Data cleaning.     First,  we  check  for  NAs  in  the  rows  and  either  drop  them  or  fill\\nthem with the mean of the column:\\n\\n#Checking for any null values and removing the null values\\'\\'\\'\\nprint(\\'Null Values =\\',dataset.isnull().values.any())\\n\\nOutput\\n\\nNull Values = True\\n\\nCase Study 1: Portfolio Management: Finding an Eigen Portfolio \\n\\n| \\n\\n205\\n\\n\\x0cSome stocks were added to the index after our start date. To ensure proper analysis,\\nwe will drop those with more than 30% missing values. Two stocks fit this criteria—\\nDow Chemicals and Visa:\\n\\nmissing_fractions = dataset.isnull().mean().sort_values(ascending=False)\\nmissing_fractions.head(10)\\ndrop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\\ndataset.drop(labels=drop_list, axis=1, inplace=True)\\ndataset.shape\\n\\nOutput\\n\\n(4804, 28)\\n\\nWe  end  up  with  return  data  for  28  companies  and  an  additional  one  for  the  DJIA\\nindex. Now we fill the NAs with the mean of the columns:\\n\\n# Fill the missing values with the last value available in the dataset.\\ndataset=dataset.fillna(method=\\'ffill\\')\\n\\n4.2. Data transformation.    In addition to handling the missing values, we also want to\\nstandardize the dataset features onto a unit scale (mean = 0 and variance = 1). All the\\nvariables should be on the same scale before applying PCA; otherwise, a feature with\\nlarge values will dominate the result. We use StandardScaler in sklearn to standard‐\\nize the dataset, as shown below:\\n\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler().fit(datareturns)\\nrescaledDataset = pd.DataFrame(scaler.fit_transform(datareturns),columns =\\\\\\n datareturns.columns, index = datareturns.index)\\n# summarize transformed data\\ndatareturns.dropna(how=\\'any\\', inplace=True)\\nrescaledDataset.dropna(how=\\'any\\', inplace=True)\\n\\nOverall, cleaning and standardizing the data is important in order to create a mean‐\\ningful and reliable dataset to be used in dimensionality reduction without error.\\n\\nLet  us  look  at  the  returns  of  one  of  the  stocks  from  the  cleaned  and  standardized\\ndataset:\\n\\n# Visualizing Log Returns for the DJIA\\nplt.figure(figsize=(16, 5))\\nplt.title(\"AAPL Return\")\\nrescaledDataset.AAPL.plot()\\nplt.grid(True);\\nplt.legend()\\nplt.show()\\n\\n206 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cOutput\\n\\n5. Evaluate algorithms and models\\n\\n5.1. Train-test split.    The portfolio is divided into training and test sets to perform the\\nanalysis regarding the best portfolio and to perform backtesting:\\n\\n# Dividing the dataset into training and testing sets\\npercentage = int(len(rescaledDataset) * 0.8)\\nX_train = rescaledDataset[:percentage]\\nX_test = rescaledDataset[percentage:]\\n\\nstock_tickers = rescaledDataset.columns.values\\nn_tickers = len(stock_tickers)\\n\\n5.2. Model evaluation: applying principal component analysis.    As the next step, we create a\\nfunction to perform PCA using the sklearn library. This function generates the prin‐\\ncipal components from the data that will be used for further analysis:\\n\\npca = PCA()\\nPrincipalComponent=pca.fit(X_train)\\n\\n5.2.1. Explained variance using PCA.    In this step, we look at the variance explained using\\nPCA.  The  decline  in  the  amount  of  variance  of  the  original  data  explained  by  each\\nprincipal  component  reflects  the  extent  of  correlation  among  the  original  features.\\nThe  first  principal  component  captures  the  most  variance  in  the  original  data,  the\\nsecond component is a representation of the second highest variance, and so on. The\\neigenvectors with the lowest eigenvalues describe the least amount of variation within\\nthe dataset. Therefore, these values can be dropped.\\n\\nThe  following  charts  show  the  number  of  principal  components  and  the  variance\\nexplained by each.\\n\\nNumEigenvalues=20\\nfig, axes = plt.subplots(ncols=2, figsize=(14,4))\\nSeries1 = pd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).sort_values()\\n\\nCase Study 1: Portfolio Management: Finding an Eigen Portfolio \\n\\n| \\n\\n207\\n\\n\\x0cSeries2 = pd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).cumsum()\\nSeries1.plot.barh(title=\\'Explained Variance Ratio by Top Factors\\', ax=axes[0]);\\nSeries1.plot(ylim=(0,1), ax=axes[1], title=\\'Cumulative Explained Variance\\');\\n\\nOutput\\n\\nWe find that the most important factor explains around 40% of the daily return var‐\\niation. This dominant principal component is usually interpreted as the “market” fac‐\\ntor. We will discuss the interpretation of this and the other factors when looking at\\nthe portfolio weights.\\n\\nThe  plot  on  the  right  shows  the  cumulative  explained  variance  and  indicates  that\\naround ten factors explain 73% of the variance in returns of the 28 stocks analyzed.\\n\\n5.2.2. Looking at portfolio weights.    In this step, we look more closely at the individual\\nprincipal  components.  These  may  be  less  interpretable  than  the  original  features.\\nHowever, we can look at the weights of the factors on each principal component to\\nassess  any  intuitive  themes  relative  to  the  28  stocks.  We  construct  five  portfolios,\\ndefining the weights of each stock as each of the first five principal components. We\\nthen create a scatterplot that visualizes an organized descending plot with the respec‐\\ntive weight of every company at the current chosen principal component:\\n\\ndef PCWeights():\\n    #Principal Components (PC) weights for each 28 PCs\\n\\n    weights = pd.DataFrame()\\n    for i in range(len(pca.components_)):\\n        weights[\"weights_{}\".format(i)] = \\\\\\n        pca.components_[i] / sum(pca.components_[i])\\n    weights = weights.values.T\\n    return weights\\nweights=PCWeights()\\n\\nsum(pca.components_[0])\\n\\nOutput\\n\\n-5.247808242068631\\n\\n208 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cNumComponents=5\\ntopPortfolios = pd.DataFrame(pca.components_[:NumComponents],\\\\\\n   columns=dataset.columns)\\neigen_portfolios = topPortfolios.div(topPortfolios.sum(1), axis=0)\\neigen_portfolios.index = [f\\'Portfolio {i}\\' for i in range( NumComponents)]\\nnp.sqrt(pca.explained_variance_)\\neigen_portfolios.T.plot.bar(subplots=True, layout=(int(NumComponents),1),  \\\\\\nfigsize=(14,10), legend=False, sharey=True, ylim= (-1,1))\\n\\nGiven that scale for the plots are the same, we can also look at the heatmap as follows:\\n\\nOutput\\n\\n# plotting heatmap\\nsns.heatmap(topPortfolios)\\n\\nCase Study 1: Portfolio Management: Finding an Eigen Portfolio \\n\\n| \\n\\n209\\n\\n\\x0cOutput\\n\\nThe  heatmap  and  barplots  show  the  contribution  of  different  stocks  in  each\\neigenvector.\\n\\nTraditionally, the intuition behind each principal portfolio is that it represents some\\nsort  of  independent  risk  factor.  The  manifestation  of  those  risk  factors  depends  on\\nthe assets in the portfolio. In our case study, the assets are all U.S. domestic equities.\\nThe  principal  portfolio  with  the  largest  variance  is  typically  a  systematic  risk  factor\\n(i.e., “market” factor). Looking at the first principal component (Portfolio 0), we see\\nthat the weights are distributed homogeneously across the stocks. This nearly equal\\nweighted portfolio explains 40% of the variance in the index and is a fair representa‐\\ntion of a systematic risk factor.\\n\\nThe rest of the eigen portfolios typically correspond to sector or industry factors. For\\nexample, Portfolio 1 assigns a high weight to JNJ and MRK, which are stocks from the\\nhealth care sector. Similarly, Portfolio 3 has high weights on technology and electron‐\\nics companies, such AAPL, MSFT, and IBM.\\n\\nWhen the asset universe for our portfolio is expanded to include broad, global invest‐\\nments,  we  may  identify  factors  for  international  equity  risk,  interest  rate  risk,  com‐\\nmodity exposure, geographic risk, and many others.\\n\\nIn the next step, we find the best eigen portfolio.\\n\\n5.2.3. Finding the best eigen portfolio.    To determine the best eigen portfolio, we use the\\nSharpe  ratio.  This  is  an  assessment  of  risk-adjusted  performance  that  explains  the\\nannualized returns against the annualized volatility of a portfolio. A high Sharpe ratio\\nexplains higher returns and/or lower volatility for the specified portfolio. The annual‐\\nized Sharpe ratio is computed by dividing the annualized returns against the annual‐\\nized volatility. For annualized return we apply the geometric average of all the returns\\n\\n210 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cin  respect  to  the  periods  per  year  (days  of  operations  in  the  exchange  in  a  year).\\nAnnualized volatility is computed by taking the standard deviation of the returns and\\nmultiplying it by the square root of the periods per year.\\n\\nThe following code computes the Sharpe ratio of a portfolio:\\n\\n# Sharpe Ratio Calculation\\n# Calculation based on conventional number of trading days per year (i.e., 252).\\ndef sharpe_ratio(ts_returns, periods_per_year=252):\\n    n_years = ts_returns.shape[0]/ periods_per_year\\n    annualized_return = np.power(np.prod(1+ts_returns), (1/n_years))-1\\n    annualized_vol = ts_returns.std() * np.sqrt(periods_per_year)\\n    annualized_sharpe = annualized_return / annualized_vol\\n\\n    return annualized_return, annualized_vol, annualized_sharpe\\n\\nWe  construct  a  loop  to  compute  the  principal  component  weights  for  each  eigen\\nportfolio.  Then  it  uses  the  Sharpe  ratio  function  to  look  for  the  portfolio  with  the\\nhighest Sharpe ratio. Once we know which portfolio has the highest Sharpe ratio, we\\ncan visualize its performance against the index for comparison:\\n\\ndef optimizedPortfolio():\\n    n_portfolios = len(pca.components_)\\n    annualized_ret = np.array([0.] * n_portfolios)\\n    sharpe_metric = np.array([0.] * n_portfolios)\\n    annualized_vol = np.array([0.] * n_portfolios)\\n    highest_sharpe = 0\\n    stock_tickers = rescaledDataset.columns.values\\n    n_tickers = len(stock_tickers)\\n    pcs = pca.components_\\n\\n    for i in range(n_portfolios):\\n\\n        pc_w = pcs[i] / sum(pcs[i])\\n        eigen_prtfi = pd.DataFrame(data ={\\'weights\\': pc_w.squeeze()*100}, \\\\\\n        index = stock_tickers)\\n        eigen_prtfi.sort_values(by=[\\'weights\\'], ascending=False, inplace=True)\\n        eigen_prti_returns = np.dot(X_train_raw.loc[:, eigen_prtfi.index], pc_w)\\n        eigen_prti_returns = pd.Series(eigen_prti_returns.squeeze(),\\\\\\n         index=X_train_raw.index)\\n        er, vol, sharpe = sharpe_ratio(eigen_prti_returns)\\n        annualized_ret[i] = er\\n        annualized_vol[i] = vol\\n        sharpe_metric[i] = sharpe\\n\\n        sharpe_metric= np.nan_to_num(sharpe_metric)\\n\\n    # find portfolio with the highest Sharpe ratio\\n    highest_sharpe = np.argmax(sharpe_metric)\\n\\n    print(\\'Eigen portfolio #%d with the highest Sharpe. Return %.2f%%,\\\\\\n     vol = %.2f%%, Sharpe = %.2f\\' %\\n\\nCase Study 1: Portfolio Management: Finding an Eigen Portfolio \\n\\n| \\n\\n211\\n\\n\\x0c          (highest_sharpe,\\n           annualized_ret[highest_sharpe]*100,\\n           annualized_vol[highest_sharpe]*100,\\n           sharpe_metric[highest_sharpe]))\\n\\n    fig, ax = plt.subplots()\\n    fig.set_size_inches(12, 4)\\n    ax.plot(sharpe_metric, linewidth=3)\\n    ax.set_title(\\'Sharpe ratio of eigen-portfolios\\')\\n    ax.set_ylabel(\\'Sharpe ratio\\')\\n    ax.set_xlabel(\\'Portfolios\\')\\n\\n    results = pd.DataFrame(data={\\'Return\\': annualized_ret,\\\\\\n    \\'Vol\\': annualized_vol,\\n    \\'Sharpe\\': sharpe_metric})\\n    results.dropna(inplace=True)\\n    results.sort_values(by=[\\'Sharpe\\'], ascending=False, inplace=True)\\n    print(results.head(5))\\n\\n    plt.show()\\n\\noptimizedPortfolio()\\n\\nOutput\\n\\nEigen portfolio #0 with the highest Sharpe. Return 11.47%, vol = 13.31%, \\\\\\nSharpe = 0.86\\n    Return    Vol  Sharpe\\n0    0.115  0.133   0.862\\n7    0.096  0.693   0.138\\n5    0.100  0.845   0.118\\n1    0.057  0.670   0.084\\n\\nAs shown by the results above, Portfolio 0 is the best performing one, with the highest\\nreturn and the lowest volatility. Let us look at the composition of this portfolio:\\n\\nweights = PCWeights()\\nportfolio = portfolio = pd.DataFrame()\\n\\n212 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cdef plotEigen(weights, plot=False, portfolio=portfolio):\\n    portfolio = pd.DataFrame(data ={\\'weights\\': weights.squeeze() * 100}, \\\\\\n    index = stock_tickers)\\n    portfolio.sort_values(by=[\\'weights\\'], ascending=False, inplace=True)\\n    if plot:\\n        portfolio.plot(title=\\'Current Eigen-Portfolio Weights\\',\\n            figsize=(12, 6),\\n            xticks=range(0, len(stock_tickers), 1),\\n            rot=45,\\n            linewidth=3\\n            )\\n        plt.show()\\n\\n    return portfolio\\n\\n# Weights are stored in arrays, where 0 is the first PC\\'s weights.\\nplotEigen(weights=weights[0], plot=True)\\n\\nOutput\\n\\nRecall that this is the portfolio that explains 40% of the variance and represents the\\nsystematic risk factor. Looking at the portfolio weights (in percentages in the y-axis),\\nthey do not vary much and are in the range of 2.7% to 4.5% across all stocks. How‐\\never, the weights seem to be higher in the financial sector, and stocks such as AXP,\\nJPM, and GS have higher-than-average weights.\\n\\nCase Study 1: Portfolio Management: Finding an Eigen Portfolio \\n\\n| \\n\\n213\\n\\n\\x0c5.2.4. Backtesting the eigen portfolios.     We  will  now  try  to  backtest  this  algorithm  on\\nthe test set. We will look at a few of the top performers and the worst performer. For\\nthe top performers we look at the 3rd- and 4th-ranked eigen portfolios (Portfolios 5\\nand 1), while the worst performer reviewed was ranked 19th (Portfolio 14):\\n\\ndef Backtest(eigen):\\n\\n    \\'\\'\\'\\n    Plots principal components returns against real returns.\\n    \\'\\'\\'\\n\\n    eigen_prtfi = pd.DataFrame(data ={\\'weights\\': eigen.squeeze()}, \\\\\\n    index=stock_tickers)\\n    eigen_prtfi.sort_values(by=[\\'weights\\'], ascending=False, inplace=True)\\n\\n    eigen_prti_returns = np.dot(X_test_raw.loc[:, eigen_prtfi.index], eigen)\\n    eigen_portfolio_returns = pd.Series(eigen_prti_returns.squeeze(),\\\\\\n     index=X_test_raw.index)\\n    returns, vol, sharpe = sharpe_ratio(eigen_portfolio_returns)\\n    print(\\'Current Eigen-Portfolio:\\\\nReturn = %.2f%%\\\\nVolatility = %.2f%%\\\\n\\\\\\n    Sharpe = %.2f\\' % (returns * 100, vol * 100, sharpe))\\n    equal_weight_return=(X_test_raw * (1/len(pca.components_))).sum(axis=1)\\n    df_plot = pd.DataFrame({\\'EigenPorfolio Return\\': eigen_portfolio_returns, \\\\\\n    \\'Equal Weight Index\\': equal_weight_return}, index=X_test.index)\\n    np.cumprod(df_plot + 1).plot(title=\\'Returns of the equal weighted\\\\\\n     index vs. First eigen-portfolio\\',\\n                          figsize=(12, 6), linewidth=3)\\n    plt.show()\\n\\nBacktest(eigen=weights[5])\\nBacktest(eigen=weights[1])\\nBacktest(eigen=weights[14])\\n\\nOutput\\n\\nCurrent Eigen-Portfolio:\\nReturn = 32.76%\\nVolatility = 68.64%\\nSharpe = 0.48\\n\\n214 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cCurrent Eigen-Portfolio:\\nReturn = 99.80%\\nVolatility = 58.34%\\nSharpe = 1.71\\n\\nCurrent Eigen-Portfolio:\\nReturn = -79.42%\\nVolatility = 185.30%\\nSharpe = -0.43\\n\\nCase Study 1: Portfolio Management: Finding an Eigen Portfolio \\n\\n| \\n\\n215\\n\\n\\x0cAs shown in the preceding charts, the eigen portfolio return of the top portfolios out‐\\nperforms  the  equally  weighted  index.  The  eigen  portfolio  ranked  19th  underper‐\\nformed  the  market  significantly \\nin  the  test  set.  The  outperformance  and\\nunderperformance are attributed to the weights of the stocks or sectors in the eigen\\nportfolio.  We  can  drill  down  further  to  understand  the  individual  drivers  of  each\\nportfolio. For example, Portfolio 1 assigns high weight to several stocks in the health\\ncare  sector,  as  discussed  previously.  This  sector  saw  a  significant  increase  in  2017\\nonwards, which is reflected in the chart for Eigen Portfolio 1.\\n\\nGiven  that  these  eigen  portfolios  are  independent,  they  also  provide  diversification\\nopportunities. As such, we can invest across these uncorrelated eigen portfolios, pro‐\\nviding other potential portfolio management benefits.\\n\\nConclusion\\n\\nIn this case study, we applied dimensionality reduction techniques in the context of\\nportfolio  management,  using  eigenvalues  and  eigenvectors  from  PCA  to  perform\\nasset allocation.\\n\\nWe  demonstrated  that,  while  some  interpretability  is  lost,  the  initution  behind  the\\nresulting  portfolios  can  be  matched  to  risk  factors.  In  this  example,  the  first  eigen\\nportfolio represented a systematic risk factor, while others exhibited sector or indus‐\\ntry concentration.\\n\\nThrough backtesting, we found that the portfolio with the best result on the training\\nset also achieved the strongest performance on the test set. Several of the portfolios\\noutperformed  the  index  based  on  the  Sharpe  ratio,  the  risk-adjusted  performance\\nmetric used in this exercise.\\n\\n216 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cOverall, we found that using PCA and analyzing eigen portfolios can yield a robust\\nmethodology for asset allocation and portfolio management.\\n\\nCase Study 2: Yield Curve Construction and Interest Rate\\nModeling\\nA  number  of  problems  in  portfolio  management,  trading,  and  risk  management\\nrequire a deep understanding and modeling of yield curves.\\n\\nA yield curve represents interest rates, or yields, across a range of maturities, usually\\ndepicted  in  a  line  graph,  as  discussed  in  “Case  Study  4:  Yield  Curve  Prediction”  on\\npage 141 in Chapter 5. Yield curve illustrates the “price of funds” at a given point in\\ntime and, due to the time value of money, often shows interest rates rising as a func‐\\ntion of maturity.\\n\\nResearchers in finance have studied the yield curve and found that shifts or changes\\nin the shape of the yield curve are attributable to a few unobservable factors. Specifi‐\\ncally, empirical studies reveal that more than 99% of the movement of various U.S.\\nTreasury  bond  yields  are  captured  by  three  factors,  which  are  often  referred  to  as\\nlevel, slope, and curvature. The names describe how each influences the yield curve\\nshape in response to a shock. A level shock changes the interest rates of all maturities\\nby  almost  identical  amounts,  inducing  a  parallel  shift  that  changes  the  level  of  the\\nentire curve up or down. A shock to the slope factor changes the difference in short-\\nterm  and  long-term  rates.  For  instance,  when  long-term  rates  increase  by  a  larger\\namount than do short-term rates, it results in a curve that becomes steeper (i.e., visu‐\\nally, the curve becomes more upward sloping). Changes in the short- and long-term\\nrates can also produce a flatter yield curve. The main effects of the shock to the curva‐\\nture  factor  focuses  on  medium-term  interest  rates,  leading  to  hump,  twist,  or  U-\\nshaped characteristics.\\n\\nDimensionality  reduction  breaks  down  the  movement  of  the  yield  curve  into  these\\nthree factors. Reducing the yield curve into fewer components means we can focus on\\na  few  intuitive  dimensions  in  the  yield  curve.  Traders  and  risk  managers  use  this\\ntechnique to condense the curve in risk factors for hedging the interest rate risk. Sim‐\\nilarly,  portfolio  managers  then  have  fewer  dimensions  to  analyze  when  allocating\\nfunds. Interest rate structurers use this technique to model the yield curve and ana‐\\nlyze its shape. Overall, it promotes faster and more effective portfolio management,\\ntrading, hedging, and risk management.\\n\\nIn  this  case  study,  we  use  PCA  to  generate  typical  movements  of  a  yield  curve  and\\nshow  that  the  first  three  principal  components  correspond  to  a  yield  curve’s  level,\\nslope, and curvature, respectively.\\n\\nCase Study 2: Yield Curve Construction and Interest Rate Modeling \\n\\n| \\n\\n217\\n\\n\\x0cThis case study will focus on:\\n\\n• Understanding the intuition behind eigenvectors.\\n\\n• Using  dimensions  resulting  from  dimensionality  reduction  to  reproduce  the\\n\\noriginal data.\\n\\nBlueprint for Using Dimensionality Reduction to Generate a\\nYield Curve\\n\\n1. Problem definition\\n\\nOur goal in this case study is to use dimensionality reduction techniques to generate\\nthe typical movements of a yield curve.\\n\\nThe  data  used  for  this  case  study  is  obtained  from  Quandl,  a  premier  source  for\\nfinancial, economic, and alternative datasets. We use the data of 11 tenors (or maturi‐\\nties), from 1-month to 30-years, of Treasury curves. These are of daily frequency and\\nare available from 1960 onwards.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.    The loading of Python packages is similar to the pre‐\\nvious  dimensionality  reduction  case  study.  Please  refer  to  the  Jupyter  notebook  of\\nthis case study for more details.\\n\\n2.2. Loading the data.     In  the  first  step,  we  load  the  data  of  different  tenors  of  the\\nTreasury curves from Quandl:\\n\\n# In order to use quandl, ApiConfig.api_key will need to be\\n# set to identify you to the quandl API. Please see API\\n# Documentation of quandl for more details\\nquandl.ApiConfig.api_key = \\'API Key\\'\\n\\ntreasury = [\\'FRED/DGS1MO\\',\\'FRED/DGS3MO\\',\\'FRED/DGS6MO\\',\\'FRED/DGS1\\',\\\\\\n\\'FRED/DGS2\\',\\'FRED/DGS3\\',\\'FRED/DGS5\\',\\'FRED/DGS7\\',\\'FRED/DGS10\\',\\\\\\n\\'FRED/DGS20\\',\\'FRED/DGS30\\']\\n\\ntreasury_df = quandl.get(treasury)\\ntreasury_df.columns = [\\'TRESY1mo\\',\\'TRESY3mo\\',\\'TRESY6mo\\',\\'TRESY1y\\',\\\\\\n\\'TRESY2y\\',\\'TRESY3y\\',\\'TRESY5y\\',\\'TRESY7y\\',\\'TRESY10y\\',\\\\\\'TRESY20y\\',\\'TRESY30y\\']\\ndataset = treasury_df\\n\\n218 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0c3. Exploratory data analysis\\n\\nHere, we will take our first look at the data.\\n\\n3.1. Descriptive statistics.    In the next step we look at the shape of the dataset:\\n\\n# shape\\ndataset.shape\\n\\nOutput\\n\\n(14420, 11)\\n\\nThe dataset has 14,420 rows and has the data of 11 tenors of the Treasury curve for\\nmore than 50 years.\\n\\n3.2. Data visualization.    Let us look at the movement of the rates from the downloaded\\ndata:\\n\\ndataset.plot(figsize=(10,5))\\nplt.ylabel(\"Rate\")\\nplt.legend(bbox_to_anchor=(1.01, 0.9), loc=2)\\nplt.show()\\n\\nOutput\\n\\nIn the next step we look at the correlations across tenors:\\n\\n# correlation\\ncorrelation = dataset.corr()\\nplt.figure(figsize=(15, 15))\\nplt.title(\\'Correlation Matrix\\')\\nsns.heatmap(correlation, vmax=1, square=True, annot=True, cmap=\\'cubehelix\\')\\n\\nCase Study 2: Yield Curve Construction and Interest Rate Modeling \\n\\n| \\n\\n219\\n\\n\\x0cOutput\\n\\nThere  is  a  significant  positive  correlation  between  the  tenors,  as  you  can  see  in  the\\noutput (full-size version available on GitHub). This is an indication that reducing the\\nnumber dimensions may be useful when modeling with the data. Additional visuali‐\\nzations of the data will be performed after implementing the dimensionality reduc‐\\ntion models.\\n\\n4. Data preparation\\n\\nData cleaning and transformation are a necessary modeling prerequisite in this case\\nstudy.\\n\\n220 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0c4.1. Data cleaning.     Here,  we  check  for  NAs  in  the  data  and  either  drop  them  or  fill\\nthem with the mean of the column.\\n\\n4.2. Data transformation.    We standardize the variables on the same scale before apply‐\\ning PCA in order to prevent a feature with large values from dominating the result.\\nWe use the StandardScaler function in sklearn to standardize the dataset’s features\\nonto a unit scale (mean = 0 and variance = 1):\\n\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler().fit(dataset)\\nrescaledDataset = pd.DataFrame(scaler.fit_transform(dataset),\\\\\\ncolumns = dataset.columns,\\nindex = dataset.index)\\n# summarize transformed data\\ndataset.dropna(how=\\'any\\', inplace=True)\\nrescaledDataset.dropna(how=\\'any\\', inplace=True)\\n\\nVisualizing the standardized dataset\\n\\nrescaledDataset.plot(figsize=(14, 10))\\nplt.ylabel(\"Rate\")\\nplt.legend(bbox_to_anchor=(1.01, 0.9), loc=2)\\nplt.show()\\n\\nOutput\\n\\nCase Study 2: Yield Curve Construction and Interest Rate Modeling \\n\\n| \\n\\n221\\n\\n\\x0c5. Evaluate algorithms and models\\n\\n5.2. Model evaluation—applying principal component analysis.    As a next step, we create a\\nfunction to perform PCA using the sklearn library. This function generates the prin‐\\ncipal components from the data that will be used for further analysis:\\n\\npca = PCA()\\nPrincipalComponent=pca.fit(rescaledDataset)\\n\\n5.2.1. Explained variance using PCA.   \\n\\nNumEigenvalues=5\\nfig, axes = plt.subplots(ncols=2, figsize=(14, 4))\\npd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).sort_values().\\\\\\nplot.barh(title=\\'Explained Variance Ratio by Top Factors\\',ax=axes[0]);\\npd.Series(pca.explained_variance_ratio_[:NumEigenvalues]).cumsum()\\\\\\n.plot(ylim=(0,1),ax=axes[1], title=\\'Cumulative Explained Variance\\');\\n# explained_variance\\npd.Series(np.cumsum(pca.explained_variance_ratio_)).to_frame\\\\\\n(\\'Explained Variance_Top 5\\').head(NumEigenvalues).style.format(\\'{:,.2%}\\'.format)\\n\\nOutput\\n\\nExplained Variance_Top 5\\n84.36%\\n\\n98.44%\\n\\n99.53%\\n\\n99.83%\\n\\n99.94%\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n222 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cThe  first  three  principal  components  account  for  84.4%,  14.08%,  and  1.09%  of  var‐\\niance,  respectively.  Cumulatively,  they  describe  over  99.5%  of  all  movement  in  the\\ndata.  This  is  an  incredibly  efficient  reduction  in  dimensions.  Recall  that  in  the  first\\ncase study, we saw the first 10 components account for only 73% of variance.\\n\\n5.2.2. Intuition behind the principal components.     Ideally,  we  can  have  some  intuition\\nand  interpretation  of  these  principal  components.  To  explore  this,  we  first  have  a\\nfunction  to  determine  the  weights  of  each  principal  component,  and  then  perform\\nthe visualization of the principal components:\\n\\ndef PCWeights():\\n    \\'\\'\\'\\n    Principal Components (PC) weights for each 28 PCs\\n    \\'\\'\\'\\n    weights = pd.DataFrame()\\n\\n    for i in range(len(pca.components_)):\\n        weights[\"weights_{}\".format(i)] = \\\\\\n        pca.components_[i] / sum(pca.components_[i])\\n\\n    weights = weights.values.T\\n    return weights\\n\\nweights=PCWeights()\\n\\nweights = PCWeights()\\nNumComponents=3\\n\\ntopPortfolios = pd.DataFrame(weights[:NumComponents], columns=dataset.columns)\\ntopPortfolios.index = [f\\'Principal Component {i}\\' \\\\\\nfor i in range(1, NumComponents+1)]\\n\\naxes = topPortfolios.T.plot.bar(subplots=True, legend=False, figsize=(14, 10))\\nplt.subplots_adjust(hspace=0.35)\\naxes[0].set_ylim(0, .2);\\n\\nCase Study 2: Yield Curve Construction and Interest Rate Modeling \\n\\n| \\n\\n223\\n\\n\\x0cOutput\\n\\npd.DataFrame(pca.components_[0:3].T).plot(style= [\\'s-\\',\\'o-\\',\\'^-\\'], \\\\\\n                            legend=False, title=\"Principal Component\")\\n\\nOutput\\n\\nBy  plotting  the  components  of  the  eigenvectors  we  can  make  the  following\\ninterpretation:\\n\\n224 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cPrincipal Component 1\\n\\nThis  eigenvector  has  all  positive  values,  with  all  tenors  weighted  in  the  same\\ndirection. This means that the first principal component reflects movements that\\ncause all maturities to move in the same direction, corresponding to directional\\nmovements  in  the  yield  curve.  These  are  movements  that  shift  the  entire  yield\\ncurve up or down.\\n\\nPrincipal Component 2\\n\\nThe second eigenvector has the first half of the components negative and the sec‐\\nond  half  positive.  Treasury  rates  on  the  short  end  (long  end)  of  the  curve  are\\nweighted  positively  (negatively).  This  means  that  the  second  principal  compo‐\\nnent reflects movements that cause the short end to go in one direction and the\\nlong  end  in  the  other.  Consequently,  it  represents  slope  movements  in  the  yield\\ncurve.\\n\\nPrincipal Component 3\\n\\nThe third eigenvector has the first third of the components negative, the second\\nthird  positive,  and  the  last  third  negative.  This  means  that  the  third  principal\\ncomponent  reflects  movements  that  cause  the  short  and  long  end  to  go  in  one\\ndirection, and the middle to go in the other, resulting in curvature movements of\\nthe yield curve.\\n\\n5.2.3. Reconstructing the curve using principal components.     One  of  the  key  features  of\\nPCA is the ability to reconstruct the initial dataset using the outputs of PCA. Using\\nsimple matrix reconstruction, we can generate a near exact replica of the initial data:\\n\\npca.transform(rescaledDataset)[:, :2]\\n\\nOutput\\n\\narray([[ 4.97514826, -0.48514999],\\n       [ 5.03634891, -0.52005102],\\n       [ 5.14497849, -0.58385444],\\n       ...,\\n       [-1.82544584,  2.82360062],\\n       [-1.69938513,  2.6936174 ],\\n       [-1.73186029,  2.73073137]])\\n\\nMechanically, PCA is just a matrix multiplication:\\n\\nY = XW\\n\\nwhere  Y  is  the  principal  components,  X  is  input  data,  and  W  is  a  matrix  of  coeffi‐\\ncients, which we can use to recover the original matrix as per the equation below:\\n\\nX = YW ′\\n\\nCase Study 2: Yield Curve Construction and Interest Rate Modeling \\n\\n| \\n\\n225\\n\\n\\x0cwhere W′ is the inverse of the matrix of coefficients W.\\n\\nnComp=3\\nreconst= pd.DataFrame(np.dot(pca.transform(rescaledDataset)[:, :nComp],\\\\\\npca.components_[:nComp,:]),columns=dataset.columns)\\nplt.figure(figsize=(10,8))\\nplt.plot(reconst)\\nplt.ylabel(\"Treasury Rate\")\\nplt.title(\"Reconstructed Dataset\")\\nplt.show()\\n\\nThis figure shows the replicated Treasury rate chart and demonstrates that, using just\\nthe  first  three  principal  components,  we  are  able  to  replicate  the  original  chart.\\nDespite reducing the data from 11 dimensions to three, we still retain more than 99%\\nof the information and can reproduce the original data easily. Additionally, we also\\nhave intuition around these three drivers of yield curve moments. Reducing the yield\\ncurve  into  fewer  components  means  practictioners  can  focus  on  fewer  factors  that\\ninfluence  interest  rates.  For  example,  in  order  to  hedge  a  portfolio,  it  may  be  suffi‐\\ncient  to  protect  the  portfolio  against  moves  in  the  first  three  principal  components\\nonly.\\n\\nOutput\\n\\n226 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cConclusion\\n\\nIn this case study, we introduced dimensionality reduction to break down the Treas‐\\nury  rate  curve  into  fewer  components.  We  saw  that  the  principal  components  are\\nquite intuitive for this case study. The first three principal components explain more\\nthan 99.5% of the variation and represent directional movements, slope movements,\\nand curvature movements, respectively.\\n\\nBy using principal component analysis, analyzing the eigenvectors, and understand‐\\ning the intuition behind them, we demonstrated how using dimensionality reduction\\nled to fewer intuitive dimensions in the yield curve. Such dimensionality reduction of\\nthe  yield  curve  can  potentially  lead  to  faster  and  more  effective  portfolio  manage‐\\nment, trading, hedging, and risk management.\\n\\nCase Study 3: Bitcoin Trading: Enhancing Speed and\\nAccuracy\\nAs  trading  becomes  more  automated,  traders  will  continue  to  seek  to  use  as  many\\nfeatures and technical indicators as they can to make their strategies more accurate\\nand efficient. One of the many challenges in this is that adding more variables leads\\nto  ever  more  complexity,  making  it  increasingly  difficult  to  arrive  at  solid  conclu‐\\nsions.  Using  dimensionality  reduction  techniques,  we  can  compress  many  features\\nand technical indicators into a few logical collections, while still maintaining a signifi‐\\ncant amount of the variance of the original data. This helps speed up model training\\nand tuning. Additionally, it helps prevent overfitting by getting rid of correlated vari‐\\nables,  which  can  ultimately  cause  more  harm  than  good.  Dimensionality  reduction\\nalso  enhances  exploration  and  visualization  of  a  dataset  to  understand  grouping  or\\nrelationships, an important task when building and continuously monitoring trading\\nstrategies.\\n\\nIn  this  case  study,  we  will  use  dimensionality  reduction  to  enhance  “Case  Study  3:\\nBitcoin Trading Strategy” on page 179 presented in Chapter 6. In this case study, we\\ndesign  a  trading  strategy  for  bitcoin  that  considers  the  relationship  between  the\\nshort-term and long-term prices to predict a buy or sell signal. We create several new\\nintuitive,  technical  indicator  features,  including  trend,  volume,  volatility,  and\\nmomentum.  We  apply  dimensionality  reduction  techniques  on  these  features  in\\norder to achieve better results.\\n\\nCase Study 3: Bitcoin Trading: Enhancing Speed and Accuracy \\n\\n| \\n\\n227\\n\\n\\x0cIn this case study, we will focus on:\\n\\n• Reducing the dimensions of a dataset to yield better and faster results for super‐\\n\\nvised learning.\\n\\n• Using SVD and t-SNE to visualize data in lower dimensions.\\n\\nBlueprint for Using Dimensionality Reduction to Enhance a\\nTrading Strategy\\n\\n1. Problem definition\\n\\nOur goal in this case study is to use dimensionality reduction techniques to enhance\\nan algorithmic trading strategy. The data and the variables used in this case study are\\nthe same as in “Case Study 3: Bitcoin Trading Strategy” on page 179. For reference,\\nwe  are  using  intraday  bitcoin  price  data,  volume,  and  weighted  bitcoin  price  from\\nJanuary 2012 to October 2017. Steps 3 and 4 presented in this case study use the same\\nsteps as the case study in Chapter 6. As such, these steps are condensed in this case\\nstudy to avoid repetition.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.    The Python packages used for this case study are the\\nsame as those presented in the previous two case studies in this chapter.\\n\\n3. Exploratory data analysis\\n\\nRefer to “3. Exploratory data analysis” on page 181 for more details of this step.\\n\\n4. Data preparation\\n\\nWe prepare the data for modeling in the following sections.\\n\\n4.1. Data cleaning.    We clean the data by filling the NAs with the last available values:\\n\\ndataset[dataset.columns] = dataset[dataset.columns].ffill()\\n\\n4.2. Preparing the data for classification.     We  attach  the  following  label  to  each  move‐\\nment:  1  if  the  short-term  price  increases  compared  to  the  long-term  price;  0  if  the\\nshort-term price decreases compared to the long-term price. This label is assigned to\\n\\n228 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0ca variable we will call signal, which is the predicted variable for this case study. Let us\\nlook at the data for prediction:\\n\\ndataset.tail(5)\\n\\nOutput\\n\\nThe dataset contains the signal column along with all other columns.\\n\\n4.3. Feature engineering.    In this step, we construct a dataset that contains the predic‐\\ntors that will be used to make the signal prediction. Using the bitcoin intraday price\\ndata,  including  daily  open,  high,  low,  close,  and  volume,  we  compute  the  following\\ntechnical indicators:\\n\\n• Moving Average\\n\\n• Stochastic Oscillator %K and %D\\n\\n• Relative Strength Index (RSI)\\n\\n• Rate Of Change (ROC)\\n\\n• Momentum (MOM)\\n\\nThe code for the construction of all of the indicators, along with their descriptions, is\\npresented in Chapter 6. The final dataset and the columns used are as follows:\\n\\n4.4. Data visualization.    Let us look at the distribution of the predicted variable:\\n\\nfig = plt.figure()\\nplot = dataset.groupby([\\'signal\\']).size().plot(kind=\\'barh\\', color=\\'red\\')\\nplt.show()\\n\\nCase Study 3: Bitcoin Trading: Enhancing Speed and Accuracy \\n\\n| \\n\\n229\\n\\n\\x0cOutput\\n\\nThe predicted signal is “buy” 52.9% of the time.\\n\\n5. Evaluate algorithms and models\\n\\nNext, we perform dimensionality reduction and evaluate the models.\\n\\n5.1. Train-test split.    In this step, we split the dataset into training and test sets:\\n\\nY= subset_dataset[\"signal\"]\\nX = subset_dataset.loc[:, dataset.columns != \\'signal\\'] validation_size = 0.2\\nX_train, X_validation, Y_train, Y_validation = train_test_split\\\\\\n(X, Y, test_size=validation_size, random_state=1)\\n\\nWe standardize the variables on the same scale before applying dimensionality reduc‐\\ntion. Data standardization is performed using the following Python code:\\n\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler().fit(X_train)\\nrescaledDataset = pd.DataFrame(scaler.fit_transform(X_train),\\\\\\ncolumns = X_train.columns, index = X_train.index)\\n# summarize transformed data\\nX_train.dropna(how=\\'any\\', inplace=True)\\nrescaledDataset.dropna(how=\\'any\\', inplace=True)\\nrescaledDataset.head(2)\\n\\nOutput\\n\\n230 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0c5.2. Singular value decomposition (feature reduction).     Here  we  will  use  SVD  to  perform\\nPCA. Specifically, we are using the TruncatedSVD method in the sklearn package to\\ntransform the full dataset into a representation using the top five components:\\n\\nncomps = 5\\nsvd = TruncatedSVD(n_components=ncomps)\\nsvd_fit = svd.fit(rescaledDataset)\\nY_pred = svd.fit_transform(rescaledDataset)\\nax = pd.Series(svd_fit.explained_variance_ratio_.cumsum()).plot(kind=\\'line\\', \\\\\\nfigsize=(10, 3))\\nax.set_xlabel(\"Eigenvalues\")\\nax.set_ylabel(\"Percentage Explained\")\\nprint(\\'Variance preserved by first 5 components == {:.2%}\\'.\\\\\\nformat(svd_fit.explained_variance_ratio_.cumsum()[-1]))\\n\\nOutput\\n\\nFollowing  the  computation,  we  preserve  92.75%  of  the  variance  by  using  just  five\\ncomponents rather than the full 25+ original features. This is a tremendously useful\\ncompression for the analysis and iterations of the model.\\n\\nCase Study 3: Bitcoin Trading: Enhancing Speed and Accuracy \\n\\n| \\n\\n231\\n\\n\\x0cFor  convenience,  we  will  create  a  Python  dataframe  specifically  for  these  top  five\\ncomponents:\\n\\ndfsvd = pd.DataFrame(Y_pred, columns=[\\'c{}\\'.format(c) for \\\\\\nc in range(ncomps)], index=rescaledDataset.index)\\nprint(dfsvd.shape)\\ndfsvd.head()\\n\\nOutput\\n\\n(8000, 5)\\n\\nc0\\n\\n2834071 –2.252\\n\\nc1\\n1.920\\n\\nc2\\n0.538\\n\\nc4\\n\\nc3\\n–0.019 –0.967\\n\\n2836517\\n\\n5.303\\n\\n–1.689 –0.678\\n\\n0.473\\n\\n0.643\\n\\n2833945 –2.315 –0.042\\n\\n1.697\\n\\n–1.704\\n\\n1.672\\n\\n2835048 –0.977\\n\\n0.782\\n\\n3.706\\n\\n–0.697\\n\\n0.057\\n\\n2838804\\n\\n2.115\\n\\n–1.915\\n\\n0.475\\n\\n–0.174 –0.299\\n\\n5.2.1. Basic visualization of reduced features.    Let us visualize the compressed dataset:\\n\\nsvdcols = [c for c in dfsvd.columns if c[0] == \\'c\\']\\n\\nPairs-plots\\n\\nPairs-plots are a simple representation of a set of 2D scatterplots, with each com‐\\nponent  plotted  against  every  other  component.  The  data  points  are  colored\\naccording to their signal classification:\\n\\nplotdims = 5\\nploteorows = 1\\ndfsvdplot = dfsvd[svdcols].iloc[:, :plotdims]\\ndfsvdplot[\\'signal\\']=Y_train\\nax = sns.pairplot(dfsvdplot.iloc[::ploteorows, :], hue=\\'signal\\', size=1.8)\\n\\n232 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cOutput\\n\\nWe can see that there is clear separation of the colored dots (full color version avail‐\\nable  on  GitHub),  meaning  that  data  points  from  the  same  signal  tend  to  cluster\\ntogether. The separation is more distinct for the first components, with the character‐\\nistics  of  signal  distributions  growing  more  similar  as  you  progress  from  the  first  to\\nthe  fifth  component.  That  said,  the  plot  provides  support  for  using  all  five  compo‐\\nnents in our model.\\n\\n5.3. t-SNE visualization.    In this step, we implement t-SNE and look at the related visu‐\\nalization. We will use the basic implementation available in Scikit-learn:\\n\\ntsne = TSNE(n_components=2, random_state=0)\\n\\nZ = tsne.fit_transform(dfsvd[svdcols])\\ndftsne = pd.DataFrame(Z, columns=[\\'x\\',\\'y\\'], index=dfsvd.index)\\n\\nCase Study 3: Bitcoin Trading: Enhancing Speed and Accuracy \\n\\n| \\n\\n233\\n\\n\\x0cdftsne[\\'signal\\'] = Y_train\\n\\ng = sns.lmplot(\\'x\\', \\'y\\', dftsne, hue=\\'signal\\', fit_reg=False, size=8\\n                , scatter_kws={\\'alpha\\':0.7,\\'s\\':60})\\n\\nOutput\\n\\nThe  plot  shows  us  that  there  is  a  good  degree  of  clustering  for  the  trading  signal.\\nThere  is  some  overlap  of  the  long  and  short  signals,  but  they  can  be  distinguished\\nquite well using the reduced number of features.\\n\\n5.4. Compare models with and without dimensionality reduction.     In  this  step,  we  analyze\\nthe  impact  of  the  dimensionality  reduction  on  the  classification  and  the  impact  on\\nthe overall accuracy and computation time:\\n\\n# test options for classification\\nscoring = \\'accuracy\\'\\n\\n5.4.1. Models.     We  first  look  at  the  time  taken  by  the  model  without  dimensionality\\nreduction, where we have all the technical indicators:\\n\\nimport time\\nstart_time = time.time()\\n\\n# spot-check the algorithms\\n\\n234 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cmodels =  RandomForestClassifier(n_jobs=-1)\\ncv_results_XTrain= cross_val_score(models, X_train, Y_train, cv=kfold, \\\\\\n  scoring=scoring)\\nprint(\"Time Without Dimensionality Reduction--- %s seconds ---\" % \\\\\\n(time.time() - start_time))\\n\\nOutput\\n\\nTime Without Dimensionality Reduction\\n7.781347990036011 seconds\\n\\nThe total time taken without dimensionality reduction is around eight seconds. Let\\nus look at the time it takes with dimensionality reduction, when only the five princi‐\\npal components from the truncated SVD are used:\\n\\nstart_time = time.time()\\nX_SVD= dfsvd[svdcols].iloc[:, :5]\\ncv_results_SVD = cross_val_score(models, X_SVD, Y_train, cv=kfold, \\\\\\n  scoring=scoring)\\nprint(\"Time with Dimensionality Reduction--- %s seconds ---\" % \\\\\\n(time.time() - start_time))\\n\\nOutput\\n\\nTime with Dimensionality Reduction\\n2.281977653503418 seconds\\n\\nThe  total  time  taken  with  dimensionality  reduction  is  around  two  seconds—four\\ntimes  a  reduction  in  time,  which  is  a  significant  improvement.  Let  us  investigate\\nwhether there is any decline in the accuracy when using the condensed dataset:\\n\\nprint(\"Result without dimensionality Reduction: %f (%f)\" %\\\\\\n (cv_results_XTrain.mean(), cv_results_XTrain.std()))\\nprint(\"Result with dimensionality Reduction: %f (%f)\" %\\\\\\n (cv_results_SVD.mean(), cv_results_SVD.std()))\\n\\nOutput\\n\\nResult without dimensionality Reduction: 0.936375 (0.010774)\\nResult with dimensionality Reduction: 0.887500 (0.012698)\\n\\nAccuracy declines roughly 5%, from 93.6% to 88.7%. The improvement in speed has\\nto be balanced against this loss in accuracy. Whether the loss in accuracy is accepta‐\\nble likely depends on the problem. If this is a model that needs to be recalibrated very\\nfrequently, then a lower computation time will be essential, especially when handling\\nlarge,  high-velocity  datasets.  The  improvement  in  the  computation  time  does  have\\nother benefits, especially in the early stages of trading strategy development. It ena‐\\nbles us to test a greater number of features (or technical indicators) in less time.\\n\\nConclusion\\n\\nIn  this  case  study,  we  demonstrated  the  efficiency  of  dimensionality  reduction  and\\nprincipal components analysis in reducing the number of dimensions in the context\\n\\nCase Study 3: Bitcoin Trading: Enhancing Speed and Accuracy \\n\\n| \\n\\n235\\n\\n\\x0cof a trading strategy. Through dimensionality reduction, we achieved a commensu‐\\nrate  accuracy  rate  with  a  fourfold  improvement  in  the  modeling  speed.  In  trading\\nstrategy  development  involving  expansive  datasets,  such  speed  enhancements  can\\nlead to improvements for the entire process.\\n\\nWe demonstrated that both SVD and t-SNE yield reduced datasets that can easily be\\nvisualized for evaluating trading signal data. This allowed us to distinguish the long\\nand short signals of this trading strategy in ways not possible with the original num‐\\nber of features.\\n\\nChapter Summary\\nThe case studies presented in this chapter focused on understanding the concepts of\\nthe  different  dimensionality  reduction  methods,  developing  intuition  around  the\\nprincipal components, and visualizing the condensed datasets.\\n\\nOverall,  the  concepts  in  Python,  machine  learning,  and  finance  presented  in  this\\nchapter through the case studies can used as a blueprint for any other dimensionality\\nreduction–based problem in finance.\\n\\nIn the next chapter, we explore concepts and case studies for another type of unsu‐\\npervised learning—clustering.\\n\\nExercises\\n\\n1. Using  dimensionality  reduction,  extract  the  different  factors  from  the  stocks\\n\\nwithin a different index and use them to build a trading strategy.\\n\\n2. Pick any of the regression-based case studies in Chapter 5 and use dimensional‐\\nity  reduction  to  see  whether  there  is  any  improvement  in  computation  time.\\nExplain  the  components  using  the  factor  loading  and  develop  some  high-level\\nintuition of them.\\n\\n3. For case study 3 presented in this chapter, perform factor loading of the principal\\n\\ncomponents and understand the intuition of the different components.\\n\\n4. Get the principal components of different currency pairs or different commodity\\nprices. Identify the drivers of the primary principal components and link them to\\nsome intuitive macroeconomic variables.\\n\\n236 \\n\\n| \\n\\nChapter 7: Unsupervised Learning: Dimensionality Reduction\\n\\n\\x0cCHAPTER 8\\nUnsupervised Learning: Clustering\\n\\nIn the previous chapter, we explored dimensionality reduction, which is one type of\\nunsupervised learning. In this chapter, we will explore clustering, a category of unsu‐\\npervised learning techniques that allows us to discover hidden structures in data.\\n\\nBoth  clustering  and  dimensionality  reduction  summarize  the  data.  Dimensionality\\nreduction compresses the data by representing it using new, fewer features while still\\ncapturing the most relevant information. Similarly, clustering is a way to reduce the\\nvolume  of  data  and  find  patterns.  However,  it  does  so  by  categorizing  the  original\\ndata and not by creating new variables. Clustering algorithms assign observations to\\nsubgroups that consist of similar data points. The goal of clustering is to find a natu‐\\nral  grouping  in  data  so  that  items  in  a  given  cluster  are  more  similar  to  each  other\\nthan  to  those  of  different  clusters.  Clustering  serves  to  better  understand  the  data\\nthrough the lens of several categories or groups created. It also permits the automatic\\ncategorization of new objects according to the learned criteria.\\n\\nIn the field of finance, clustering has been used by traders and investment managers\\nto find homogeneous groups of assets, classes, sectors, and countries based on similar\\ncharacteristics. Clustering analysis augments trading strategies by providing insights\\ninto categories of trading signals. The technique has been used to segment customers\\nor investors into a number of groups to better understand their behavior and to per‐\\nform additional analysis.\\n\\nIn  this  chapter,  we  will  discuss  fundamental  clustering  techniques  and  introduce\\nthree  case  studies  in  the  areas  of  portfolio  management  and  trading  strategy\\ndevelopment.\\n\\nIn  “Case  Study  1:  Clustering  for  Pairs  Trading”  on  page  243,  we  use  clustering\\nmethods  to  select  pairs  of  stocks  for  a  trading  strategy.  A  pairs  trading  strategy\\ninvolves matching a long position with a short position in two financial instruments\\n\\n237\\n\\n\\x0cthat are closely related. Finding appropriate pairs can be a challenge when the num‐\\nber of instruments is high. In this case study, we demonstrate how clustering can be a\\nuseful technique in trading strategy development and other similar situations.\\n\\nIn  “Case  Study  2:  Portfolio  Management:  Clustering  Investors”  on  page  259,  we\\nidentify  clusters  of  investors  with  similar  abilities  and  willingness  to  take  risks.  We\\nshow how clustering techniques can be used for effective asset allocation and portfo‐\\nlio rebalancing. This illustrates how part of the portfolio management process can be\\nautomated,  which  is  immensely  useful  for  investment  managers  and  robo-advisors\\nalike.\\n\\nIn  “Case  Study  3:  Hierarchical  Risk  Parity”  on  page  267,  we  use  a  clustering-based\\nalgorithm  to  allocate  capital  into  different  asset  classes  and  compare  the  results\\nagainst other portfolio allocation techniques.\\n\\nIn  this  chapter,  we  will  learn  about  the  following  concepts  related  to  clustering\\ntechniques:\\n\\n• Basic concepts of models and techniques used for clustering.\\n\\n• How to implement different clustering techniques in Python.\\n\\n• How to effectively perform visualizations of clustering outcomes.\\n\\n• Understanding the intuitive meaning of clustering results.\\n\\n• How to choose the right clustering techniques for a problem.\\n\\n• Selecting the appropriate number of clusters in different clustering algorithms.\\n\\n• Building hierarchical clustering trees using Python.\\n\\nThis Chapter’s Code Repository\\n\\nA  Python-based  master  template  for  clustering,  along  with  the\\nJupyter notebook for the case studies presented in this chapter are\\nin Chapter 8 - Unsup. Learning - Clustering in the code repository\\nfor this book. To work through any machine learning problems in\\nPython involving the models for clustering (such as k-means, hier‐\\narchical  clustering,  etc.)  presented  in  this  chapter,  readers  simply\\nneed to modify the template to align with their problem statement.\\nSimilar to the previous chapters, the case studies presented in this\\nchapter  use  the  standard  Python  master  template  with  the  stan‐\\ndardized model development steps presented in Chapter 2. For the\\nclustering  case  studies,  steps  6  (Model  Tuning  and  Grid  Search)\\nand  7  (Finalizing  the  Model)  have  merged  with  step  5  (Evaluate\\nAlgorithms and Models).\\n\\n238 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cClustering Techniques\\nThere are many types of clustering techniques, and they differ with respect to their\\nstrategy of identifying groupings. Choosing which technique to apply depends on the\\nnature  and  structure  of  the  data.  In  this  chapter,  we  will  cover  the  following  three\\nclustering techniques:\\n\\n• k-means clustering\\n\\n• Hierarchical clustering\\n\\n• Affinity propagation clustering\\n\\nThe  following  section  summarizes  these  clustering  techniques,  including  their\\nstrengths  and  weaknesses.  Additional  details  for  each  of  the  clustering  methods  are\\nprovided in the case studies.\\n\\nk-means Clustering\\nk-means  is  the  most  well-known  clustering  technique.  The  algorithm  of  k-means\\naims  to  find  and  group  data  points  into  classes  that  have  high  similarity  between\\nthem.  This  similarity  is  understood  as  the  opposite  of  the  distance  between  data\\npoints. The closer the data points are, the more likely they are to belong to the same\\ncluster.\\n\\nThe  algorithm  finds  k  centroids  and  assigns  each  data  point  to  exactly  one  cluster\\nwith  the  goal  of  minimizing  the  within-cluster  variance  (called  inertia).  It  typically\\nuses  the  Euclidean  distance  (ordinary  distance  between  two  points),  but  other  dis‐\\ntance  metrics  can  be  used.  The  k-means  algorithm  delivers  a  local  optimum  for  a\\ngiven k and proceeds as follows:\\n\\n1. This algorithm specifies the number of clusters.\\n\\n2. Data points are randomly selected as cluster centers.\\n\\n3. Each data point is assigned to the cluster center it is nearest to.\\n\\n4. Cluster centers are updated to the mean of the assigned points.\\n\\n5. Steps 3–4 are repeated until all cluster centers remain unchanged.\\n\\nIn  simple  terms,  we  randomly  move  around  the  specified  number  of  centroids  in\\neach iteration, assigning each data point to the closest centroid. Once we have done\\nthat, we calculate the mean distance of all points in each centroid. Then, once we can\\nno  longer  reduce  the  minimum  distance  from  data  points  to  their  respective  cent‐\\nroids, we have found our clusters.\\n\\nClustering Techniques \\n\\n| \\n\\n239\\n\\n\\x0ck-means hyperparameters\\n\\nThe k-means hyperparameters include:\\n\\nNumber of clusters\\n\\nThe number of clusters and centroids to generate.\\n\\nMaximum iterations\\n\\nMaximum iterations of the algorithm for a single run.\\n\\nNumber initial\\n\\nThe number of times the algorithm will be run with different centroid seeds. The\\nfinal result will be the best output of the defined number of consecutive runs, in\\nterms of inertia.\\n\\nWith k-means, different random starting points for the cluster centers often result in\\nvery different clustering solutions. Therefore, the k-means algorithm is run in sklearn\\nwith at least 10 different random initializations, and the solution occurring the great‐\\nest number of times is chosen.\\n\\nThe strengths of k-means include its simplicity, wide range of applicability, fast con‐\\nvergence, and linear scalability to large data while producing clusters of an even size.\\nIt is most useful when we know the exact number of clusters, k, beforehand. In fact, a\\nmain weakness of k-means is having to tune this hyperparameter. Additional draw‐\\nbacks include the lack of a guarantee to find a global optimum and its sensitivity to\\noutliers.\\n\\nImplementation in Python\\n\\nPython’s sklearn library offers a powerful implementation of k-means. The following\\ncode snippet illustrates how to apply k-means clustering on a dataset:\\n\\nfrom sklearn.cluster import KMeans\\n#Fit with k-means\\nk_means = KMeans(n_clusters=nclust)\\nk_means.fit(X)\\n\\nThe number of clusters is the key hyperparameter to be tuned. We will look at the k-\\nmeans clustering technique in case studies 1 and 2 of this chapter, in which further\\ndetails  on  choosing  the  right  number  of  clusters  and  detailed  visualizations  are\\nprovided.\\n\\nHierarchical Clustering\\nHierarchical  clustering  involves  creating  clusters  that  have  a  predominant  ordering\\nfrom top to bottom. The main advantage of hierarchical clustering is that we do not\\nneed  to  specify  the  number  of  clusters;  the  model  determines  that  by  itself.  This\\n\\n240 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cclustering technique is divided into two types: agglomerative hierarchical clustering\\nand divisive hierarchical clustering.\\n\\nAgglomerative hierarchical clustering is the most common type of hierarchical cluster‐\\ning  and  is  used  to  group  objects  based  on  their  similarity.  It  is  a  “bottom-up”\\napproach  where  each  observation  starts  in  its  own  cluster,  and  pairs  of  clusters  are\\nmerged  as  one  moves  up  the  hierarchy.  The  agglomerative  hierarchical  clustering\\nalgorithm delivers a local optimum and proceeds as follows:\\n\\n1. Make each data point a single-point cluster and form N clusters.\\n\\n2. Take the two closest data points and combine them, leaving N-1 clusters.\\n\\n3. Take the two closest clusters and combine them, forming N-2 clusters.\\n\\n4. Repeat step 3 until left with only one cluster.\\n\\nDivisive hierarchical clustering works “top-down” and sequentially splits the remain‐\\ning clusters to produce the most distinct subgroups.\\n\\nBoth produce N-1 hierarchical levels and facilitate the clustering creation at the level\\nthat best partitions data into homogeneous groups. We will focus on the more com‐\\nmon agglomerative clustering approach.\\n\\nHierarchical clustering enables the plotting of dendrograms, which are visualizations\\nof a binary hierarchical clustering. A dendrogram is a type of tree diagram showing\\nhierarchical relationships between different sets of data. They provide an interesting\\nand  informative  visualization  of  hierarchical  clustering  results.  A  dendrogram  con‐\\ntains  the  memory  of  the  hierarchical  clustering  algorithm,  so  you  can  tell  how  the\\ncluster is formed simply by inspecting the chart.\\n\\nFigure 8-1 shows an example of dendrograms based on hierarchical clustering. The\\ndistance between data points represents dissimilarities, and the height of the blocks\\nrepresents the distance between clusters.\\n\\nObservations that fuse at the bottom are similar, while those at the top are quite dif‐\\nferent. With dendrograms, conclusions are made based on the location of the vertical\\naxis rather than on the horizontal one.\\n\\nThe advantages of hierarchical clustering are that it is easy to implement it, does not\\nrequire one to specify the number of clusters, and it produces dendrograms that are\\nvery useful in understanding the data. However, the time complexity for hierarchical\\nclustering can result in long computation times relative to other algorithms, such as\\nk-means. If we have a large dataset, it can be difficult to determine the correct num‐\\nber of clusters by looking at the dendrogram. Hierarchical clustering is very sensitive\\nto outliers, and in their presence, model performance decreases significantly.\\n\\nClustering Techniques \\n\\n| \\n\\n241\\n\\n\\x0cFigure 8-1. Hierarchical clustering\\n\\nImplementation in Python\\n\\nThe following code snippet illustrates how to apply agglomerative hierarchical clus‐\\ntering with four clusters on a dataset:\\n\\nfrom sklearn.cluster import AgglomerativeClustering\\nmodel = AgglomerativeClustering(n_clusters=4, affinity=\\'euclidean\\',\\\\\\n  linkage=\\'ward\\')\\nclust_labels1 = model.fit_predict(X)\\n\\nMore details regarding the hyperparameters of agglomerative hierarchical clustering\\ncan be found on the sklearn website. We will look at the hierarchical clustering tech‐\\nnique in case studies 1 and 3 in this chapter.\\n\\nAffinity Propagation Clustering\\nAffinity  propagation  creates  clusters  by  sending  messages  between  data  points  until\\nconvergence. Unlike clustering algorithms such as k-means, affinity propagation does\\nnot require the number of clusters to be determined or estimated before running the\\nalgorithm. Two important parameters are used in affinity propagation to determine\\nthe number of clusters: the preference, which controls how many exemplars (or proto‐\\ntypes) are used; and the damping factor, which dampens the responsibility and availa‐\\nbility of messages to avoid numerical oscillations when updating these messages.\\n\\n242 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cA dataset is described using a small number of exemplars. These are members of the\\ninput set that are representative of clusters. The affinity propagation algorithm takes\\nin a set of pairwise similarities between data points and finds clusters by maximizing\\nthe  total  similarity  between  data  points  and  their  exemplars.  The  messages  sent\\nbetween pairs represent the suitability of one sample to be the exemplar of the other,\\nwhich is updated in response to the values from other pairs. This updating happens\\niteratively until convergence, at which point the final exemplars are chosen, and we\\nobtain the final clustering.\\n\\nIn terms of strengths, affinity propagation does not require the number of clusters to\\nbe determined before running the algorithm. The algorithm is fast and can be applied\\nto  large  similarity  matrices.  However,  the  algorithm  often  converges  to  suboptimal\\nsolutions, and at times it can fail to converge.\\n\\nImplementation in Python\\n\\nThe  following  code  snippet  illustrates  how  to  implement  the  affinity  propagation\\nalgorithm for a dataset:\\n\\nfrom sklearn.cluster import AffinityPropagation\\n# Initialize the algorithm and set the number of PC\\'s\\nap = AffinityPropagation()\\nap.fit(X)\\n\\nMore details regarding the hyperparameters of affinity propagation clustering can be\\nfound on the sklearn website. We will look at the affinity propagation technique in\\ncase studies 1 and 2 in this chapter.\\n\\nCase Study 1: Clustering for Pairs Trading\\nA pairs trading strategy constructs a portfolio of correlated assets with similar market\\nrisk factor exposure. Temporary price discrepancies in these assets can create oppor‐\\ntunities to profit through a long position in one instrument and a short position in\\nanother.  A  pairs  trading  strategy  is  designed  to  eliminate  market  risk  and  exploit\\nthese temporary discrepancies in the relative returns of stocks.\\n\\nThe  fundamental  premise  in  pairs  trading  is  that  mean  reversion  is  an  expected\\ndynamic  of  the  assets.  This  mean  reversion  should  lead  to  a  long-run  equilibrium\\nrelationship,  which  we  try  to  approximate  through  statistical  methods.  When\\nmoments of (presumably temporary) divergence from this long-term trend arise, one\\ncan possibly profit. The key to successful pairs trading is the ability to select the right\\npairs of assets to be used.\\n\\nTraditionally, trial and error was used for pairs selection. Stocks or instruments that\\nwere merely in the same sector or industry were grouped together. The idea was that\\nif  these  stocks  were  for  companies  in  similar  industries,  their  stocks  should  move\\n\\nCase Study 1: Clustering for Pairs Trading \\n\\n| \\n\\n243\\n\\n\\x0csimilarly as well. However, this was and is not necessarily the case. Additionally, with\\na large universe of stocks, finding a suitable pair is a difficult task, given that there are\\na total of n(n–1)/2 possible pairs, where n is the number of instruments. Clustering\\ncan be a useful technique here.\\n\\nIn this case study, we will use clustering algorithms to select pairs of stocks for a pairs\\ntrading strategy.\\n\\nThis case study will focus on:\\n\\n• Evaluating three main clustering methods: k-means, hierarchical clustering, and\\n\\naffinity propagation clustering.\\n\\n• Understanding  approaches  to  finding  the  right  number  of  clusters  in  k-means\\n\\nand hierarchical clustering.\\n\\n• Visualizing data in the clusters, including viewing dendrograms.\\n\\n• Selecting the right clustering algorithm.\\n\\nBlueprint for Using Clustering to Select Pairs\\n\\n1. Problem definition\\n\\nOur goal in this case study is to perform clustering analysis on the stocks in the S&P\\n500  to  come  up  with  pairs  for  a  pairs  trading  strategy.  S&P  500  stock  data  was\\nobtained using pandas_datareader from Yahoo Finance. It includes price data from\\n2018 onwards.\\n\\n2. Getting started—loading the data and Python packages\\n\\nThe  list  of  the  libraries  used  for  data  loading,  data  analysis,  data  preparation,  and\\nmodel evaluation are shown below.\\n\\n2.1. Loading the Python packages.    The details of most of these packages and functions\\nhave been provided in Chapters 2 and 4. The use of these packages will be demon‐\\nstrated in different steps of the model development process.\\n\\nPackages for clustering\\n\\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation\\nfrom scipy.cluster.hierarchy import fcluster\\n\\n244 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cfrom scipy.cluster.hierarchy import dendrogram, linkage, cophenet\\nfrom scipy.spatial.distance import pdist\\nfrom sklearn.metrics import adjusted_mutual_info_score\\nfrom sklearn import cluster, covariance, manifold\\n\\nPackages for data processing and visualization\\n\\n# Load libraries\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom pandas import read_csv, set_option\\nfrom pandas.plotting import scatter_matrix\\nimport seaborn as sns\\nfrom sklearn.preprocessing import StandardScaler\\nimport datetime\\nimport pandas_datareader as dr\\nimport matplotlib.ticker as ticker\\nfrom itertools import cycle\\n\\n2.2. Loading the data.    The stock data is loaded below.1\\n\\ndataset = read_csv(\\'SP500Data.csv\\', index_col=0)\\n\\n3. Exploratory data analysis\\n\\nWe take a quick look at the data in this section.\\n\\n3.1. Descriptive statistics.    Let us look at the shape of the data:\\n\\n# shape\\ndataset.shape\\n\\nOutput\\n\\n(448, 502)\\n\\nThe data contains 502 columns and 448 observations.\\n\\n3.2. Data visualization.    We will take a detailed look into the visualization postcluster‐\\ning.\\n\\n4. Data preparation\\n\\nWe prepare the data for modeling in the following sections.\\n\\n1 Refer to the Jupyter notebook to understand fetching price data using pandas_datareader.\\n\\nCase Study 1: Clustering for Pairs Trading \\n\\n| \\n\\n245\\n\\n\\x0c4.1. Data cleaning.    In this step, we check for NAs in the rows and either drop them or\\nfill them with the mean of the column:\\n\\n#Checking for any null values and removing the null values\\'\\'\\'\\nprint(\\'Null Values =\\',dataset.isnull().values.any())\\n\\nOutput\\n\\nNull Values = True\\n\\nLet us get rid of the columns with more than 30% missing values:\\n\\nmissing_fractions = dataset.isnull().mean().sort_values(ascending=False)\\nmissing_fractions.head(10)\\ndrop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\\ndataset.drop(labels=drop_list, axis=1, inplace=True)\\ndataset.shape\\n\\nOutput\\n\\n(448, 498)\\n\\nGiven that there are null values, we drop some rows:\\n\\n# Fill the missing values with the last value available in the dataset.\\ndataset=dataset.fillna(method=\\'ffill\\')\\n\\nThe  data  cleaning  steps  identified  those  with  missing  values  and  populated  them.\\nThis step is important for creating a meaningful, reliable, and clean dataset that can\\nbe used without any errors in the clustering.\\n\\n4.2. Data transformation.     For  the  purpose  of  clustering,  we  will  be  using  annual\\nreturns and variance as the variables, as they are primary indicators of stock perfor‐\\nmance and volatility. The following code prepares these variables:\\n\\n#Calculate average annual percentage return and volatilities\\nreturns = pd.DataFrame(dataset.pct_change().mean() * 252)\\nreturns.columns = [\\'Returns\\']\\nreturns[\\'Volatility\\'] = dataset.pct_change().std() * np.sqrt(252)\\ndata = returns\\n\\nAll the variables should be on the same scale before applying clustering; otherwise, a\\nfeature with large values will dominate the result. We use StandardScaler in sklearn\\nto standardize the dataset features onto unit scale (mean = 0 and variance = 1):\\n\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler().fit(data)\\nrescaledDataset = pd.DataFrame(scaler.fit_transform(data),\\\\\\n  columns = data.columns, index = data.index)\\n# summarize transformed data\\nrescaledDataset.head(2)\\n\\n246 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cOutput\\n\\nReturns\\n0.794067 –0.702741 ABBV\\n\\nVolatility\\n\\nABT\\n\\nWith the data prepared, we can now explore the clustering algorithms.\\n\\n5. Evaluate algorithms and models\\n\\nWe will look at the following models:\\n\\n• k-means\\n\\n• Hierarchical clustering (agglomerative clustering)\\n\\n• Affinity propagation\\n\\n5.1. k-means clustering.    Here, we model using k-means and evaluate two ways to find\\nthe optimal number of clusters.\\n\\n5.1.1. Finding the optimal number of clusters.     We  know  that  k-means  initially  assigns\\ndata points to clusters randomly and then calculates centroids or mean values. Fur‐\\nther, it calculates the distances within each cluster, squares these, and sums them to\\nget the sum of squared errors.\\n\\nThe  basic  idea  is  to  define  k  clusters  so  that  the  total  within-cluster  variation  (or\\nerror) is minimized. The following two methods are useful in finding the number of\\nclusters in k-means:\\n\\nElbow method\\n\\nBased on the sum of squared errors (SSE) within clusters\\n\\nSilhouette method\\n\\nBased on the silhouette score\\n\\nFirst, let’s examine the elbow method. The SSE for each point is the square of the dis‐\\ntance of the point from its representation (i.e., its predicted cluster center). The sum\\nof squared errors is plotted for a range of values for the number of clusters. The first\\ncluster  will  add  much  information  (explain  a  lot  of  variance),  but  eventually  the\\nmarginal  gain  will  drop,  giving  an  angle  in  the  graph.  The  number  of  clusters  is\\nchosen at this point; hence it is referred to as the “elbow criterion.”\\n\\nLet us implement this in Python using the sklearn library and plot the SSE for a range\\nof values for k:\\n\\ndistortions = []\\nmax_loop=20\\nfor k in range(2, max_loop):\\n\\nCase Study 1: Clustering for Pairs Trading \\n\\n| \\n\\n247\\n\\n\\x0c    kmeans = KMeans(n_clusters=k)\\n    kmeans.fit(X)\\n    distortions.append(kmeans.inertia_)\\nfig = plt.figure(figsize=(15, 5))\\nplt.plot(range(2, max_loop), distortions)\\nplt.xticks([i for i in range(2, max_loop)], rotation=75)\\nplt.grid(True)\\n\\nOutput\\n\\nInspecting the sum of squared errors chart, it appears the elbow kink occurs around\\nfive or six clusters for this data. Certainly we can see that as the number of clusters\\nincreases past six, the SSE within clusters begins to plateau.\\n\\nNow let’s look at the silhouette method. The silhouette score measures how similar a\\npoint  is  to  its  own  cluster  (cohesion)  compared  to  other  clusters  (separation).  The\\nrange of the silhouette value is between 1 and –1. A high value is desirable and indi‐\\ncates  that  the  point  is  placed  in  the  correct  cluster.  If  many  points  have  a  negative\\nsilhouette value, that may indicate that we have created too many or too few clusters.\\n\\nLet  us  implement  this  in  Python  using  the  sklearn  library  and  plot  the  silhouette\\nscore for a range of values for k:\\n\\nfrom sklearn import metrics\\n\\nsilhouette_score = []\\nfor k in range(2, max_loop):\\n        kmeans = KMeans(n_clusters=k,  random_state=10, n_init=10, n_jobs=-1)\\n        kmeans.fit(X)\\n        silhouette_score.append(metrics.silhouette_score(X, kmeans.labels_, \\\\\\n          random_state=10))\\nfig = plt.figure(figsize=(15, 5))\\n\\n248 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cplt.plot(range(2, max_loop), silhouette_score)\\nplt.xticks([i for i in range(2, max_loop)], rotation=75)\\nplt.grid(True)\\n\\nOutput\\n\\nLooking  at  the  silhouette  score  chart,  we  can  see  that  there  are  various  parts  of  the\\ngraph at which a kink can be seen. Since there is not much of a difference in the SSE\\nafter  six  clusters,  it  implies  that  six  clusters  is  a  preferred  choice  in  this  k-means\\nmodel.\\n\\nCombining information from both methods, we infer the optimum number of clus‐\\nters to be six.\\n\\n5.1.2. Clustering and visualization.    Let us build the k-means model with six clusters and\\nvisualize the results:\\n\\nnclust=6\\n#Fit with k-means\\nk_means = cluster.KMeans(n_clusters=nclust)\\nk_means.fit(X)\\n#Extracting labels\\ntarget_labels = k_means.predict(X)\\n\\nVisualizing how clusters are formed is no easy task when the number of variables in\\nthe dataset is very large. A basic scatterplot is one method for visualizing a cluster in a\\ntwo-dimensional space. We create one below to identify the relationships inherent in\\nour data:\\n\\ncentroids = k_means.cluster_centers_\\nfig = plt.figure(figsize=(16,10))\\nax = fig.add_subplot(111)\\n\\nCase Study 1: Clustering for Pairs Trading \\n\\n| \\n\\n249\\n\\n\\x0cscatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c=k_means.labels_, \\\\\\n  cmap=\"rainbow\", label = X.index)\\nax.set_title(\\'k-means results\\')\\nax.set_xlabel(\\'Mean Return\\')\\nax.set_ylabel(\\'Volatility\\')\\nplt.colorbar(scatter)\\n\\nplt.plot(centroids[:,0],centroids[:,1],\\'sg\\',markersize=11)\\n\\nOutput\\n\\nIn the preceding plot, we can somewhat see that there are distinct clusters separated\\nby different colors (full-color version available on GitHub). The grouping of data in\\nthe plot seems to be separated quite well. There is also a degree of separation in the\\ncentroids of the clusters, represented by square dots.\\n\\nLet us look at the number of stocks in each of the clusters:\\n\\n# show number of stocks in each cluster\\nclustered_series = pd.Series(index=X.index, data=k_means.labels_.flatten())\\n# clustered stock with its cluster label\\nclustered_series_all = pd.Series(index=X.index, data=k_means.labels_.flatten())\\nclustered_series = clustered_series[clustered_series != -1]\\n\\nplt.figure(figsize=(12,7))\\nplt.barh(\\n\\n250 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0c    range(len(clustered_series.value_counts())), # cluster labels, y axis\\n    clustered_series.value_counts()\\n)\\nplt.title(\\'Cluster Member Counts\\')\\nplt.xlabel(\\'Stocks in Cluster\\')\\nplt.ylabel(\\'Cluster Number\\')\\nplt.show()\\n\\nOutput\\n\\nThe number of stocks per cluster ranges from around 40 to 120. Although the distri‐\\nbution is not equal, we have a significant number of stocks in each cluster.\\n\\nLet’s look at the hierarchical clustering.\\n\\n5.2. Hierarchical clustering (agglomerative clustering).     In  the  first  step,  we  look  at  the\\nhierarchy graph and check for the number of clusters.\\n\\n5.2.1. Building hierarchy graph/dendrogram.     The  hierarchy  class  has  a  dendrogram\\nmethod  that  takes  the  value  returned  by  the  linkage  method  of  the  same  class.  The\\nlinkage method takes the dataset and the method to minimize distances as parame‐\\nters. We use ward as the method since it minimizes the variance of distances between\\nthe clusters:\\n\\nCase Study 1: Clustering for Pairs Trading \\n\\n| \\n\\n251\\n\\n\\x0cfrom scipy.cluster.hierarchy import dendrogram, linkage, ward\\n\\n#Calculate linkage\\nZ= linkage(X, method=\\'ward\\')\\nZ[0]\\n\\nOutput\\n\\narray([3.30000000e+01, 3.14000000e+02, 3.62580431e-03, 2.00000000e+00])\\n\\nThe best way to visualize an agglomerative clustering algorithm is through a dendro‐\\ngram, which displays a cluster tree, the leaves being the individual stocks and the root\\nbeing  the  final  single  cluster.  The  distance  between  each  cluster  is  shown  on  the  y-\\naxis. The longer the branches are, the less correlated the two clusters are:\\n\\n#Plot Dendrogram\\nplt.figure(figsize=(10, 7))\\nplt.title(\"Stocks Dendrograms\")\\ndendrogram(Z,labels = X.index)\\nplt.show()\\n\\nOutput\\n\\n252 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cThis chart can be used to visually inspect the number of clusters that would be cre‐\\nated for a selected distance threshold (although the names of the stocks on the hori‐\\nzontal axis are not very clear, we can see that they are grouped into several clusters).\\nThe number of vertical lines a hypothetical straight, horizontal line will pass through\\nis the number of clusters created for that distance threshold value. For example, at a\\nvalue of 20, the horizontal line would pass through two vertical branches of the den‐\\ndrogram,  implying  two  clusters  at  that  distance  threshold.  All  data  points  (leaves)\\nfrom  that  branch  would  be  labeled  as  that  cluster  that  the  horizontal  line  passed\\nthrough.\\n\\nChoosing  a  threshold  cut  at  13  yields  four  clusters,  as  confirmed  in  the  following\\nPython code:\\n\\ndistance_threshold = 13\\nclusters = fcluster(Z, distance_threshold, criterion=\\'distance\\')\\nchosen_clusters = pd.DataFrame(data=clusters, columns=[\\'cluster\\'])\\nchosen_clusters[\\'cluster\\'].unique()\\n\\nOutput\\n\\narray([1, 4, 3, 2], dtype=int64)\\n\\n5.2.2. Clustering and visualization.     Let  us  build  the  hierarchical  clustering  model  with\\nfour clusters and visualize the results:\\n\\nnclust = 4\\nhc = AgglomerativeClustering(n_clusters=nclust, affinity=\\'euclidean\\', \\\\\\nlinkage=\\'ward\\')\\nclust_labels1 = hc.fit_predict(X)\\n\\nfig = plt.figure(figsize=(16,10))\\nax = fig.add_subplot(111)\\nscatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c=clust_labels1, cmap=\"rainbow\")\\nax.set_title(\\'Hierarchical Clustering\\')\\nax.set_xlabel(\\'Mean Return\\')\\nax.set_ylabel(\\'Volatility\\')\\nplt.colorbar(scatter)\\n\\nSimilar to the plot of k-means clustering, we see that there are some distinct clusters\\nseparated by different colors (full-size version available on GitHub).\\n\\nCase Study 1: Clustering for Pairs Trading \\n\\n| \\n\\n253\\n\\n\\x0cOutput\\n\\nNow let us look at affinity propagation clustering.\\n\\n5.3. Affinity propagation.    Let us build the affinity propagation model and visualize the\\nresults:\\n\\nap = AffinityPropagation()\\nap.fit(X)\\nclust_labels2 = ap.predict(X)\\n\\nfig = plt.figure(figsize=(10,8))\\nax = fig.add_subplot(111)\\nscatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c=clust_labels2, cmap=\"rainbow\")\\nax.set_title(\\'Affinity\\')\\nax.set_xlabel(\\'Mean Return\\')\\nax.set_ylabel(\\'Volatility\\')\\nplt.colorbar(scatter)\\n\\n254 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cOutput\\n\\nThe  affinity  propagation  model  with  the  chosen  hyperparameters  produced  many\\nmore  clusters  than  k-means  and  hierarchical  clustering.  There  is  some  clear  group‐\\ning,  but  also  more  overlap  due  to  the  larger  number  of  clusters  (full-size  version\\navailable on GitHub). In the next step, we will evaluate the clustering techniques.\\n\\n5.4. Cluster evaluation.    If the ground truth labels are not known, evaluation must be\\nperformed  using  the  model  itself.  The  silhouette  coefficient  (sklearn.metrics.sil\\nhouette_score)  is  one  example  we  can  use.  A  higher  silhouette  coefficient  score\\nimplies a model with better defined clusters. The silhouette coefficient is computed\\nfor each of the clustering methods defined above:\\n\\nfrom sklearn import metrics\\nprint(\"km\", metrics.silhouette_score(X, k_means.labels_, metric=\\'euclidean\\'))\\nprint(\"hc\", metrics.silhouette_score(X, hc.fit_predict(X), metric=\\'euclidean\\'))\\nprint(\"ap\", metrics.silhouette_score(X, ap.labels_, metric=\\'euclidean\\'))\\n\\nCase Study 1: Clustering for Pairs Trading \\n\\n| \\n\\n255\\n\\n\\x0cOutput\\n\\nkm 0.3350720873411941\\nhc 0.3432149515640865\\nap 0.3450647315156527\\n\\nGiven that affinity propagation performs the best, we proceed with affinity propaga‐\\ntion and use 27 clusters as specified by this clustering method.\\n\\nVisualizing the return within a cluster.    We have the clustering technique and the num‐\\nber of clusters finalized, but we need to check whether the clustering leads to a sensi‐\\nble  output.  To  do  this,  we  visualize  the  historical  behavior  of  the  stocks  in  a  few\\nclusters:\\n\\n# all stock with its cluster label (including -1)\\nclustered_series = pd.Series(index=X.index, data=ap.fit_predict(X).flatten())\\n# clustered stock with its cluster label\\nclustered_series_all = pd.Series(index=X.index, data=ap.fit_predict(X).flatten())\\nclustered_series = clustered_series[clustered_series != -1]\\n# get the number of stocks in each cluster\\ncounts = clustered_series_ap.value_counts()\\n# let\\'s visualize some clusters\\ncluster_vis_list = list(counts[(counts<25) & (counts>1)].index)[::-1]\\ncluster_vis_list\\n# plot a handful of the smallest clusters\\nplt.figure(figsize=(12, 7))\\ncluster_vis_list[0:min(len(cluster_vis_list), 4)]\\n\\nfor clust in cluster_vis_list[0:min(len(cluster_vis_list), 4)]:\\n    tickers = list(clustered_series[clustered_series==clust].index)\\n    # calculate the return (lognormal) of the stocks\\n    means = np.log(dataset.loc[:\"2018-02-01\", tickers].mean())\\n    data = np.log(dataset.loc[:\"2018-02-01\", tickers]).sub(means)\\n    data.plot(title=\\'Stock Time Series for Cluster %d\\' % clust)\\nplt.show()\\n\\nOutput\\n\\n256 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cLooking at the charts above, across all the clusters with small number of stocks, we\\nsee similar movement of the stocks under different clusters, which corroborates the\\neffectiveness of the clustering technique.\\n\\n6. Pairs selection\\n\\nOnce the clusters are created, several cointegration-based statistical techniques can be\\napplied on the stocks within a cluster to create the pairs. Two or more time series are\\nconsidered to be cointegrated if they are nonstationary and tend to move together.2\\nThe  presence  of  cointegration  between  time  series  can  be  validated  through  several\\nstatistical techniques, including the Augmented Dickey-Fuller test and the Johansen\\ntest.\\n\\nIn this step, we scan through a list of securities within a cluster and test for cointegra‐\\ntion  between  the  pairs.  First,  we  write  a  function  that  returns  a  cointegration  test\\nscore matrix, a p-value matrix, and any pairs for which the p-value was less than 0.05.\\n\\nCointegration and pair selection function.   \\n\\ndef find_cointegrated_pairs(data, significance=0.05):\\n    # This function is from https://www.quantopian.com\\n    n = data.shape[1]\\n    score_matrix = np.zeros((n, n))\\n    pvalue_matrix = np.ones((n, n))\\n    keys = data.keys()\\n    pairs = []\\n    for i in range(1):\\n        for j in range(i+1, n):\\n            S1 = data[keys[i]]\\n            S2 = data[keys[j]]\\n            result = coint(S1, S2)\\n            score = result[0]\\n            pvalue = result[1]\\n            score_matrix[i, j] = score\\n\\n2 Refer to Chapter 5 for more details.\\n\\nCase Study 1: Clustering for Pairs Trading \\n\\n| \\n\\n257\\n\\n\\x0c            pvalue_matrix[i, j] = pvalue\\n            if pvalue < significance:\\n                pairs.append((keys[i], keys[j]))\\n    return score_matrix, pvalue_matrix, pairs\\n\\nNext,  we  check  the  cointegration  of  different  pairs  within  several  clusters  using  the\\nfunction created above and return the pairs found:\\n\\nfrom statsmodels.tsa.stattools import coint\\ncluster_dict = {}\\nfor i, which_clust in enumerate(ticker_count_reduced.index):\\n    tickers = clustered_series[clustered_series == which_clust].index\\n    score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(\\n        dataset[tickers]\\n    )\\n    cluster_dict[which_clust] = {}\\n    cluster_dict[which_clust][\\'score_matrix\\'] = score_matrix\\n    cluster_dict[which_clust][\\'pvalue_matrix\\'] = pvalue_matrix\\n    cluster_dict[which_clust][\\'pairs\\'] = pairs\\n\\npairs = []\\nfor clust in cluster_dict.keys():\\n    pairs.extend(cluster_dict[clust][\\'pairs\\'])\\n\\nprint (\"Number of pairs found : %d\" % len(pairs))\\nprint (\"In those pairs, there are %d unique tickers.\" % len(np.unique(pairs)))\\n\\nOutput\\n\\nNumber of pairs found : 32\\nIn those pairs, there are 47 unique tickers.\\n\\nLet  us  visualize  the  results  of  the  pair  selection  process  now.  Refer  to  the  Jupyter\\nnotebook of this case study for the details of the steps related to the pair visualization\\nusing the t-SNE technique.\\n\\nThe  following  chart  shows  the  strength  of  k-means  for  finding  nontraditional  pairs\\n(pointed out with an arrow in the visualization). DXC is the ticker symbol for DXC\\nTechnology, and XEC is the ticker symbol for Cimarex Energy. These two stocks are\\nfrom  different  sectors  and  appear  to  have  nothing  in  common  on  the  surface,  but\\nthey are identified as pairs using k-means clustering and cointegration testing. This\\nimplies  that  a  long-run  stable  relationship  exists  between  their  stock  price  move‐\\nments.\\n\\n258 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cOnce  the  pairs  are  created,  they  can  be  used  in  a  pairs  trading  strategy.  When  the\\nshare prices of the pair deviate from the identified long-run relationship, an investor\\nwould seek to take a long position in the underperforming security and sell short the\\noutperforming security. If the securities return to their historical relationship, a profit\\nis made from the convergence of the prices.\\n\\nConclusion\\n\\nIn this case study, we demonstrated the efficiency of clustering techniques by finding\\nsmall pools of stocks in which to identify pairs to be used in a pairs trading strategy.\\nA  next  step  beyond  this  case  study  would  be  to  explore  and  backtest  various  long/\\nshort trading strategies with pairs of stocks from the groupings of stocks.\\n\\nClustering can be used for dividing stocks and other types of assets into groups with\\nsimilar characteristics for several other kinds of trading strategies. It can also be effec‐\\ntive in portfolio construction, helping to ensure we choose a pool of assets with suffi‐\\ncient diversification between them.\\n\\nCase Study 2: Portfolio Management: Clustering Investors\\nAsset management and investment allocation is a tedious and time-consuming pro‐\\ncess  in  which  investment  managers  often  must  design  customized  approaches  for\\neach client or investor.\\n\\nWhat  if  we  were  able  to  organize  these  clients  into  particular  investor  profiles,  or\\nclusters, wherein each group is indicative of investors with similar characteristics?\\n\\nCase Study 2: Portfolio Management: Clustering Investors \\n\\n| \\n\\n259\\n\\n\\x0cClustering investors based on similar characteristics can lead to simplicity and stand‐\\nardization  in  the  investment  management  process.  These  algorithms  can  group\\ninvestors  based  on  different  factors,  such  as  age,  income,  and  risk  tolerance.  It  can\\nhelp investment managers identify distinct groups within their investors base. Addi‐\\ntionally, by using these techniques, managers can avoid introducing any biases that\\notherwise  could  adversely  impact  decision  making.  The  factors  analyzed  through\\nclustering  can  have  a  big  impact  on  asset  allocation  and  rebalancing,  making  it  an\\ninvaluable tool for faster and effective investment management.\\n\\nIn  this  case  study,  we  will  use  clustering  methods  to  identify  different  types  of\\ninvestors.\\n\\nThe data used for this case study is from the Survey of Consumer Finances, which is\\nconducted by the Federal Reserve Board. The same dataset was used in “Case Study 3:\\nInvestor Risk Tolerance and Robo-Advisors” on page 125 in Chapter 5.\\n\\nIn this case study, we focus on:\\n\\n• Understanding the intuitive meaning of the groupings coming out of clustering.\\n\\n• Choosing the right clustering techniques.\\n\\n• Visualization of the clustering outcome and selecting the correct number of clus‐\\n\\nters in k-means.\\n\\nBlueprint for Using Clustering for Grouping Investors\\n\\n1. Problem definition\\n\\nThe  goal  of  this  case  study  is  to  build  a  clustering  model  to  group  individuals  or\\ninvestors based on parameters related to the ability and willingness to take risk. We\\nwill focus on using common demographic and financial characteristics to accomplish\\nthis.\\n\\nThe  survey  data  we’re  using  includes  responses  from  10,000+  individuals  in  2007\\n(precrisis) and 2009 (postcrisis). There are over 500 features. Since the data has many\\nvariables,  we  will  first  reduce  the  number  of  variables  and  select  the  most  intuitive\\nfeatures directly linked to an investor’s ability and willingness to take risk.\\n\\n260 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0c2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  packages  loaded  for  this  case  study  are  similar\\nto those loaded in the case study presented in Chapter 5. However, some additional\\npackages related to the clustering techniques are shown in the following code snippet:\\n\\n#Import packages for clustering techniques\\nfrom sklearn.cluster import KMeans, AgglomerativeClustering,AffinityPropagation\\nfrom sklearn.metrics import adjusted_mutual_info_score\\nfrom sklearn import cluster, covariance, manifold\\n\\n2.2. Loading the data.     The  data  (again,  previously  used  in  Chapter  5)  is  further  pro‐\\ncessed to give the following attributes that represent an individual’s ability and will‐\\ningness  to  take  risk.  This  preprocessed  data  is  for  the  2007  survey  and  is  loaded\\nbelow:\\n\\n# load dataset\\ndataset = pd.read_excel(\\'ProcessedData.xlsx\\')\\n\\n3. Exploratory data analysis\\n\\nNext, we take a closer look at the different columns and features found in the data.\\n\\n3.1. Descriptive statistics.    First, looking at the shape of the data:\\n\\ndataset.shape\\n\\nOutput\\n\\n(3866, 13)\\n\\nThe data has information for 3,886 individuals across 13 columns:\\n\\n# peek at data\\nset_option(\\'display.width\\', 100)\\ndataset.head(5)\\n\\nAs we can see in the table above, there are 12 attributes for each of the individuals.\\nThese  attributes  can  be  categorized  as  demographic,  financial,  and  behavioral\\nattributes. They are summarized in Figure 8-2.\\n\\nCase Study 2: Portfolio Management: Clustering Investors \\n\\n| \\n\\n261\\n\\n\\x0cFigure 8-2. Attributes for clustering individuals\\n\\nMany of these were previously used and defined in the Chapter 5 case study. A few\\nadditional attributes (LIFECYCL, HHOUSES, and SPENDMOR) are used in this case\\nstudy and are defined below:\\n\\nLIFECYCL\\n\\nThis is a lifecycle variable, used to approximate a person’s ability to take on risk.\\nThere are six categories in increasing level of ability to take risk. A value of 1 rep‐\\nresents “age under 55, not married, and no kids,” and a value of 6 represents “age\\nover 55 and not working.”\\n\\nHHOUSES\\n\\nThis is a flag indicating whether the individual is a homeowner. A value of 1 (0)\\nimplies the individual does (does not) own a home.\\n\\nSPENDMOR\\n\\nThis represents higher spending preference if assets appreciated on a scale of 1 to\\n5.\\n\\n3.2. Data visualization.     We  will \\npostclustering.\\n\\n4. Data preparation\\n\\ntake  a  detailed \\n\\nlook \\n\\ninto \\n\\nthe  visualization\\n\\nHere, we perform any necessary changes to the data in preparation for modeling.\\n\\n4.1. Data cleaning.    In this step, we check for NAs in the rows and either drop them or\\nfill them with the mean of the column:\\n\\nprint(\\'Null Values =\\', dataset.isnull().values.any())\\n\\nOutput\\n\\nNull Values = False\\n\\n262 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cGiven that there is not any missing data, and the data is already in categorical format,\\nno  further  data  cleaning  was  performed.  The  ID  column  is  unnecessary  and  is\\ndropped:\\n\\nX=X.drop([\\'ID\\'], axis=1)\\n\\n4.2. Data transformation.    As we saw in Section 3.1, all the columns represent categori‐\\ncal data with similar numeric scale, with no outliers. Hence, no data transformation\\nwill be required for clustering.\\n\\n5. Evaluate algorithms and models\\n\\nWe will analyze the performance of k-means and affinity propagation.\\n\\n5.1. k-means clustering.    We look at the details of the k-means clustering in this step.\\nFirst, we find the optimal number of clusters, followed by the creation of a model.\\n\\n5.1.1. Finding the optimal number of clusters.     We  look  at  the  following  two  metrics  to\\nevaluate the number of clusters in the k-means model. The Python code to get these\\ntwo metrics is the same as in case study 1:\\n\\n1. Sum of squared errors (SSE)\\n\\n2. Silhouette score\\n\\nSum of squared errors (SSE) within clusters\\n\\nCase Study 2: Portfolio Management: Clustering Investors \\n\\n| \\n\\n263\\n\\n\\x0cSilhouette score\\n\\nLooking at both of the preceding charts, the optimum number of clusters seems to be\\naround 7. We can see that as the number of clusters increases past 6, the SSE within\\nclusters begins to plateau. From the second graph, we can see that there are various\\nparts of the graph where a kink can be seen. Since there is not much of a difference in\\nthe SSE after 7 clusters, we proceed with using 7 clusters in the k-means model below.\\n\\n5.1.2. Clustering and visualization.    Let us create a k-means model with 7 clusters:\\n\\nnclust=7\\n\\n#Fit with k-means\\nk_means = cluster.KMeans(n_clusters=nclust)\\nk_means.fit(X)\\n\\nLet us assign a target cluster to each individual in the dataset. This assignment is used\\nfurther for exploratory data analysis to understand the behavior of each cluster:\\n\\n#Extracting labels\\ntarget_labels = k_means.predict(X)\\n\\n5.2. Affinity propagation.    Here, we build an affinity propagation model and look at the\\nnumber of clusters:\\n\\nap = AffinityPropagation()\\nap.fit(X)\\nclust_labels2 = ap.predict(X)\\n\\ncluster_centers_indices = ap.cluster_centers_indices_\\nlabels = ap.labels_\\n\\n264 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cn_clusters_ = len(cluster_centers_indices)\\nprint(\\'Estimated number of clusters: %d\\' % n_clusters_)\\n\\nOutput\\n\\nEstimated number of clusters: 161\\n\\nThe affinity propagation resulted in over 150 clusters. Such a large number will likely\\nmake it difficult to ascertain proper differentiation between them.\\n\\n5.3. Cluster evaluation.    In this step, we check the performance of the clusters using sil‐\\nhouette  coefficient  (sklearn.metrics.silhouette_score).  Recall  that  a  higher  silhouette\\ncoefficient score relates to a model with better defined clusters:\\n\\nfrom sklearn import metrics\\nprint(\"km\", metrics.silhouette_score(X, k_means.labels_))\\nprint(\"ap\", metrics.silhouette_score(X, ap.labels_))\\n\\nOutput\\n\\nkm 0.170585217843582\\nap 0.09736878398868973\\n\\nThe k-means model has a much higher silhouette coefficient compared to the affinity\\npropagation.  Additionally,  the  large  number  of  clusters  resulting  from  the  affinity\\npropagation is untenable. In the context of the problem at hand, having fewer clus‐\\nters, or categorizations of investors, helps build simplicity and standardization in the\\ninvestment management process. It gives the users of this information (e.g., financial\\nadvisors) some manageable intuition around the representation of the clusters. Com‐\\nprehending and being able to speak to six to eight investor types is much more prac‐\\ntical  than  maintaining  a  meaningful  understanding  of  over  100  different  profiles.\\nWith this in mind, we proceed with k-means as the preferred clustering technique.\\n\\n6. Cluster intuition\\n\\nIn the next step, we will analyze the clusters and attempt to draw conclusions from\\nthem. We do that by plotting the average of each variable of the cluster and summa‐\\nrizing the findings:\\n\\ncluster_output= pd.concat([pd.DataFrame(X),  pd.DataFrame(k_means.labels_, \\\\\\n  columns = [\\'cluster\\'])],axis=1)\\noutput=cluster_output.groupby(\\'cluster\\').mean()\\n\\nDemographics Features: Plot for each of the clusters\\n\\noutput[[\\'AGE\\',\\'EDUC\\',\\'MARRIED\\',\\'KIDS\\',\\'LIFECL\\',\\'OCCAT\\']].\\\\\\nplot.bar(rot=0, figsize=(18,5));\\n\\nCase Study 2: Portfolio Management: Clustering Investors \\n\\n| \\n\\n265\\n\\n\\x0cOutput\\n\\nThe plot here shows the average values of the attributes for each of the clusters (full\\nsize version available on GitHub). For example, in comparing clusters 0 and 1, cluster\\n0 has lower average age, yet higher average education. However, these two clusters are\\nmore similar in marital status and number of children. So, based on the demographic\\nattributes,  the  individuals  in  cluster  0  will,  on  average,  have  higher  risk  tolerance\\ncompared to those in cluster 1.\\n\\nFinancial and Behavioral Attributes: Plot for each of the clusters\\n\\noutput[[\\'HHOUSES\\',\\'NWCAT\\',\\'INCCL\\',\\'WSAVED\\',\\'SPENDMOR\\',\\'RISK\\']].\\\\\\nplot.bar(rot=0, figsize=(18,5));\\n\\nOutput\\n\\nThe  plot  here  shows  the  average  values  of  the  financial  and  behavior  attributes  for\\neach of the clusters (full size version available on GitHub). Again, comparing clusters\\n0  and  1,  the  former  has  higher  average  house  ownership,  higher  average  net  worth\\nand income, and a lower willingness to take risk compared to the latter. In terms of\\nsaving versus income comparison and willingness to save, the two clusters are com‐\\nparable. Therefore, we can posit that the individuals in cluster 0 will, on average, have\\na higher ability and yet a lower willingness to take risks compared to the individuals\\nin cluster 1.\\n\\n266 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cCombining  the  information  from  the  demographics,  financial,  and  behavioral\\nattributes for these two clusters, the overall ability to take risks for an individual in\\ncluster 0 is higher than someone in cluster 1. Performing similar analyses across all\\nother  clusters,  we  summarize  the  results  in  the  table  below.  The  risk  tolerance  col‐\\numn represents the subjective assessment of the risk tolerance of each cluster.\\n\\nCluster\\nCluster 0\\n\\nFeatures\\nLow age, high net worth and income, less risky life category, willingness to spend more\\n\\nRisk capacity\\nHigh\\n\\nCluster 1 High age, low net worth and income, highly risky life category, willingness to take risk, low\\n\\nLow\\n\\neducation\\n\\nCluster 2 High age, high net worth and income, highly risky life category, willingness to take risk, owns\\n\\nMedium\\n\\nhome\\n\\nCluster 3\\n\\nLow age, very low income and net worth, high willingness to take risk, many kids\\n\\nCluster 4 Medium age, very high income and net worth, high willingness to take risk, many kids, owns\\n\\nhome\\n\\nCluster 5\\n\\nLow age, very low income and net worth, high willingness to take risk, no kids\\n\\nLow\\n\\nHigh\\n\\nMedium\\n\\nCluster 6\\n\\nLow age, medium income and net worth, high willingness to take risk, many kids, owns home\\n\\nLow\\n\\nConclusion\\n\\nOne of the key takeaways from this case study is the approach to understanding the\\ncluster  intuition.  We  used  visualization  techniques  to  understand  the  expected\\nbehavior  of  a  cluster  member  by  qualitatively  interpreting  mean  values  of  the  vari‐\\nables in each cluster. We demonstrated the efficiency of clustering in discovering the\\nnatural groups of different investors based on their risk tolerance.\\n\\nGiven that clustering algorithms can successfully group investors based on different\\nfactors (such as age, income, and risk tolerance), they can be further used by portfolio\\nmanagers to standardize portfolio allocation and rebalance strategies across the clus‐\\nters, making the investment management process faster and more effective.\\n\\nCase Study 3: Hierarchical Risk Parity\\nMarkowitz’s mean-variance portfolio optimization is the most commonly used tech‐\\nnique  for  portfolio  construction  and  asset  allocation.  In  this  technique,  we  need  to\\nestimate the covariance matrix and expected returns of assets to be used as inputs. As\\ndiscussed  in  “Case  Study  1:  Portfolio  Management:  Finding  an  Eigen  Portfolio”  on\\npage 202 in Chapter 7, the erratic nature of financial returns causes estimation errors\\nin  the  expected  returns  and  the  covariance  matrix,  especially  when  the  number  of\\nassets is large compared to the sample size. These errors greatly jeopardize the opti‐\\nmality  of  the  resulting  portfolios,  which  leads  to  erroneous  and  unstable  results.\\nAdditionally, small changes in the assumed asset returns, volatilities, or covariances\\n\\nCase Study 3: Hierarchical Risk Parity \\n\\n| \\n\\n267\\n\\n\\x0ccan  lead  to  large  effects  on  the  output  of  the  optimization  procedure.  In  this  sense,\\nthe  Markowitz  mean-variance  optimization  is  an  ill-posed  (or  ill-conditioned)\\ninverse problem.\\n\\nIn  “Building  Diversified  Portfolios  That  Outperform  Out-of-Sample”  by  Marcos\\nLópez de Prado (2016), the author proposes a portfolio allocation method based on\\nclustering called hierarchical risk parity. The main idea of hierarchical risk parity is to\\nrun hierarchical clustering on the covariance matrix of stock returns and then find a\\ndiversified weighting by distributing capital equally to each cluster hierarchy (so that\\nmany correlated strategies will receive the same total allocation as a single uncorrela‐\\nted one). This alleviates some of the issues (highlighted above) found in Markowitz’s\\nmean-variance optimization and improves numerical stability.\\n\\nIn  this  case  study,  we  will  implement  hierarchical  risk  parity  based  on  clustering\\nmethods and compare it against Markowitz’s mean-variance optimization method.\\n\\nThe dataset used for this case study is price data for stocks in the S&P 500 from 2018\\nonwards. The dataset can be downloaded from Yahoo Finance. It is the same dataset\\nas was used in case study 1.\\n\\nIn this case study, we will focus on:\\n\\n• Application of clustering-based techniques for portfolio allocation.\\n\\n• Developing a framework for comparing portfolio allocation methods.\\n\\nBlueprint for Using Clustering to Implement Hierarchical\\nRisk Parity\\n\\n1. Problem definition\\n\\nOur  goal  in  this  case  study  is  to  use  a  clustering-based  algorithm  on  a  dataset  of\\nstocks to allocate capital into different asset classes. In order to backtest and compare\\nthe  portfolio  allocation  against  the  traditional  Markowitz  mean-variance  optimiza‐\\ntion, we will perform visualization and use performance metrics, such as the Sharpe\\nratio.\\n\\n268 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0c2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  packages  loaded  for  this  case  study  are  similar\\nto  those  loaded  in  the  previous  case  study.  However,  some  additional  packages\\nrelated to the clustering techniques are shown in the following code snippet:\\n\\n#Import Model Packages\\nimport scipy.cluster.hierarchy as sch\\nfrom sklearn.cluster import AgglomerativeClustering\\nfrom scipy.cluster.hierarchy import fcluster\\nfrom scipy.cluster.hierarchy import dendrogram, linkage, cophenet\\nfrom sklearn.metrics import adjusted_mutual_info_score\\nfrom sklearn import cluster, covariance, manifold\\nimport ffn\\n\\n#Package for optimization of mean variance optimization\\nimport cvxopt as opt\\nfrom cvxopt import blas, solvers\\n\\nSince this case study uses the same data as case study 1, some of the next steps (i.e.,\\nloading the data) have been skipped to avoid repetition. As a reminder, the data con‐\\ntains around 500 stocks and 448 observations.\\n\\n3. Exploratory data analysis\\n\\nWe  will  take  a  detailed  look  into  the  visualization  postclustering  later  in  this  case\\nstudy.\\n\\n4. Data preparation\\n\\n4.1. Data cleaning.    Refer to case study 1 for data cleaning steps.\\n\\n4.2. Data transformation.    We will be using annual returns for clustering. Additionally,\\nwe will train the data and then test the data. Here, we prepare the dataset for training\\nand testing by separating 20% of the dataset for testing, and we generate the return\\nseries:\\n\\nX= dataset.copy(\\'deep\\')\\nrow= len(X)\\ntrain_len = int(row*.8)\\n\\nX_train = X.head(train_len)\\nX_test = X.tail(row-train_len)\\n\\n#Calculate percentage return\\nreturns = X_train.to_returns().dropna()\\nreturns_test=X_test.to_returns().dropna()\\n\\nCase Study 3: Hierarchical Risk Parity \\n\\n| \\n\\n269\\n\\n\\x0c5. Evaluate algorithms and models\\n\\nIn this step, we will look at hierarchical clustering and perform further analysis and\\nvisualization.\\n\\n5.1. Building a hierarchy graph/dendrogram.    The first step is to look for clusters of cor‐\\nrelations  using  the  agglomerative  hierarchical  clustering  technique.  The  hierarchy\\nclass has a dendrogram method that takes the value returned by the linkage method\\nof the same class. The linkage method takes the dataset and the method to minimize\\ndistances as parameters. There are different options for measurement of the distance.\\nThe  option  we  will  choose  is  ward,  since  it  minimizes  the  variance  of  distances\\nbetween the clusters. Other possible measures of distance include single and centroid.\\n\\nLinkage does the actual clustering in one line of code and returns a list of the clusters\\njoined in the format:\\n\\nZ= [stock_1, stock_2, distance, sample_count]\\n\\nAs a precursor, we define a function to convert correlation into distances:\\n\\ndef correlDist(corr):\\n    # A distance matrix based on correlation, where 0<=d[i,j]<=1\\n    # This is a proper distance metric\\n    dist = ((1 - corr) / 2.) ** .5  # distance matrix\\n    return dist\\n\\nNow we convert the correlation of the returns of the stocks into distances, followed\\nby  the  computation  of  linkages  in  the  step  below.  Computation  of  linkages  is  fol‐\\nlowed by the visualization of the clusters through a dendrogram. Again, the leaves are\\nthe  individual  stocks,  and  the  root  is  the  final  single  cluster.  The  distance  between\\neach  cluster  is  shown  on  the  y-axis;  the  longer  the  branches  are,  the  less  correlated\\ntwo clusters are.\\n\\n#Calculate linkage\\ndist = correlDist(returns.corr())\\nlink = linkage(dist, \\'ward\\')\\n\\n#Plot Dendrogram\\nplt.figure(figsize=(20, 7))\\nplt.title(\"Dendrograms\")\\ndendrogram(link,labels = X.columns)\\nplt.show()\\n\\nIn the following chart, the horizontal axis represents the clusters. Although the names\\nof the stocks on the horizontal axis are not very clear (not surprising, given that there\\nare 500 stocks), we can see that they are grouped into several clusters. The appropri‐\\nate  number  of  clusters  appears  to  be  2,  3,  or  6,  depending  on  the  desired  distance\\nthreshold level. Next, we will leverage the linkages computed from this step to com‐\\npute the asset allocation based on hierarchical risk parity.\\n\\n270 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cOutput\\n\\n5.2. Steps for hierarchical risk parity.     The  hierarchical  risk  parity  (HRP)  algorithm\\nworks in three stages, as outlined in Prado’s paper:\\n\\nTree clustering\\n\\nGrouping  similar  investments  into  clusters  based  on  their  correlation  matrix.\\nHaving  a  hierarchical  structure  helps  us  improve  stability  issues  of  quadratic\\noptimizers when inverting the covariance matrix.\\n\\nQuasi-diagonalization\\n\\nReorganizing  the  covariance  matrix  so  similar  investments  will  be  placed\\ntogether.  This  matrix  diagonalization  allows  us  to  distribute  weights  optimally\\nfollowing an inverse-variance allocation.\\n\\nRecursive bisection\\n\\nDistributing  the  allocation  through  recursive  bisection  based  on  cluster\\ncovariance.\\n\\nHaving performed the first stage in the previous section, where we identified clusters\\nbased on the distance metrics, we proceed to quasi-diagonalization.\\n\\n5.2.1. Quasi-diagonalization.    Quasi-diagonalization is a process known as matrix seria‐\\ntion, which reorganizes the rows and columns of a covariance matrix so that the larg‐\\nest  values  lie  along  the  diagonal.  As  shown  in  the  following  code,  the  process\\nreorganizes  the  covariance  matrix  so  similar  investments  are  placed  together.  This\\nmatrix diagonalization allows us to distribute weights optimally following an inverse-\\nvariance allocation:\\n\\ndef getQuasiDiag(link):\\n    # Sort clustered items by distance\\n    link = link.astype(int)\\n    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\\n    numItems = link[-1, 3]  # number of original items\\n\\nCase Study 3: Hierarchical Risk Parity \\n\\n| \\n\\n271\\n\\n\\x0c    while sortIx.max() >= numItems:\\n        sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space\\n        df0 = sortIx[sortIx >= numItems]  # find clusters\\n        i = df0.index\\n        j = df0.values - numItems\\n        sortIx[i] = link[j, 0]  # item 1\\n        df0 = pd.Series(link[j, 1], index=i + 1)\\n        sortIx = sortIx.append(df0)  # item 2\\n        sortIx = sortIx.sort_index()  # re-sort\\n        sortIx.index = range(sortIx.shape[0])  # re-index\\n    return sortIx.tolist()\\n\\n5.2.2. Recursive bisection.    In the next step, we perform recursive bisection, which is a\\ntop-down  approach  to  splitting  portfolio  weights  between  subsets  based  on  the\\ninverse proportion to their aggregated variances. The function getClusterVar com‐\\nputes the cluster variance, and in this process, it requires the inverse-variance portfo‐\\nlio from the function getIVP. The output of the function getClusterVar is used by\\nthe  function  getRecBipart  to  compute  the  final  allocation  through  recursive  bisec‐\\ntion based on cluster covariance:\\n\\ndef getIVP(cov, **kargs):\\n# Compute the inverse-variance portfolio\\nivp = 1. / np.diag(cov)\\nivp /= ivp.sum()\\nreturn ivp\\n\\ndef getClusterVar(cov,cItems):\\n    # Compute variance per cluster\\n    cov_=cov.loc[cItems,cItems] # matrix slice\\n    w_=getIVP(cov_).reshape(-1, 1)\\n    cVar=np.dot(np.dot(w_.T,cov_),w_)[0, 0]\\n    return cVar\\n\\ndef getRecBipart(cov, sortIx):\\n    # Compute HRP alloc\\n    w = pd.Series(1, index=sortIx)\\n    cItems = [sortIx]  # initialize all items in one cluster\\n    while len(cItems) > 0:\\n        cItems = [i[j:k] for i in cItems for j, k in ((0,\\\\\\n           len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]  # bi-section\\n        for i in range(0, len(cItems), 2):  # parse in pairs\\n            cItems0 = cItems[i]  # cluster 1\\n            cItems1 = cItems[i + 1]  # cluster 2\\n            cVar0 = getClusterVar(cov, cItems0)\\n            cVar1 = getClusterVar(cov, cItems1)\\n            alpha = 1 - cVar0 / (cVar0 + cVar1)\\n            w[cItems0] *= alpha  # weight 1\\n            w[cItems1] *= 1 - alpha  # weight 2\\n    return w\\n\\n272 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cThe  following  function  getHRP  combines  the  three  stages—clustering,  quasi-\\ndiagonalization, and recursive bisection—to produce the final weights:\\n\\ndef getHRP(cov, corr):\\n    # Construct a hierarchical portfolio\\n    dist = correlDist(corr)\\n    link = sch.linkage(dist, \\'single\\')\\n    #plt.figure(figsize=(20, 10))\\n    #dn = sch.dendrogram(link, labels=cov.index.values)\\n    #plt.show()\\n    sortIx = getQuasiDiag(link)\\n    sortIx = corr.index[sortIx].tolist()\\n    hrp = getRecBipart(cov, sortIx)\\n    return hrp.sort_index()\\n\\n5.3. Comparison against other asset allocation methods.    A main focus of this case study is\\nto develop an alternative to Markowitz’s mean-variance portfolio optimization using\\nclustering. In this step, we define a function to compute the allocation of a portfolio\\nbased  on  Markowitz’s  mean-variance  technique.  This  function  (getMVP)  takes  the\\ncovariance  matrix  of  the  assets  as  an  input,  performs  the  mean-variance  optimiza‐\\ntion, and produces the portfolio allocations:\\n\\ndef getMVP(cov):\\n    cov = cov.T.values\\n    n = len(cov)\\n    N = 100\\n    mus = [10 ** (5.0 * t / N - 1.0) for t in range(N)]\\n\\n    # Convert to cvxopt matrices\\n    S = opt.matrix(cov)\\n    #pbar = opt.matrix(np.mean(returns, axis=1))\\n    pbar = opt.matrix(np.ones(cov.shape[0]))\\n\\n    # Create constraint matrices\\n    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix\\n    h = opt.matrix(0.0, (n, 1))\\n    A = opt.matrix(1.0, (1, n))\\n    b = opt.matrix(1.0)\\n\\n    # Calculate efficient frontier weights using quadratic programming\\n    solvers.options[\\'show_progress\\'] = False\\n    portfolios = [solvers.qp(mu * S, -pbar, G, h, A, b)[\\'x\\']\\n                  for mu in mus]\\n    ## Calculate risk and return of the frontier\\n    returns = [blas.dot(pbar, x) for x in portfolios]\\n    risks = [np.sqrt(blas.dot(x, S * x)) for x in portfolios]\\n    ## Calculate the 2nd degree polynomial of the frontier curve.\\n    m1 = np.polyfit(returns, risks, 2)\\n    x1 = np.sqrt(m1[2] / m1[0])\\n    # CALCULATE THE OPTIMAL PORTFOLIO\\n    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)[\\'x\\']\\n\\nCase Study 3: Hierarchical Risk Parity \\n\\n| \\n\\n273\\n\\n\\x0c    return list(wt)\\n\\n5.4. Getting the portfolio weights for all types of asset allocation.     In  this  step,  we  use  the\\nfunctions above to compute the asset allocation using the two asset allocation meth‐\\nods. We then visualize the asset allocation results:\\n\\ndef get_all_portfolios(returns):\\n\\n    cov, corr = returns.cov(), returns.corr()\\n    hrp = getHRP(cov, corr)\\n    mvp = getMVP(cov)\\n    mvp = pd.Series(mvp, index=cov.index)\\n    portfolios = pd.DataFrame([mvp, hrp], index=[\\'MVP\\', \\'HRP\\']).T\\n    return portfolios\\n\\n#Now getting the portfolios and plotting the pie chart\\nportfolios = get_all_portfolios(returns)\\n\\nportfolios.plot.pie(subplots=True, figsize=(20, 10),legend = False);\\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(30,20))\\nax1.pie(portfolios.iloc[:, 0], );\\nax1.set_title(\\'MVP\\',fontsize=30)\\nax2.pie(portfolios.iloc[:, 1]);\\nax2.set_title(\\'HRP\\',fontsize=30)\\n\\nThe  following  pie  charts  show  the  asset  allocation  of  MVP  versus  HRP.  We  clearly\\nsee more diversification in HRP. Now let us look at the backtesting results.\\n\\nOutput\\n\\n274 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0c6. Backtesting\\n\\nWe  will  now  backtest  the  performance  of  portfolios  produced  by  the  algorithms,\\nlooking at both in-sample and out-of-sample results:\\n\\nInsample_Result=pd.DataFrame(np.dot(returns,np.array(portfolios)), \\\\\\n\\'MVP\\',\\'HRP\\'], index = returns.index)\\nOutOfSample_Result=pd.DataFrame(np.dot(returns_test,np.array(portfolios)), \\\\\\ncolumns=[\\'MVP\\', \\'HRP\\'], index = returns_test.index)\\n\\nInsample_Result.cumsum().plot(figsize=(10, 5), title =\"In-Sample Results\",\\\\\\n                              style=[\\'--\\',\\'-\\'])\\nOutOfSample_Result.cumsum().plot(figsize=(10, 5), title =\"Out Of Sample Results\",\\\\\\n                                 style=[\\'--\\',\\'-\\'])\\n\\nOutput\\n\\nCase Study 3: Hierarchical Risk Parity \\n\\n| \\n\\n275\\n\\n\\x0cLooking at the charts, MVP underperforms for a significant amount of time in the in-\\nsample  test.  In  the  out-of-sample  test,  MVP  performed  better  than  HRP  for  a  brief\\nperiod of time from August 2019 to mid-September 2019. In the next step, we exam‐\\nine the Sharpe ratio for the two allocation methods:\\n\\nIn-sample results.   \\n\\n#In_sample Results\\nstddev = Insample_Result.std() * np.sqrt(252)\\nsharp_ratio = (Insample_Result.mean()*np.sqrt(252))/(Insample_Result).std()\\nResults = pd.DataFrame(dict(stdev=stddev, sharp_ratio = sharp_ratio))\\nResults\\n\\nOutput\\n\\nstdev\\n0.086\\n\\nsharp_ratio\\n0.785\\n\\n0.127\\n\\n0.524\\n\\nMVP\\n\\nHRP\\n\\nOut-of-sample results.   \\n\\n#OutOf_sample Results\\nstddev_oos = OutOfSample_Result.std() * np.sqrt(252)\\nsharp_ratio_oos = (OutOfSample_Result.mean()*np.sqrt(252))/(OutOfSample_Result).\\\\\\nstd()\\nResults_oos = pd.DataFrame(dict(stdev_oos=stddev_oos, sharp_ratio_oos = \\\\\\n  sharp_ratio_oos))\\nResults_oos\\n\\nOutput\\n\\nstdev_oos\\n0.103\\n\\nsharp_ratio_oos\\n0.787\\n\\n0.126\\n\\n0.836\\n\\nMVP\\n\\nHRP\\n\\nAlthough  the  in-sample  results  of  MVP  look  promising,  the  out-of-sample  Sharpe\\nratio and overall return of the portfolio constructed using the hierarchical clustering\\napproach are better. The diversification that HRP achieves across uncorrelated assets\\nmakes the methodology more robust against shocks.\\n\\nConclusion\\n\\nIn  this  case  study,  we  saw  that  portfolio  allocation  based  on  hierarchical  clustering\\noffers  better  separation  of  assets  into  clusters  with  similar  characteristics  without\\nrelying on classical correlation analysis used in Markowitz’s mean-variance portfolio\\noptimization.\\n\\n276 \\n\\n| \\n\\nChapter 8: Unsupervised Learning: Clustering\\n\\n\\x0cUsing  Markowitz’s  technique  yields  a  less  diverse  portfolio,  concentrated  in  a  few\\nstocks.  The  HRP  approach,  leveraging  hierarchical  clustering–based  allocation,\\nresults in a more diverse and distributed portfolio. This approach presented the best\\nout-of-sample  performance  and  offers  better  tail  risk  management  due  to  the\\ndiversification.\\n\\nIndeed,  the  corresponding  hierarchical  risk  parity  strategies  address  the  shortcom‐\\nings of minimum-variance-based portfolio allocation. It is visual and flexible, and it\\nseems  to  offer  a  robust  methodology  for  portfolio  allocation  and  portfolio\\nmanagement.\\n\\nChapter Summary\\nIn  this  chapter,  we  learned  about  different  clustering  techniques  and  used  them  to\\ncapture the natural structure of data to enhance decision making across several areas\\nof finance. Through the case studies, we demonstrated that clustering techniques can\\nbe useful in enhancing trading strategies and portfolio management.\\n\\nIn  addition  to  offering  an  approach  to  different  finance  problems,  the  case  studies\\nfocused  on  understanding  the  concepts  of  clustering  models,  developing  intuition,\\nand  visualizing  clusters.  Overall,  the  concepts  in  Python,  machine  learning,  and\\nfinance presented in this chapter through the case studies can used as a blueprint for\\nany other clustering-based problem in finance.\\n\\nHaving covered supervised and unsupervised learning, we will explore another type\\nof machine learning, reinforcement learning, in the next chapter.\\n\\nExercises\\n\\n• Use  hierarchical  clustering  to  form  clusters  of  investments  in  a  different  asset\\n\\nclass, such as forex or commodities.\\n\\n• Apply clustering analysis for pairs trading in the interest rate market on the uni‐\\n\\nverse of bonds.\\n\\nChapter Summary \\n\\n| \\n\\n277\\n\\n\\x0c\\x0cPART IV\\nReinforcement Learning and Natural\\nLanguage Processing\\n\\n\\x0c\\x0cCHAPTER 9\\nReinforcement Learning\\n\\nIncentives drive nearly everything, and finance is not an exception. Humans do not\\nlearn from millions of labeled examples. Instead, we often learn from positive or neg‐\\native experiences that we associate with our actions. Learning from experiences and\\nthe associated rewards or punishments is the core idea behind reinforcement learning\\n(RL).1\\n\\nReinforcement  learning  is  an  approach  toward  training  a  machine  to  find  the  best\\ncourse of action through optimal policies that maximize rewards and minimize pun‐\\nishments.\\n\\nThe RL algorithms that empowered AlphaGo (the first computer program to defeat a\\nprofessional human Go player) are also finding inroads into finance. Reinforcement\\nlearning’s main idea of maximizing the rewards aligns beautifully with several areas in\\nfinance,  including  algorithmic  trading  and  portfolio  management.  Reinforcement\\nlearning  is  particularly  suitable  for  algorithmic  trading,  because  the  concept  of  a\\nreturn-maximizing  agent  in  an  uncertain,  dynamic  environment  has  much  in  com‐\\nmon with an investor or a trading strategy that interacts with financial markets. Rein‐\\nforcement  learning–based  models  go  one  step  further  than  the  price  prediction–\\nbased trading strategies discussed in previous chapters and determine rule-based pol‐\\nicies for actions (i.e., place an order, do nothing, cancel an order, and so on).\\n\\nSimilarly,  in  portfolio  management  and  asset  allocation,  reinforcement  learning–\\nbased algorithms do not yield predictions and do not learn the structure of the mar‐\\nket implicitly. They do more. They directly learn the policy of changing the portfolio\\nallocation weights dynamically in the continuously changing market. Reinforcement\\nlearning  models  are  also  useful  for  order  execution  problems,  which  involve  the\\n\\n1 Reinforcement learning is also referred to as RL throughout this chapter.\\n\\n281\\n\\n\\x0cprocess  of  completing  a  buy  or  sell  order  for  a  market  instrument.  Here,  the  algo‐\\nrithms  learn  through  trial  and  error,  figuring  out  the  optimal  path  of  execution  on\\ntheir own.\\n\\nReinforcement  learning  algorithms,  with  their  ability  to  tackle  more  nuances  and\\nparameters  within  the  operational  environment,  can  also  produce  derivatives  hedg‐\\ning  strategies.  Unlike  traditional  finance-based  hedging  strategies,  these  hedging\\nstrategies are optimal and valid under real-world market frictions, such as transaction\\ncosts, market impact, liquidity constraints, and risk limits.\\n\\nIn  this  chapter,  we  cover  three  reinforcement  learning–based  case  studies  covering\\nmajor  finance  applications:  algorithmic  trading,  derivatives  hedging,  and  portfolio\\nallocation. In terms of the model development steps, the case studies follow a stan‐\\ndardized  seven-step  model  development  process  presented  in  Chapter  2.  Model\\ndevelopment and evaluation are key steps for reinforcement learning, and these steps\\nwill be emphasized. With multiple concepts in machine learning and finance imple‐\\nmented,  these  case  studies  can  be  used  as  a  blueprint  for  any  other  reinforcement\\nlearning–based problem in finance.\\n\\nIn “Case Study 1: Reinforcement Learning–Based Trading Strategy” on page 298, we\\ndemonstrate the use of RL to develop an algorithmic trading strategy.\\n\\nIn “Case Study 2: Derivatives Hedging” on page 316, we implement and analyze rein‐\\nforcement learning–based techniques to calculate the optimal hedging strategies for\\nportfolios of derivatives under market frictions.\\n\\nIn  “Case  Study  3:  Portfolio  Allocation”  on  page  334,  we  illustrate  the  use  of  a\\nreinforcement  learning–based  technique  on  a  dataset  of  cryptocurrency  in  order  to\\nallocate capital into different cryptocurrencies to maximize risk-adjusted returns. We\\nalso  introduce  a  reinforcement  learning–based  simulation  environment  to  train  and\\ntest the model.\\n\\nIn  addition  to  the  points  mentioned  above,  readers  will  understand  the  following\\npoints by the end of this chapter:\\n\\n• Key  components  of  reinforcement  learning  (i.e.,  reward,  agent,  environment,\\n\\naction, and policy).\\n\\n• Model-based  and  model-free  algorithms  for  reinforcement  learning  along  with\\n\\npolicy and value-based models.\\n\\n• Fundamental  approaches  to  solving  reinforcement  learning  problems,  such  as\\nMarkov decision processes (MDP), temporal difference (TD) learning, and artifi‐\\ncial neural networks (ANNs).\\n\\n• Methods  to  train  and  test  value-based  and  policy-based  reinforcement  learning\\n\\nalgorithms using artificial neural networks and deep learning.\\n\\n282 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0c• How  to  set  up  an  agent  or  simulation  environment  for  reinforcement  learning\\n\\nproblems using Python.\\n\\n• How to design and implement a problem statement related to algorithmic trad‐\\ning strategy, portfolio management, and instrument hedging in a classification-\\nbased machine learning framework.\\n\\nThis Chapter’s Code Repository\\n\\nA Python-based Jupyter notebook for all the case studies presented\\nin this chapter is included under the folder Chapter 9 - Reinforce‐\\nment  Learning  in  the  code  repository  for  this  book.  To  work\\nthrough  any  machine  learning  problems  in  Python  involving  RL\\nmodels (such as DQN or policy gradient) presented in this chapter,\\nreaders  need  to  modify  the  template  slightly  to  align  with  their\\nproblem statement.\\n\\nReinforcement Learning—Theory and Concepts\\nReinforcement learning is an extensive topic covering a wide range of concepts and\\nterminology. The theory section of this chapter covers the items and topics listed in\\nFigure 9-1.2\\n\\nFigure 9-1. RL summary of concepts\\n\\nIn order to solve any problem using RL, it is important to first understand and define\\nthe RL components.\\n\\n2 For more details, be sure to check out Reinforcement Learning: An Introduction by Richard Sutton and\\n\\nAndrew Barto (MIT Press), or David Silver’s free online RL course at University College London.\\n\\nReinforcement Learning—Theory and Concepts \\n\\n| \\n\\n283\\n\\n\\x0cRL Components\\nThe  main  components  of  an  RL  system  are  agent,  actions,  environment,  state,  and\\nreward.\\n\\nAgent\\n\\nThe entity that performs actions.\\n\\nActions\\n\\nThe things an agent can do within its environment.\\n\\nEnvironment\\n\\nThe world in which the agent resides.\\n\\nState\\n\\nThe current situation.\\n\\nReward\\n\\nThe immediate return sent by the environment to evaluate the last action by the\\nagent.\\n\\nThe  goal  of  reinforcement  learning  is  to  learn  an  optimal  strategy  through  experi‐\\nmental  trials  and  relatively  simple  feedback  loops.  With  the  optimal  strategy,  the\\nagent  is  capable  of  actively  adapting  to  the  environment  to  maximize  the  rewards.\\nUnlike in supervised learning, these reward signals are not given to the model imme‐\\ndiately. Instead, they are returned as a consequence of a sequence of actions that the\\nagent makes.\\n\\nAn agent’s actions are usually conditioned on what the agent perceives from the envi‐\\nronment. What the agent perceives is referred to as the observation or the state of the\\nenvironment.  Figure  9-2  summarizes  the  components  of  a  reinforcement  learning\\nsystem.\\n\\nFigure 9-2. RL components\\n\\n284 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cThe  interaction  between  the  agent  and  the  environment  involves  a  sequence  of\\nactions  and  observed  rewards  in  time,  t = 1, 2...T .  During  the  process,  the  agent\\naccumulates knowledge about the environment, learns the optimal policy, and makes\\ndecisions on which action to take next so as to efficiently learn the best policy. Let’s\\nlabel the state, action, and reward at time step t as St, At...Rt, respectively. Thus, the\\ninteraction sequence is fully described by one episode (also known as “trial” or “tra‐\\njectory”), and the sequence ends at the terminal state ST : S1, A1, R2, S2, A2...AT .\\n\\nIn addition to the five components of reinforcement learning mentioned so far, there\\nare  three  additional  components  of  reinforcement  learning:  policy,  value  function\\n(and  Q-value),  and  model  of  the  environment.  Let  us  discuss  the  components  in\\ndetail.\\n\\nPolicy\\n\\nA policy is an algorithm or a set of rules that describes how an agent makes its deci‐\\nsions. More formally, a policy is a function, usually denoted as π, that maps a state (s)\\nand an action (a):\\n\\nat = π(st)\\n\\nThis means that an agent decides its action given its current state. The policy can be\\ncan  be  either  deterministic  or  stochastic.  A  deterministic  policy  maps  a  state  to\\nactions. On the other hand, a stochastic policy outputs a probability distribution over\\nactions. It means that instead of being sure of taking action a, there is a probability\\nassigned to the action given a state.\\n\\nOur  goal  in  reinforcement  learning  is  to  learn  an  optimal  policy  (which  is  also\\nreferred to as π *). An optimal policy tells us how to act to maximize return in every\\nstate.\\n\\nValue function (and Q-value)\\n\\nThe  goal  of  a  reinforcement  learning  agent  is  to  learn  to  perform  a  task  well  in  an\\nenvironment. Mathematically, this means maximizing the future reward, or cumula‐\\ntive  discounted  reward,  G,  which  can  be  expressed  in  the  following  equation  as  a\\nfunction of reward function R at different times:\\n\\nGt = Rt +1 + γRt +2 + ... = ∑\\n\\n∞\\n\\ny k Rt +k +1\\n\\n0\\n\\nThe  discounting  factor  γ  is  a  value  between  0  and  1  to  penalize  the  rewards  in  the\\nfuture,  as  future  rewards  do  not  provide  immediate  benefits  and  may  have  higher\\nuncertainty. Future reward is an important input to the value function.\\n\\nReinforcement Learning—Theory and Concepts \\n\\n| \\n\\n285\\n\\n\\x0cThe  value  function  (or  state  value)  measures  the  attractiveness  of  a  state  through  a\\nprediction of future reward Gt. The value function of a state s is the expected return,\\nwith a policy π if we are in this state at time t:\\n\\nV (s) = E Gt | St = s\\n\\nSimilarly,  we  define  the  action-value  function  (Q-value)  of  a  state-action  pair  (s, a)\\nas:\\n\\nQ(s, a) = E Gt | St = s, At = a\\n\\nSo the value function is the expected return for a state following a policy π. The Q-\\nvalue is the expected reward for the state-action pair following a policy π.\\n\\nThe value function and the Q-value are interconnected as well. Since we follow the\\ntarget policy π, we can make use of the probability distribution over possible actions\\nand the Q-values to recover the value function:\\n\\nV (s) = ∑\\na∈A\\n\\nQ(s, a)π(a | s)\\n\\nThe  preceding  equation  represents  the  relationship  between  the  value  function  and\\nQ-value.\\n\\nThe  relationship  between  reward  function  (R),  future  rewards  (G),  value  function,\\nand Q-value is used to derive the Bellman equations (discussed later in this chapter),\\nwhich are one of the key components of many reinforcement learning models.\\n\\nModel\\n\\nThe model is a descriptor of the environment. With the model, we can learn or infer\\nhow the environment would interact with and provide feedback to the agent. Models\\nare used for planning, by which we mean any way of deciding on a course of action\\nby considering possible future situations. A model of the stock market, for example,\\nis tasked with predicting what the prices will look like in the future. The model has\\ntwo major parts: transition probability function (P) and reward function. We already\\ndiscussed the reward function. The transition function (P) records the probability of\\ntransitioning from one state to another after taking an action.\\n\\nOverall,  an  RL  agent  may  be  directly  or  indirectly  trying  to  learn  a  policy  or  value\\nfunction shown in Figure 9-3. The approach to learning a policy varies depending on\\nthe RL model type. When we fully know the environment, we can find the optimal\\n\\n286 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0csolution by using model-based approaches.3 When we do not know the environment,\\nwe follow a model-free approach and try to learn the model explicitly as part of the\\nalgorithm.\\n\\nFigure 9-3. Model, value, and policy\\n\\nRL components in a trading context\\n\\nLet’s try to understand what the RL components correspond to in a trading setting:\\n\\nAgent\\n\\nThe agent is our trading agent. We can think of the agent as a human trader who\\nmakes  trading  decisions  based  on  the  current  state  of  the  exchange  and  their\\naccount.\\n\\nAction\\n\\nThere would be three actions: Buy, Hold, and Sell.\\n\\nReward function\\n\\nAn obvious reward function would be the realized PnL (Profit and Loss). Other\\nreward  functions  can  be  Sharpe  ratio  or  maximum  drawdown.4  There  can  be  a\\nwide range of complex reward functions that offer a trade-off between profit and\\nrisk.\\n\\nEnvironment\\n\\nThe environment in a trading context would be the exchange. In the case of trad‐\\ning  on  an  exchange,  we  do  not  observe  the  complete  state  of  the  environment.\\nSpecifically,  we  are  unaware  of  the  other  agents,  and  what  an  agent  observes  is\\nnot the true state of the environment but some derivation of it.\\n\\n3 See “Reinforcement Learning Models” on page 293 for more details on model-based and model-free\\n\\napproaches.\\n\\n4 A maximum drawdown is the maximum observed loss from peak to trough of a portfolio before a new peak is\\n\\nattained; it is an indicator of downside risk over a specified time period.\\n\\nReinforcement Learning—Theory and Concepts \\n\\n| \\n\\n287\\n\\n\\x0cThis is referred to as a partially observable Markov decision process (POMDP). This is\\nthe most common type of environment that we encounter in finance.\\n\\nRL Modeling Framework\\nIn this section, we describe the core framework of reinforcement learning used across\\nseveral RL models.\\n\\nBellman equations\\n\\nBellman equations refer to a set of equations that decompose the value function and\\nQ-value into the immediate reward plus the discounted future values.\\n\\nIn  RL,  the  main  aim  of  an  agent  is  to  get  the  most  expected  sum  of  rewards  from\\nevery state it lands in. To achieve that, we must try to get the optimal value function\\nand Q-value; the Bellman equations help us to do so.\\n\\nWe use the relationship between reward function (R), future rewards (G), value func‐\\ntion,  and  Q-value  to  derive  the  Bellman  equation  for  value  function,  as  shown  in\\nEquation 9-1.\\n\\nEquation 9-1. Bellman equation for value function\\n\\nV (s) = E Rt +1 + γV (St +1) | St = s\\n\\nHere,  the  value  function  is  decomposed  into  two  parts;  an  immediate  reward,  Rt +1,\\nand the discounted value of the successor state, γV (St +1), as shown in the preceding\\nequation. Hence, we have broken down the problem into the immediate reward and\\nthe  discounted  successor  state.  The  state  value  V(s)  for  the  state  s  at  time  t  can  be\\ncomputed using the current reward Rt +1 and the value function at the time t+1. This\\nis the Bellman equation for value function. This equation can be maximized to get an\\nequation  called  Bellman  Optimality  Equation  for  value  function,  represented  by\\nV*(s).\\n\\nWe  follow  a  very  similar  algorithm  to  estimate  the  optimal  state-action  values  (Q-\\nvalues). The simplified iteration algorithms for value function and Q-value are shown\\nin Equations 9-2 and 9-3.\\n\\nEquation 9-2. Iteration algorithm for value function\\n\\nV k +1(s) = m\\na\\n\\nax∑\\ns\\n\\n′\\n\\nPs s\\n\\na (Rs s\\n\\na + γV k(s ′))\\n\\n′\\n\\n′\\n\\nEquation 9-3. Iteration algorithm for Q-value\\n\\nQk +1(s, a) = ∑\\n\\n′\\n\\ns\\n\\nPss\\n\\na Rs s\\n\\n′\\n\\n′\\n\\na + γ * m\\na\\n\\n′\\n\\nax * Qk(s ′, a ′)\\n\\n288 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cwhere\\n\\n• Pss ′\\n\\na   is  the  transition  probability  from  state  s  to  state  s′,  given  that  action  a  was\\n\\nchosen.\\n\\n• Rss ′\\n\\na  is the reward that the agent gets when it goes from state s to state s′, given\\n\\nthat action a was chosen.\\n\\nBellman equations are important because they let us express values of states as values\\nof other states. This means that if we know the value function or Q-value of st+1, we\\ncan  very  easily  calculate  the  value  of  st.  This  opens  a  lot  of  doors  for  iterative\\napproaches for calculating the value for each state, since if we know the value of the\\nnext state, we can know the value of the current state.\\n\\nIf  we  have  complete  information  about  the  environment,  the  iteration  algorithms\\nshown in Equations 9-2 and 9-3 turn into a planning problem, solvable by dynamic\\nprogramming  that  we  will  demonstrate  in  the  next  section.  Unfortunately,  in  most\\nscenarios, we do not know  Rss ′ or  Pss ′ and thus cannot apply the Bellman equations\\ndirectly, but they lay the theoretical foundation for many RL algorithms.\\n\\nMarkov decision processes\\n\\nAlmost all RL problems can be framed as Markov decision processes (MDPs). MDPs\\nformally  describe  an  environment  for  reinforcement  learning.  A  Markov  decision\\nprocess  consists  of  five  elements:  M = S, A, P, R, γ,  where  the  symbols  carry  the\\nsame meanings as defined in the previous section:\\n\\n• S: a set of states\\n\\n• A: a set of actions\\n\\n• P: transition probability\\n\\n• R: reward function\\n\\n• γ: discounting factor for future rewards\\n\\nMDPs  frame  the  agent–environment  interaction  as  a  sequential  decision  problem\\nover a series of time steps t = 1, …, T. The agent and the environment interact con‐\\ntinually, the agent selecting actions and the environment responding to these actions\\nand presenting new situations to the agent, with the aim of coming up with an opti‐\\nmal policy or strategy. Bellman equations form the basis for the overall algorithm.\\n\\nAll  states  in  MDP  have  the  Markov  property,  referring  to  the  fact  that  the  future\\ndepends only on the current state, not on the history.\\n\\nLet us look into an example of MDP in a financial context and analyze the Bellman\\nequation.  Trading  in  the  market  can  be  formalized  as  an  MDP,  which  is  a  process\\n\\nReinforcement Learning—Theory and Concepts \\n\\n| \\n\\n289\\n\\n\\x0cthat  has  specified  transition  probabilities  from  state  to  state.  Figure  9-4  shows  an\\nexample of MDP in the financial market, with a set of states, transition probability,\\naction, and reward.\\n\\nFigure 9-4. Markov decision process\\n\\nThe  MDP  presented  here  has  three  states:  bull,  bear,  and  stagnant  market,  repre‐\\nsented by three states (s0, s1, s2). The three actions of a trader are hold, buy, and sell,\\nrepresented by a0, a1, a2, respectively. This is a hypothetical setup in which we assume\\nthat transition probabilities are known and the action of the trader leads to a change\\nin the state of the market. In the subsequent sections we will look at approaches for\\nsolving  RL  problems  without  making  such  assumptions.  The  chart  also  shows  the\\ntransition  probabilities  and  the  rewards  for  different  actions.  If  we  start  in  state  s0\\n(bull market), the agent can choose between actions a0, a1, a2 (sell, buy, or hold). If it\\nchooses action buy (a1), it remains in state s0 with certainty, and without any reward.\\nIt can thus decide to stay there forever if it wants. But if it chooses action hold (a0), it\\nhas  a  70%  probability  of  gaining  a  reward  of  +50,  and  remaining  in  state  s0.  It  can\\nthen try again to gain as much reward as possible. But at some point, it is going to\\nend  up  instead  in  state  s1  (stagnant  market).  In  state  s1  it  has  only  two  possible\\nactions: hold (a0) or buy (a1). It can choose to stay put by repeatedly choosing action\\na1, or it can choose to move on to state s2 (bear market) and get a negative reward of –\\n250.  In  state  s3  it  has  no  other  choice  than  to  take  action  buy  (a1),  which  will  most\\nlikely lead it back to state s0 (bull market), gaining a reward of +200 on the way.\\n\\nNow, by looking at this MDP, it is possible to come up with an optimal policy or a\\nstrategy to achieve the most reward over time. In state s0 it is clear that action a0 is the\\n\\n290 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cbest option, and in state s2 the agent has no choice but to take action a1, but in state s1\\nit is not obvious whether the agent should stay put (a0) or sell (a2).\\n\\nLet’s apply the following Bellman equation as per Equation 9-3 to get the optimal Q-\\nvalue:\\n\\nQk +1(s, a) = ∑\\n\\n′\\n\\ns\\n\\nPss\\n\\na Rss\\n\\n′\\n\\n′\\n\\na + γ * m\\na\\n\\n′\\n\\nax * Qk(s ′, a ′)\\n\\nimport numpy as np\\nnan=np.nan # represents impossible actions\\n#Array for transition probability\\nP = np.array([ # shape=[s, a, s\\']\\n[[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\\n[[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\\n[[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],\\n])\\n\\n# Array for the return\\nR = np.array([ # shape=[s, a, s\\']\\n[[50., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\\n[[50., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -250.]],\\n[[nan, nan, nan], [200., 0.0, 0.0], [nan, nan, nan]],\\n])\\n#Actions\\nA = [[0, 1, 2], [0, 2], [1]]\\n#The data already obtained from yahoo finance is imported.\\n\\n#Now let\\'s run the Q-Value Iteration algorithm:\\nQ = np.full((3, 3), -np.inf) # -inf for impossible actions\\nfor state, actions in enumerate(A):\\n    Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions\\ndiscount_rate = 0.95\\nn_iterations = 100\\nfor iteration in range(n_iterations):\\n    Q_prev = Q.copy()\\n    for s in range(3):\\n        for a in A[s]:\\n            Q[s, a] = np.sum([\\n                T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))\\n        for sp in range(3)])\\nprint(Q)\\n\\nOutput\\n\\n[[109.43230584 103.95749333  84.274035  ]\\n [  5.5402017          -inf   5.83515676]\\n [        -inf 269.30353051         -inf]]\\n\\nThis gives us the optimal policy (Q-value) for this MDP, when using a discount rate\\nof 0.95. Looking for the highest Q-value for each of the states: in a bull market (s0)\\n\\nReinforcement Learning—Theory and Concepts \\n\\n| \\n\\n291\\n\\n\\x0cchoose action hold (a0); in a stagnant market (s1) choose action sell (a2); and in a bear\\nmarket (s2) choose action buy (a1).\\n\\nThe  preceding  example  is  a  demonstration  of  a  dynamic  programming  (DP)  algo‐\\nrithm  for  obtaining  optimal  policy.  These  methods  make  an  unrealistic  assumption\\nof  complete  knowledge  of  the  environment  but  are  the  conceptual  foundations  for\\nmost other approaches.\\n\\nTemporal difference learning\\n\\nReinforcement learning problems with discrete actions can often be modeled as Mar‐\\nkov  decision  processes,  as  we  saw  in  the  previous  example,  but  in  most  cases  the\\nagent initially has no insight into the transition probabilities. It also does not know\\nwhat  the  rewards  are  going  to  be.  This  is  where  temporal  difference  (TD)  learning\\ncan be useful.\\n\\nA  TD  learning  algorithm  is  very  similar  to  the  value  iteration  algorithm  (Equation\\n9-2) based on the Bellman equation but is tweaked to take into account the fact that\\nthe  agent  has  only  partial  knowledge  of  the  MDP.  In  general,  we  assume  that  the\\nagent  initially  knows  only  the  possible  states  and  actions  and  nothing  more.  For\\nexample, the agent uses an exploration policy, a purely random policy, to explore the\\nMDP,  and  as  it  progresses,  the  TD  learning  algorithm  updates  the  estimates  of  the\\nstate values based on the transitions and rewards that are actually observed.\\n\\nThe key idea in TD learning is to update the value function V(St) toward an estimated\\nreturn  Rt +1 + γV (St +1)  (known  as  the  TD  target).  The  extent  to  which  we  want  to\\nupdate the value function is controlled by the learning rate hyperparameter α, which\\ndefines how aggressive we want to be when updating our value. When α is close to\\nzero,  we’re  not  updating  very  aggressively.  When  α  is  close  to  one,  we’re  simply\\nreplacing the old value with the updated value:\\n\\nV (st) ← V (st) + α(Rt +1 + γV (st +1) – V (st))\\n\\nSimilarly, for Q-value estimation:\\n\\nQ(st, at) ← Q(st, at) + α(Rt +1 + γQ(st +1, at +1) – Q(st, at))\\n\\nMany RL models use the TD learning algorithm that we will see in the next section.\\n\\nArtificial neural network and deep learning\\n\\nReinforcement learning models often leverage an artificial neural network and deep\\nlearning methods to approximate a value or policy function. That is, ANN can learn\\nto  map  states  to  values,  or  state-action  pairs  to  Q-values.  ANNs  use  coefficients,  or\\nweights, to approximate the function relating inputs to outputs. In the context of RL,\\n\\n292 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cthe learning of ANNs means finding the right weights by iteratively adjusting them in\\nsuch  a  way  that  the  rewards  are  maximized.  Refer  to  3  and  5  for  more  details  on\\nmethods related to ANN (including deep learning).\\n\\nReinforcement Learning Models\\nReinforcement  learning  can  be  categorized  into  model-based  and  model-free  algo‐\\nrithms,  based  on  whether  the  rewards  and  probabilities  for  each  step  are  readily\\naccessible.\\n\\nModel-based algorithms\\n\\nModel-based  algorithms  try  to  understand  the  environment  and  create  a  model  to\\nrepresent it. When the RL problem includes well-defined transition probabilities and\\na  limited  number  of  states  and  actions,  it  can  be  framed  as  a  finite  MDP  for  which\\ndynamic programming (DP) can compute an exact solution, similar to the previous\\nexample.5\\n\\nModel-free algorithms\\n\\nModel-free  algorithms  try  to  maximize  the  expected  reward  only  from  real  experi‐\\nence, without a model or prior knowledge. Model-free algorithms are used when we\\nhave  incomplete  information  about  the  model.  The  agent’s  policy  π(s)  provides  the\\nguideline on what is the optimal action to take in a certain state with the goal of max‐\\nimizing the total rewards. Each state is associated with a value function V(s) predict‐\\ning  the  expected  amount  of  future  rewards  we  are  able  to  receive  in  this  state  by\\nacting on the corresponding policy. In other words, the value function quantifies how\\ngood a state is. Model-free algorithms are further divided into value-based and policy-\\nbased. Value-based algorithms learn the state, or Q-value, by choosing the best action\\nin  a  state.  These  algorithms  are  generally  based  upon  temporal  difference  learning\\nthat we discussed in the RL framework section. Policy-based algorithms (also known\\nas direct policy search) directly learn an optimal policy that maps state to action (or\\ntries to approximate optimal policy, if true optimal policy is not attainable).\\n\\nIn  most  situations  in  finance,  we  do  not  fully  know  the  environment,  rewards,  or\\ntransition probabilities, and we must fall back to model-free algorithms and related\\napproaches.6 Hence, the focus of the next section and of the case studies will be the\\nmodel-free methods and related algorithms.\\n\\n5 If the state and action spaces of MDP are finite, then it is called a finite Markov decision process.\\n\\n6 The MDP example based on dynamic programming that was discussed in the previous section was an exam‐\\nple of a model-based algorithm. As seen there, example rewards and transition probabilities are needed for\\nsuch algorithms.\\n\\nReinforcement Learning—Theory and Concepts \\n\\n| \\n\\n293\\n\\n\\x0cFigure 9-5 shows a taxonomy of model-free reinforcement learning. We highly rec‐\\nommend that readers refer to Reinforcement Learning: An Introduction for a more in-\\ndepth understanding of the algorithms and the concepts.\\n\\nFigure 9-5. Taxonomy of RL models\\n\\nIn the context of model-free methods, temporal difference learning is one of the most\\nused  approaches.  In  TD,  the  algorithm  refines  its  estimates  based  on  its  own  prior\\nestimates. The value-based algorithms Q-learning and SARSA use this approach.\\n\\nModel-free  methods  often  leverage  an  artificial  neural  network  to  approximate  a\\nvalue  or  policy  function.  Policy  gradient  and  deep  Q-network  (DQN)  are  two  com‐\\nmonly used model-free algorithms that use artificial neutral networks. Policy gradi‐\\nent  is  a  policy-based  approach  that  directly  parameterizes  the  policy.  Deep  Q-\\nnetwork is a value-based method that combines deep learning with Q-learning, which\\nsets the learning objective to optimize the estimates of Q-value.7\\n\\nQ-Learning\\n\\nQ-learning is an adaptation of TD learning. The algorithm evaluates which action to\\ntake based on a Q-value (or action-value) function that determines the value of being\\nin a certain state and taking a certain action at that state. For each state-action pair (s,\\na), this algorithm keeps track of a running average of the rewards, R, which the agent\\ngets upon leaving the state s with action a, plus the rewards it expects to earn later.\\n\\n7 There are some models, such as the actor-critic model, that leverage both policy-based and value-based\\n\\nmethods.\\n\\n294 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cSince the target policy would act optimally, we take the maximum of the Q-value esti‐\\nmates for the next state.\\n\\nThe  learning  proceeds  off-policy—that  is,  the  algorithm  does  not  need  to  select\\nactions based on the policy that is implied by the value function alone. However, con‐\\nvergence  requires  that  all  state-action  pairs  continue  to  be  updated  throughout  the\\ntraining process, and a straightforward way to ensure that this occurs is to use an ε-\\ngreedy policy, which is defined further in the following section.\\n\\nThe steps of Q-learning are:\\n\\n1. At  time  step  t,  we  start  from  state  st  and  pick  an  action  according  to  Q-values,\\n\\nat = maxaQ(st, a).\\n\\n2. We apply an ε-greedy approach that selects an action randomly with a probability\\nof ε or otherwise chooses the best action according to the Q-value function. This\\nensures the exploration of new actions in a given state while also exploiting the\\nlearning experience.8\\n\\n3. With action at, we observe reward Rt+1 and get into the next state St+1.\\n4. We update the action-value function:\\n\\nQ(st, at) ← Q(st, at) + α(Rt +1 + γmax\\n\\nQ(st +1, at) – Q(st, at))\\n\\na\\n\\n5. We increment the time step, t = t+1, and repeat the steps.\\n\\nGiven enough iterations of the steps above, this algorithm will converge to the opti‐\\nmal Q-value.\\n\\nSARSA\\n\\nSARSA is also a TD learning–based algorithm. It refers to the procedure of updating\\nthe Q-value by following a sequence of ...St, At, Rt +1, St +1, At +1, .... The first two steps\\nof SARSA are similar to the steps of Q-learning. However, unlike Q-learning, SARSA\\nis an on-policy algorithm in which the agent grasps the optimal policy and uses the\\nsame  to  act.  In  this  algorithm,  the  policies  used  for  updating  and  for  acting  are  the\\nsame. Q-learning is considered an off-policy algorithm.\\n\\n8 Off-policy, ε-greedy, exploration, and exploitation are commonly used terms in RL and will be used in other\\n\\nsections and case studies as well.\\n\\nReinforcement Learning—Theory and Concepts \\n\\n| \\n\\n295\\n\\n\\x0cDeep Q-Network\\n\\nIn  the  previous  section,  we  saw  how  Q-learning  allows  us  to  learn  the  optimal  Q-\\nvalue function in an environment with discrete state actions using iterative updates\\nbased  on  the  Bellman  equation.  However,  Q-learning  may  have  the  following\\ndrawbacks:\\n\\n• In  cases  where  the  state  and  action  space  are  large,  the  optimal  Q-value  table\\n\\nquickly becomes computationally infeasible.\\n\\n• Q-learning may suffer from instability and divergence.\\n\\nTo address these shortcomings, we use ANNs to approximate Q-values. For example,\\nif we use a function with parameter θ to calculate Q-values, we can label the Q-value\\nfunction  as  Q(s,a;θ).  The  deep  Q-learning  algorithm  approximates  the  Q-values  by\\nlearning  a  set  of  weights,  θ,  of  a  multilayered  deep  Q-network  that  maps  states  to\\nactions. The algorithm aims to greatly improve and stabilize the training procedure\\nof Q-learning through two innovative mechanisms:\\n\\nExperience replay\\n\\nInstead of running Q-learning on state-action pairs as they occur during simula‐\\ntion or actual experience, the algorithm stores the history of state, action, reward,\\nand  next  state  transitions  that  are  experienced  by  the  agent  in  one  large  replay\\nmemory.  This  can  be  referred  to  as  a  mini-batch  of  observations.  During  Q-\\nlearning  updates,  samples  are  drawn  at  random  from  the  replay  memory,  and\\nthus one sample could be used multiple times. Experience replay improves data\\nefficiency, removes correlations in the observation sequences, and smooths over\\nchanges in the data distribution.\\n\\nPeriodically updated target\\n\\nQ  is  optimized  toward  target  values  that  are  only  periodically  updated.  The  Q-\\nnetwork is cloned and kept frozen as the optimization targets every C step (C is a\\nhyperparameter).  This  modification  makes  the  training  more  stable  as  it  over‐\\ncomes  the  short-term  oscillations.  To  learn  the  network  parameters,  the  algo‐\\nrithm  applies  gradient  descent9  to  a  loss  function  defined  as  the  squared\\ndifference  between  the  DQN’s  estimate  of  the  target  and  its  estimate  of  the  Q-\\nvalue of the current state-action pair, Q(s,a:θ). The loss function is as follows:\\n\\nL (θi) =  (r + γmax\\n\\n′\\n\\na\\n\\nQ(s ′, a ′ ; θi–1) – Q(s, a; θi))2\\n\\n9 Refer to Chapter 3 for more details on gradient descent.\\n\\n296 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cThe  loss  function  is  essentially  a  mean  squared  error  (MSE)  function,  where\\n(r + γmaxa ′ Q(s ′, a ′ ; θi–1)) represents the target value and Q s, a; θi  represents the\\npredicted value. θ are the weights of the network, which are computed when the loss\\nfunction is minimized. Both the target and the current estimate depend on the set of\\nweights,  underlining  the  distinction  from  supervised  learning,  in  which  targets  are\\nfixed prior to training.\\n\\nAn  example  of  the  DQN  for  the  trading  example  containing  buy,  sell,  and  hold\\nactions is represented in Figure 9-6. Here, we provide the network only the state (s)\\nas input, and we receive Q-values for all possible actions (i.e., buy, sell, and hold) at\\nonce. We will be using DQN in the first and third case studies of this chapter.\\n\\nFigure 9-6. DQN\\n\\nPolicy gradient\\n\\nPolicy gradient is a policy-based method in which we learn a policy function, π, which\\nis a direct map from each state to the best corresponding action at that state. It is a\\nmore straightforward approach than the value-based method, without the need for a\\nQ-value function.\\n\\nPolicy  gradient  methods  learn  the  policy  directly  with  a  parameterized  function\\nrespect  to  θ,  π(a|s;θ).  This  function  can  be  a  complex  function  and  might  require  a\\nsophisticated model. In policy gradient methods, we use ANNs to map state to action\\nbecause  they  are  efficient  at  learning  complex  functions.  The  loss  function  of  the\\nANN is the opposite of the expected return (cumulative future rewards).\\n\\nThe objective function of the policy gradient method can be defined as:\\n\\nJ (θ) = V πθ(S1) = πθ V 1\\n\\nwhere θ represents a set of weights of the ANN that maps states to actions. The idea\\nhere is to maximize the objective function and compute the weights (θ) of the ANN.\\n\\nReinforcement Learning—Theory and Concepts \\n\\n| \\n\\n297\\n\\n\\x0cSince this is a maximization problem, we optimize the policy by taking the gradient\\nascent (as opposed to gradient descent, which is used to minimize the loss function),\\nwith the partial derivative of the objective with respect to the policy parameter θ:\\n\\nθ ← θ +\\n\\n∂\\n∂θ\\n\\nJ (θ)\\n\\nUsing gradient ascent, we can find the best θ that produces the highest return. Com‐\\nputing the gradient numerically can be done by perturbing θ by a small amount ε in\\nthe kth dimension or by using an analytical approach for deriving the gradient.\\n\\nWe will be using the policy gradient method for case study 2 later in this chapter.\\n\\nKey Challenges in Reinforcement Learning\\nSo far, we have covered only what reinforcement learning algorithms can do. How‐\\never, several shortcomings are outlined below:\\n\\nResource efficiency\\n\\nCurrent  deep  reinforcement  learning  algorithms  require  vast  amounts  of  time,\\ntraining data, and computational resources in order to reach a desirable level of\\nproficiency.  Thus,  making  reinforcement  learning  algorithms  trainable  under\\nlimited resources will continue to be an important issue.\\n\\nCredit assignment\\n\\nIn RL, reward signals can occur significantly later than actions that contributed\\nto the result, complicating the association of actions with their consequences.\\n\\nInterpretability\\n\\nIn  RL,  it  is  relatively  difficult  for  a  model  to  provide  any  meaningful,  intuitive\\nrelationships  between  input  and  their  corresponding  output  that  can  be  easily\\nunderstood. Most advanced reinforcement learning algorithms incorporate deep\\nneural networks, which make interpretability even more difficult due to a large\\nnumber of layers and nodes inside the neural network.\\n\\nLet us look at the case studies now.\\n\\nCase Study 1: Reinforcement Learning–Based Trading\\nStrategy\\nAlgorithmic trading primarily has three components: policy development, parameter\\noptimization,  and  backtesting.  The  policy  determines  what  actions  to  take  based  on\\nthe current state of the market. Parameter optimization is performed using a search\\nover possible values of strategy parameters, such as thresholds or coefficients. Finally,\\n\\n298 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cbacktesting assesses the viability of a trading strategy by exploring how it would have\\nplayed out using historical data.\\n\\nRL is based around coming up with a policy to maximize the reward in a given envi‐\\nronment. Instead of needing to hand code a rule-based trading policy, RL learns one\\ndirectly.  There  is  no  need  to  explicitly  specify  rules  and  thresholds.  Their  ability  to\\ndecide  policy  on  their  own  makes  RL  models  very  suitable  machine  learning  algo‐\\nrithms to create automated algorithmic trading models, or trading bots.\\n\\nIn  terms  of  parameter  optimization  and  backtesting  steps,  RL  allows  for  end-to-end\\noptimization  and  maximizes  (potentially  delayed)  rewards.  Reinforcement  learning\\nagents are trained in a simulation, which can be as complex as desired. Taking into\\naccount latencies, liquidity, and fees, we can seamlessly combine the backtesting and\\nparameter optimization steps without needing to go through separate stages.\\n\\nAdditionally, RL algorithms learn powerful policies parameterized by artificial neural\\nnetworks.  RL  algorithms  can  also  learn  to  adapt  to  various  market  conditions  by\\nexperiencing  them  in  historical  data,  given  that  they  are  trained  over  a  long-time\\nhorizon  and  have  sufficient  memory.  This  allows  them  to  be  much  more  robust  to\\nchanging  markets  than  supervised  learning–based  trading  strategies,  which,  due  to\\nthe simplistic nature of the policy, may not have a parameterization powerful enough\\nto learn to adapt to changing market conditions.\\n\\nReinforcement learning, with its capability to easily handle policy, parameter optimi‐\\nzation, and backtesting, is ideal for the next wave of algorithmic trading. Anecdotally,\\nit  seems  that  several  of  the  more  sophisticated  algorithmic  execution  desks  at  large\\ninvestment  banks  and  hedge  funds  are  beginning  to  use  reinforcement  learning  to\\noptimize their decision making.\\n\\nIn this case study, we will create an end-to-end trading strategy based on reinforce‐\\nment learning. We will use the Q-learning approach with deep Q-network (DQN) to\\ncome up with a policy and an implementation of the trading strategy. As discussed\\nbefore, the name “Q-learning” is in reference to the Q(s, a) function, which returns\\nthe  expected  reward  based  on  the  state  s  and  provided  action  a.  In  addition  to\\ndeveloping a specific trading strategy, this case study will discuss the general frame‐\\nwork and components of a reinforcement learning–based trading strategy.\\n\\nIn this case study, we will focus on:\\n\\n• Understanding the key components of an RL framework from a trading strategy\\n\\nstandpoint.\\n\\n• Evaluating  the  Q-learning  method  of  RL  in  Python  by  defining  an  agent,  fol‐\\n\\nlowed by training and testing setup.\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n299\\n\\n\\x0c• Implementing  a  deep  neural  network  to  be  used  for  RL  problems  in  Python\\n\\nusing the Keras package.\\n\\n• Understanding the class structure of Python programming while implementing\\n\\nan RL-based model.\\n\\n• Understanding the intuition and interpretation of RL-based algorithms.\\n\\nBlueprint for Creating a Reinforcement Learning–Based\\nTrading Strategy\\n\\n1. Problem definition\\n\\nIn the reinforcement learning framework for this case study, the algorithm takes an\\naction (buy, sell, or hold) depending on the current state of the stock price. The algo‐\\nrithm is trained using a deep Q-learning model to perform the best action. The key\\ncomponents of the reinforcement learning framework for this case study are:\\n\\nAgent\\n\\nTrading agent.\\n\\nAction\\n\\nBuy, sell, or hold.\\n\\nReward function\\n\\nRealized profit and loss (PnL) is used as the reward function for this case study.\\nThe  reward  depends  on  the  action:  sell  (realized  profit  and  loss),  buy  (no\\nreward), or hold (no reward).\\n\\nState\\n\\nA sigmoid function10 of the differences of past stock prices for a given time win‐\\ndow  is  used  as  the  state.  State  St  is  described  as  (dt -τ+1, dt -1, dt),  where\\ndT = sigmoid(pt – pt –1),  pt is price at time t, and τ is the time window size. A sig‐\\nmoid  function  converts  the  differences  of  the  past  stock  prices  into  a  number\\nbetween zero and one, which helps to normalize the values to probabilities and\\nmakes the state simpler to interpret.\\n\\nEnvironment\\n\\nStock exchange or the stock market.\\n\\n10 Refer to Chapter 3 for more details on the sigmoid function.\\n\\n300 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cSelecting the RL Components for a Trading Strategy\\n\\nFormulating an intelligent behavior for a reinforcement learning–\\nbased  trading  strategy  begins  with  identification  of  the  correct\\ncomponents of the RL model. Hence, before we go into the model\\ndevelopment,  we  should  carefully  identify  the  following  RL  com‐\\nponents:\\n\\nReward function\\n\\nThis is an important parameter, as it decides whether the RL\\nalgorithm will learn to optimize the appropriate metric. In\\naddition to the return or PnL, the reward function can incor‐\\nporate risk embedded in the underlying instrument or include\\nother parameters such as volatility or maximum drawdown. It\\ncan also include the transaction costs of the buy/sell actions.\\n\\nState\\n\\nState determines the observations that the agent receives from\\nthe environment for taking a decision. The state should be\\nrepresentative of current market behavior as compared to the\\npast and can also include values of any signals that are\\nbelieved to be predictive or items related to market micro‐\\nstructure, such as volume traded.\\n\\nThe  data  that  we  will  use  will  be  the  S&P  500  closing  prices.  The  data  is  extracted\\nfrom Yahoo Finance and contains ten years of daily data from 2010 to 2019.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.    The list of libraries used for all of the steps of model\\nimplementation,  from  data  loading  to  model  evaluation,  including  deep  learning–\\nbased model development, are included here. The details of most of these packages\\nand functions have been provided in Chapters 2, 3, and 4. The packages used for dif‐\\nferent purposes have been separated in the Python code here, and their usage will be\\ndemonstrated in different steps of the model development process.\\n\\nPackages for reinforcement learning\\n\\nimport keras\\nfrom keras import layers, models, optimizers from keras import backend as K\\nfrom collections import namedtuple, deque\\nfrom keras.models import Sequential\\nfrom keras.models import load_model\\nfrom keras.layers import Dense\\nfrom keras.optimizers import Adam\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n301\\n\\n\\x0cPackages/modules for data processing and visualization\\n\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom pandas import read_csv, set_option\\nimport datetime\\nimport math\\nfrom numpy.random import choice\\nimport random\\nfrom collections import deque\\n\\n2.2. Loading the data.    The fetched data for the time period of 2010 to 2019 is loaded:\\n\\ndataset = read_csv(\\'data/SP500.csv\\', index_col=0)\\n\\n3. Exploratory data analysis\\n\\nWe will look at descriptive statistics and data visualization in this section. Let us have\\na look at the dataset we have:\\n\\n# shape\\ndataset.shape\\n\\nOutput\\n\\n(2515, 6)\\n\\n# peek at data\\nset_option(\\'display.width\\', 100)\\ndataset.head(5)\\n\\nOutput\\n\\nThe  data  has  a  total  of  2,515  rows  and  six  columns,  which  contain  the  categories\\nopen, high, low, close, adjusted close price, and total volume. The adjusted close price is\\nthe  closing  price  adjusted  for  the  split  and  dividends.  For  the  purpose  of  this  case\\nstudy, we will be focusing on the closing price.\\n\\n302 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cThe chart shows that S&P 500 has been in an upward-trending series between 2010\\nand 2019. Let us perform the data preparation.\\n\\n4. Data preparation\\n\\nThis step is important in order to create a meaningful, reliable, and clean dataset that\\ncan be used without any errors in the reinforcement learning algorithm.\\n\\n4.1. Data cleaning.    In this step, we check for NAs in the rows and either drop them or\\nfill them with the mean of the column:\\n\\n#Checking for any null values and removing the null values\\'\\'\\'\\nprint(\\'Null Values =\\', dataset.isnull().values.any())\\n\\nOutput\\n\\nNull Values = False\\n\\nAs there are no null values in the data, there is no need to perform any further data\\ncleaning.\\n\\n5. Evaluate algorithms and models\\n\\nThis is the key step of the reinforcement learning model development, where we will\\ndefine all the relevant functions and classes and train the algorithm. In the first step,\\nwe prepare the data for the training set and the test set.\\n\\n5.1. Train-test split.    In this step, we partition the original dataset into training set and\\ntest  set.  We  use  the  test  set  to  confirm  the  performance  of  our  final  model  and  to\\nunderstand  if  there  is  any  overfitting.  We  will  use  80%  of  the  dataset  for  modeling\\nand 20% for testing:\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n303\\n\\n\\x0cX=list(dataset[\"Close\"])\\nX=[float(x) for x in X]\\nvalidation_size = 0.2\\ntrain_size = int(len(X) * (1-validation_size))\\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\\n\\n5.2. Implementation steps and modules.    The overall algorithm of this case study (and of\\nreinforcement learning in general) is a bit complex as it requires building class-based\\ncode structure and the simultaneous use of many modules and functions. This addi‐\\ntional  section  was  added  for  this  case  study  to  provide  a  functional  explanation  of\\nwhat is happening in the program.\\n\\nThe algorithm, in simple terms, decides whether to buy, sell, or hold when provided\\nwith the current market price.\\n\\nFigure 9-7 provides an overview of the training of the Q-learning-based algorithm in\\nthe context of this case study. The algorithm evaluates which action to take based on\\na Q-value, which determines the value of being in a certain state and taking a certain\\naction at that state.\\n\\nAs  per  Figure  9-7,  the  state  (s)  is  decided  on  the  basis  of  the  current  and  historical\\nbehavior of the price (Pt, Pt–1,…). Based on the current state, the action is “buy.” With\\nthis action, we observe a reward of $10 (i.e., the PnL associated with the action) and\\nmove into the next state. Using the current reward and the next state’s Q-value, the\\nalgorithm updates the Q-value function. The algorithm keeps on moving through the\\nnext time steps. Given sufficient iterations of the steps above, this algorithm will con‐\\nverge to the optimal Q-value.\\n\\nFigure 9-7. Reinforcement learning for trading\\n\\nThe deep Q-network that we use in this case study uses an ANN to approximate Q-\\nvalues;  hence,  the  action  value  function  is  defined  as  Q(s,a;θ).  The  deep  Q-learning\\nalgorithm  approximates  the  Q-value  function  by  learning  a  set  of  weights,  θ,  of  a\\nmultilayered DQN that maps states to actions.\\n\\n304 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cModules and functions\\n\\nImplementing this DQN algorithm requires implementation of several functions and\\nmodules that interact with each other during the model training. Here is a summary\\nof the modules and functions:\\n\\nAgent class\\n\\nThe agent is defined as “Agent” class. This holds the variables and member func‐\\ntions that perform the Q-learning. An object of the Agent class is created using\\nthe training phase and is used for training the model.\\n\\nHelper functions\\n\\nIn this module, we create additional functions that are helpful for training.\\n\\nTraining module\\n\\nIn this step, we perform the training of the data using the variables and the func‐\\ntions  defined  in  the  agent  and  helper  methods.  During  training,  the  prescribed\\naction for each day is predicted, the rewards are computed, and the deep learn‐\\ning–based  Q-learning  model  weights  are  updated  iteratively  over  a  number  of\\nepisodes. Additionally, the profit and loss of each action is summed to determine\\nwhether an overall profit has occurred. The aim is to maximize the total profit.\\n\\nWe provide a deep dive into the interaction between the different modules and func‐\\ntions in “5.5. Training the model” on page 308.\\n\\nLet us look at each of these in detail.\\n\\n5.3. Agent class.    The agent class consists of the following components:\\n\\n• Constructor\\n• Function model\\n• Function act\\n• Function expReplay\\n\\nThe  Constructor  is  defined  as  init  function  and  contains  important  parameters\\nsuch  as  discount  factor  for  reward  function,  epsilon  for  the  ε-greedy  approach,\\nstate size, and  action size. The number of actions is set at three (i.e., buy, sell,\\nand hold). The memory variable defines the replay memory size. The input parameter\\nof this function also consists of is_eval parameter, which defines whether training is\\nongoing. This variable is changed to True during the evaluation/testing phase. Also, if\\nthe  pretrained  model  has  to  be  used  in  the  evaluation/training  phase,  it  is  passed\\nusing the model_name variable:\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n305\\n\\n\\x0cclass Agent:\\n    def __init__(self, state_size, is_eval=False, model_name=\"\"):\\n        self.state_size = state_size # normalized previous days\\n        self.action_size = 3 # hold, buy, sell\\n        self.memory = deque(maxlen=1000)\\n        self.inventory = []\\n        self.model_name = model_name\\n        self.is_eval = is_eval\\n\\n        self.gamma = 0.95\\n        self.epsilon = 1.0\\n        self.epsilon_min = 0.01\\n        self.epsilon_decay = 0.995\\n\\n        self.model = load_model(\"models/\" + model_name) if is_eval \\\\\\n         else self._model()\\n\\nThe  function  model  is  a  deep  learning  model  that  maps  the  states  to  actions.  This\\nfunction takes in the state of the environment and returns a Q-value table or a policy\\nthat refers to a probability distribution over actions. This function is built using the\\nKeras Python library.11 The architecture for the deep learning model used is:\\n\\n• The model expects rows of data with number of variables equal to the state size,\\n\\nwhich comes as an input.\\n\\n• The first, second, and third hidden layers have 64, 32, and 8 nodes, respectively,\\n\\nand all of these layers use the ReLU activation function.\\n\\n• The  output  layer  has  the  number  of  nodes  equal  to  the  action  size  (i.e.,  three),\\n\\nand the node uses a linear activation function:12\\n\\n    def _model(self):\\n        model = Sequential()\\n        model.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\\n        model.add(Dense(units=32, activation=\"relu\"))\\n        model.add(Dense(units=8, activation=\"relu\"))\\n        model.add(Dense(self.action_size, activation=\"linear\"))\\n        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\\n\\n        return model\\n\\nThe function act returns an action given a state. The function uses the model func‐\\ntion and returns a buy, sell, or hold action:\\n\\n11 The details of the Keras-based implementation of deep learning models are shown in Chapter 3.\\n\\n12 Refer to Chapter 3 for more details on the linear and ReLU activation functions.\\n\\n306 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0c    def act(self, state):\\n        if not self.is_eval and random.random() <= self.epsilon:\\n            return random.randrange(self.action_size)\\n\\n        options = self.model.predict(state)\\n        return np.argmax(options[0])\\n\\nThe  function  expReplay  is  the  key  function,  where  the  neural  network  is  trained\\nbased  on  the  observed  experience.  This  function  implements  the  Experience  replay\\nmechanism as previously discussed. Experience replay stores a history of state, action,\\nreward, and next state transitions that are experienced by the agent. It takes a mini‐\\nbatch  of  the  observations  (replay  memory)  as  an  input  and  updates  the  deep  learn‐\\ning–based  Q-learning  model  weights  by  minimizing  the  loss  function.  The  epsilon\\ngreedy  approach  implemented  in  this  function  prevents  overfitting.  In  order  to\\nexplain the function, different steps are numbered in the comments of the following\\nPython code, along with an outline of the steps:\\n\\n1. Prepare the replay buffer memory, which is the set of observation used for train‐\\ning. New experiences are added to the replay buffer memory using a for loop.\\n\\n2. Loop across all the observations of state, action, reward, and next state transitions\\n\\nin the mini-batch.\\n\\n3. The  target  variable  for  the  Q-table  is  updated  based  on  the  Bellman  equation.\\nThe update happens if the current state is the terminal state or the end of the epi‐\\nsode. This is represented by the variable done and is defined further in the train‐\\ning function. If it is not done, the target is just set to reward.\\n\\n4. Predict the Q-value of the next state using a deep learning model.\\n\\n5. The Q-value of this state for the action in the current replay buffer is set to the\\n\\ntarget.\\n\\n6. The deep learning model weights are updated by using the model.fit function.\\n\\n7. The epsilon greedy approach is implemented. Recall that this approach selects an\\naction  randomly  with  a  probability  of  ε  or  the  best  action,  according  to  the  Q-\\nvalue function, with probability 1–ε.\\n\\n    def expReplay(self, batch_size):\\n        mini_batch = []\\n        l = len(self.memory)\\n        #1: prepare replay memory\\n        for i in range(l - batch_size + 1, l):\\n            mini_batch.append(self.memory[i])\\n\\n        #2: Loop across the replay memory batch.\\n        for state, action, reward, next_state, done in mini_batch:\\n            target = reward # reward or Q at time t\\n            #3: update the target for Q table. table equation\\n            if not done:\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n307\\n\\n\\x0c                target = reward + self.gamma * \\\\\\n                 np.amax(self.model.predict(next_state)[0])\\n            #set_trace()\\n\\n            # 4: Q-value of the state currently from the table\\n            target_f = self.model.predict(state)\\n            # 5: Update the output Q table for the given action in the table\\n            target_f[0][action] = target\\n            # 6. train and fit the model.\\n            self.model.fit(state, target_f, epochs=1, verbose=0)\\n\\n        #7. Implement epsilon greedy algorithm\\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n\\n5.4. Helper functions.     In  this  module,  we  create  additional  functions  that  are  helpful\\nfor  training.  Some  of  the  important  helper  functions  are  discussed  here.  For  details\\nabout other helper functions, refer to the Jupyter notebook in the GitHub repository\\nfor this book.\\n\\nThe  function  getState  generates  the  states  given  the  stock  data,  time  t  (the  day  of\\nprediction), and window n (number of days to go back in time). First, the vector of\\nprice difference is computed, followed by scaling this vector from zero to one with a\\nsigmoid function. This is returned as the state.\\n\\ndef getState(data, t, n):\\n    d = t - n + 1\\n    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1]\\n    res = []\\n    for i in range(n - 1):\\n        res.append(sigmoid(block[i + 1] - block[i]))\\n    return np.array([res])\\n\\nThe function plot_behavior returns the plot of the market price along with indica‐\\ntors for the buy and sell actions. It is used for the overall evaluation of the algorithm\\nduring the training and testing phase.\\n\\ndef plot_behavior(data_input, states_buy, states_sell, profit):\\n    fig = plt.figure(figsize = (15, 5))\\n    plt.plot(data_input, color=\\'r\\', lw=2.)\\n    plt.plot(data_input, \\'^\\', markersize=10, color=\\'m\\', label=\\'Buying signal\\',\\\\\\n     markevery=states_buy)\\n    plt.plot(data_input, \\'v\\', markersize=10, color=\\'k\\', label=\\'Selling signal\\',\\\\\\n     markevery = states_sell)\\n    plt.title(\\'Total gains: %f\\'%(profit))\\n    plt.legend()\\n    plt.show()\\n\\n5.5. Training the model.     We  will  proceed  to  train  the  data.  Based  on  our  agent,  we\\ndefine the following variables and instantiate the stock agent:\\n\\n308 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cEpisode\\n\\nThe  number  of  times  the  code  is  trained  through  the  entire  data.  In  this  case\\nstudy, we use 10 episodes.\\n\\nWindows size\\n\\nNumber of market days to consider to evaluate the state.\\n\\nBatch size\\n\\nSize of the replay buffer or memory use during training.\\n\\nOnce these variables are defined, we train the model iterating through the episodes.\\nFigure 9-8 provides a deep dive into the training steps and brings together all the ele‐\\nments discussed so far. The upper section showing steps 1 to 7 describes the steps in\\nthe training module, and the lower section describes the steps in the replay buffer\\nfunction (i.e., exeReplay function).\\n\\nFigure 9-8. Training steps of Q-trading\\n\\nSteps 1 to 6 shown in Figure 9-8 are numbered in the following Python code and are\\ndescribed as follows:\\n\\n1. Get  the  current  state  using  the  helper  function  getState.  It  returns  a  vector  of\\nstates, where the length of the vector is defined by windows size and the values of\\nthe states are between zero and one.\\n\\n2. Get the action for the given state using the act function of the agent class.\\n\\n3. Get  the  reward  for  the  given  action.  The  mapping  of  the  action  and  reward  is\\n\\ndescribed in the problem definition section of this case study.\\n\\n4. Get the next state using the getState function. The detail of the next state is fur‐\\n\\nther used in the Bellman equation for updating the Q-function.\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n309\\n\\n\\x0c5. The  details  of  the  state,  next  state,  action,  etc.,  are  saved  in  the  memory  of  the\\nagent  object,  which  is  used  further  by  the  exeReply  function.  A  sample  mini-\\nbatch is as follows:\\n\\n6. Check  if  the  batch  is  complete.  The  size  of  a  batch  is  defined  by  the  batch  size\\nvariable. If the batch is complete, then we move to the Replay buffer function\\nand update the Q-function by minimizing the MSE between the Q-predicted and\\nthe Q-target. If not, then we move to the next time step.\\n\\nThe code produces the final results of each episode, along with the plot showing the\\nbuy and sell actions and the total profit for each episode of the training phase.\\n\\nwindow_size = 1\\nagent = Agent(window_size)\\nl = len(data) - 1\\nbatch_size = 10\\nstates_sell = []\\nstates_buy = []\\nepisode_count = 3\\n\\nfor e in range(episode_count + 1):\\n    print(\"Episode \" + str(e) + \"/\" + str(episode_count))\\n    # 1-get state\\n    state = getState(data, 0, window_size + 1)\\n\\n    total_profit = 0\\n    agent.inventory = []\\n\\n    for t in range(l):\\n        # 2-apply best action\\n        action = agent.act(state)\\n\\n        # sit\\n        next_state = getState(data, t + 1, window_size + 1)\\n        reward = 0\\n\\n        if action == 1: # buy\\n\\n310 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0c            agent.inventory.append(data[t])\\n            states_buy.append(t)\\n            print(\"Buy: \" + formatPrice(data[t]))\\n\\n        elif action == 2 and len(agent.inventory) > 0: # sell\\n            bought_price = agent.inventory.pop(0)\\n             #3: Get Reward\\n\\n            reward = max(data[t] - bought_price, 0)\\n            total_profit += data[t] - bought_price\\n            states_sell.append(t)\\n            print(\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" \\\\\\n            + formatPrice(data[t] - bought_price))\\n\\n        done = True if t == l - 1 else False\\n        # 4: get next state to be used in bellman\\'s equation\\n        next_state = getState(data, t + 1, window_size + 1)\\n\\n        # 5: add to the memory\\n        agent.memory.append((state, action, reward, next_state, done))\\n        state = next_state\\n\\n        if done:\\n\\n            print(\"--------------------------------\")\\n            print(\"Total Profit: \" + formatPrice(total_profit))\\n            print(\"--------------------------------\")\\n\\n        # 6: Run replay buffer function\\n        if len(agent.memory) > batch_size:\\n            agent.expReplay(batch_size)\\n\\n    if e % 10 == 0:\\n        agent.model.save(\"models/model_ep\" + str(e))\\n\\nOutput\\n\\nRunning episode 0/10\\nTotal Profit: $6738.87\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n311\\n\\n\\x0cRunning episode 1/10\\nTotal Profit: –$45.07\\n\\nRunning episode 9/10\\nTotal Profit: $1971.54\\n\\nRunning episode 10/10\\nTotal Profit: $1926.84\\n\\n312 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cThe charts show the details of the buy/sell pattern and the total gains of the first two\\n(zero and one) and last two (9 and 10) episodes. The details of other episodes can be\\nseen in Jupyter notebook under the GitHub repository for this book.\\n\\nAs we can see, in the beginning of episodes 0 and 1, since the agent has no precon‐\\nception of the consequences of its actions, it takes randomized actions to observe the\\nrewards  associated  with  it.  In  episode  zero,  there  is  an  overall  profit  of  $6,738,  a\\nstrong result indeed, but in episode one we experience an overall loss of $45. The fact\\nthat the cumulative reward per episode fluctuates substantially in the beginning illus‐\\ntrates the exploration process the algorithm is going through. Looking at episodes 9\\nand 10, it seems as though the agent begins learning from its training. It discovers the\\nstrategy and starts to exploit it consistently. The buy and sell actions of these last two\\nepisodes lead a PnL that is perhaps less than that of episode zero, but far more robust.\\nThe  buy  and  sell  actions  in  the  later  episodes  have  been  performed  uniformly  over\\nthe entire time period, and the overall profit is stable.\\n\\nIdeally, the number of training episodes should be higher than the number used in\\nthis  case  study.  A  higher  number  of  training  episodes  will  lead  to  a  better  training\\nperformance.  Before  we  move  on  to  the  testing,  let  us  go  through  the  details  about\\nmodel tuning.\\n\\n5.6. Model tuning.    Similar to other machine learning techniques, we can find the best\\ncombination  of  model  hyperparameters  in  RL  by  using  techniques  such  as  grid\\nsearch. The grid search for RL-based problems are computationally intensive. Hence,\\nin this section, rather than performing the grid search, we present the key hyperpara‐\\nmeters  to  consider,  along  with  their  intuition  and  potential  impact  on  the  model\\noutput.\\n\\nGamma (discount factor)\\n\\nDecaying  gamma  will  have  the  agent  prioritize  short-term  rewards  as  it  learns\\nwhat those rewards are, and place less emphasis on long-term rewards. Lowering\\nthe  discount  factor  in  this  case  study  may  cause  the  algorithm  to  focus  on  the\\nlong-term rewards.\\n\\nEpsilon\\n\\nThe  epsilon  variable  drives  the  exploration  versus  exploitation  property  of  the\\nmodel. The more we get to know our environment, the less random exploration\\nwe  want  to  do.  When  we  reduce  epsilon,  the  likelihood  of  a  random  action\\nbecomes  smaller,  and  we  take  more  opportunities  to  benefit  from  the  high-\\nvalued actions that we already discovered. However, in the trading setup, we do\\nnot want the algorithm to overfit to the training data, and the epsilon should be\\nmodified accordingly.\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n313\\n\\n\\x0cEpisodes and batch size\\n\\nA higher number of episodes and larger batch size in the training set will lead to\\nbetter  training  and  a  more  optimal  Q-value.  However,  there  is  a  trade-off,  as\\nincreasing  the  number  of  episodes  and  batch  size  increases  the  total  training\\ntime.\\n\\nWindow size\\n\\nWindow size determines the number of market days to consider to evaluate the\\nstate.  This  can  be  increased  in  case  we  want  the  state  to  be  determined  by  a\\ngreater number of days in the past.\\n\\nNumber of layers and nodes of the deep learning model\\n\\nThis can be modified for better training and a more optimal Q-value. The details\\nabout the impact of changing the layers and nodes of ANN models are discussed\\nin  Chapter  3,  and  the  grid  search  for  a  deep  learning  model  is  discussed  in\\nChapter 5.\\n\\n6. Testing the data\\n\\nAfter  training  the  data,  it  is  evaluated  against  the  test  dataset.  This  is  an  important\\nstep,  especially  for  reinforcement  learning,  as  the  agent  may  mistakenly  correlate\\nreward  with  certain  spurious  features  from  the  data,  or  it  may  overfit  a  particular\\nchart pattern. In the testing step, we look at the performance of the already trained\\nmodel (model_ep10) from the training step on the test data. The Python code looks\\nsimilar to the training set we saw before. However, the is_eval flag is set to true, the\\nreply  buffer  function  is  not  called,  and  there  is  no  training.  Let  us  look  at  the\\nresults:\\n\\n#agent is already defined in the training set above.\\ntest_data = X_test\\nl_test = len(test_data) - 1\\nstate = getState(test_data, 0, window_size + 1)\\ntotal_profit = 0\\nis_eval = True\\ndone = False\\nstates_sell_test = []\\nstates_buy_test = []\\nmodel_name = \"model_ep10\"\\nagent = Agent(window_size, is_eval, model_name)\\nstate = getState(data, 0, window_size + 1)\\ntotal_profit = 0\\nagent.inventory = []\\n\\nfor t in range(l_test):\\n    action = agent.act(state)\\n\\n    next_state = getState(test_data, t + 1, window_size + 1)\\n    reward = 0\\n\\n314 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0c    if action == 1:\\n\\n        agent.inventory.append(test_data[t])\\n        print(\"Buy: \" + formatPrice(test_data[t]))\\n\\n    elif action == 2 and len(agent.inventory) > 0:\\n        bought_price = agent.inventory.pop(0)\\n        reward = max(test_data[t] - bought_price, 0)\\n        total_profit += test_data[t] - bought_price\\n        print(\"Sell: \" + formatPrice(test_data[t]) + \" | profit: \" +\\\\\\n         formatPrice(test_data[t] - bought_price))\\n\\n    if t == l_test - 1:\\n        done = True\\n    agent.memory.append((state, action, reward, next_state, done))\\n    state = next_state\\n\\n    if done:\\n        print(\"------------------------------------------\")\\n        print(\"Total Profit: \" + formatPrice(total_profit))\\n        print(\"------------------------------------------\")\\n\\nOutput\\n\\nTotal Profit: $1280.40\\n\\nLooking at the results above, our model resulted in an overall profit of $1,280, and we\\ncan say that our DQN agent performs quite well on the test set.\\n\\nConclusion\\n\\nIn  this  case  study,  we  created  an  automated  trading  strategy,  or  a  trading  bot,  that\\nsimply needs to be fed running stock market data to produce a trading signal. We saw\\nthat the algorithm decides the policy by itself, and the overall approach is much sim‐\\npler and more principled than the supervised learning–based approach. The trained\\n\\nCase Study 1: Reinforcement Learning–Based Trading Strategy \\n\\n| \\n\\n315\\n\\n\\x0cmodel was profitable in the test set, corroborating the effectiveness of the RL-based\\ntrading strategy.\\n\\nIn using a reinforcement learning model such as DQN, which is based on a deep neu‐\\nral network, we can learn policies that are more complex and powerful than what a\\nhuman trader could learn.\\n\\nGiven the high complexity and low interpretability of the RL-based model, visualiza‐\\ntion and testing steps become quite important. For interpretability, we used the plots\\nof the training episodes of the training algorithm and found that the model starts to\\nlearn over a period of time, discovers the strategy, and starts to exploit it. A sufficient\\nnumber of tests should be conducted on different time periods before deploying the\\nmodel for live trading.\\n\\nWhile using RL-based models, we should carefully select the RL components, such as\\nthe reward function and state, and ensure understanding of their impact on the over‐\\nall model results. Before implementing or training the model, it is important to think\\nof questions, such as “How can we engineer the reward function or the state so that\\nthe RL algorithm has the potential to learn to optimize the right metric?”\\n\\nOverall,  these  RL-based  models  can  enable  financial  practitioners  to  create  trading\\nstrategies  with  a  very  flexible  approach.  The  framework  provided  in  this  case  study\\ncan  be  a  great  starting  point  to  develop  more  powerful  models  for  algorithmic\\ntrading.\\n\\nCase Study 2: Derivatives Hedging\\nMuch of traditional finance theory for handling derivatives pricing and risk manage‐\\nment is based on the idealized complete markets assumption of perfect hedgability,\\nwithout  trading  restrictions,  transaction  costs,  market  impact,  or  liquidity  con‐\\nstraints. In practice, however, these frictions are very real. As a consequence, practical\\nrisk  management  using  derivatives  requires  human  oversight  and  maintenance;  the\\nmodels  themselves  are  insufficient.  Implementation  is  still  partially  driven  by  the\\ntrader’s intuitive understanding of the shortcomings of the existing tools.\\n\\nReinforcement  learning  algorithms,  with  their  ability  to  tackle  more  nuances  and\\nparameters  within  the  operational  environment,  are  inherently  aligned  with  the\\nobjective of hedging. These models can produce dynamic strategies that are optimal,\\neven in a world with frictions. The model-free RL approaches demand very few theo‐\\nretical  assumptions.  This  allows  for  automation  of  hedging  without  requiring  fre‐\\nquent  human  intervention,  making  the  overall  hedging  process  significantly  faster.\\nThese models can learn from large amounts of historical data and can consider many\\nvariables to make more precise and accurate hedging decisions. Moreover, the availa‐\\n\\n316 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cbility of vast amounts of data makes RL-based models more useful and effective than\\never before.\\n\\nIn  this  case  study,  we  implement  a  reinforcement  learning–based  hedging  strategy\\nthat  adopts  the  ideas  presented  in  the  paper  “Deep  Hedging”  by  Hans  Bühler  et  al.\\nWe  will  build  an  optimal  hedging  strategy  for  a  specific  type  of  derivative  (call\\noptions)  by  minimizing  the  risk-adjusted  PnL.  We  use  the  measure  CVaR  (condi‐\\ntional value at risk), which quantifies the amount of tail risk of a position or portfolio\\nas a risk assessment measure.\\n\\nIn this case study, we will focus on:\\n\\n• Using  policy-based  (or  direct  policy  search–based)  reinforcement  learning  and\\n\\nimplementing it using a deep neural network.\\n\\n• Comparing  the  effectiveness  of  an  RL-based  trading  strategy  to  the  traditional\\n\\nBlack-Scholes model.\\n\\n• Setting up an agent for an RL problem using class structure in Python.\\n\\n• Implementing and evaluating a policy gradient–based RL method.\\n\\n• Introducing the basic concept of functions in the TensorFlow Python package.\\n\\n• Implementing  a  Monte  Carlo  simulation  of  stock  price  and  the  Black-Scholes\\n\\npricing model, and computing option Greeks.\\n\\nBlueprint for Implementing a Reinforcement Learning–\\nBased Hedging Strategy\\n\\n1. Problem definition\\n\\nIn  the  reinforcement  learning  framework  for  this  case  study,  the  algorithm  decides\\nthe best hedging strategy for call options using market prices of the underlying asset.\\nA  direct  policy  search  reinforcement  learning  strategy  is  used.  The  overall  idea,\\nderived  from  the  “Deep  Hedging”  paper,  is  based  on  minimizing  the  hedge  error\\nunder  a  risk  assessment  measure.  The  overall  PnL  of  a  call  option  hedging  strategy\\nover a period of time, from t=1 to t=T, can be written as:\\n\\nPnL T (Z , δ) = – ZT + ∑\\n\\nT\\n\\nδt –1(St – St –1) – ∑\\n\\nT\\n\\nCt\\n\\nt=1\\n\\nt=1\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n317\\n\\n\\x0cwhere\\n\\n• ZT  is the payoff of a call option at maturity.\\n• δt –1(St – St –1) is the cash flow from the hedging instruments on day t, where δ is\\n\\nthe hedge and St is the spot price on day t.\\n\\n• Ct  is  the  transaction  cost  at  time  t  and  may  be  constant  or  proportional  to  the\\n\\nhedge size.\\n\\nThe  individual  components  in  the  equation  are  the  components  of  the  cash  flow.\\nHowever, it would be preferable to take into account the risk arising from any posi‐\\ntion  while  designing  the  reward  function.  We  use  the  measure  CVaR  as  the  risk\\nassessment  measure.  CVaR  quantifies  the  amount  of  tail  risk  and  is  the  expected\\nshortfall  (risk  aversion  parameter)13  for  the  confidence  level  α.  Now  the  reward\\nfunction is modified to the following:\\n\\nV T = f ( – ZT + ∑\\n\\nT\\n\\nδt –1(St – St –1) – ∑\\n\\nT\\n\\nt=1\\n\\nt=1\\n\\nCt)\\n\\nwhere  f  represents the CVaR.\\n\\nWe  will  train  an  RNN-based  network  to  learn  the  optimal  hedging  strategy  (i.e.,\\nδ1, δ2..., δT )  given  the  stock  price,  strike  price,  and  risk  aversion  parameter,  (α),  by\\nminimizing CVaR. We assume transaction costs to be zero for simplicity. The model\\ncan easily be extended to incorporate transaction costs and other market frictions.\\n\\nThe  data  used  for  the  synthetic  underlying  stock  price  is  generated  using  Monte\\nCarlo  simulation,  assuming  a  lognormal  price  distribution.  We  assume  an  interest\\nrate of 0% and annual volatility of 20%.\\n\\nThe key components of the model are:\\n\\nAgent\\n\\nTrader or trading agent.\\n\\nAction\\n\\nHedging strategy (i.e., δ1, δ2..., δT ).\\n\\nReward function\\n\\nCVaR—this is a convex function and is minimized during the model training.\\n\\n13 The expected shortfall is the expected value of an investment in the tail scenario.\\n\\n318 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cState\\n\\nState is the representation of the current market and relevant product variables.\\nThe  state  represents  the  model  inputs,  which  include  the  simulated  stock  price\\npath (i.e., S1, S2..., ST ), strike, and risk aversion parameter (α).\\n\\nEnvironment\\n\\nStock exchange or stock market.\\n\\n2. Getting started\\n\\n2.1. Loading the Python packages.    The loading of Python packages is similar to the pre‐\\nvious case studies. Please refer to the Jupyter notebook for this case study for more\\ndetails.\\n\\n2.2. Generating the data.    In this step we generate the data for this case study using a\\nBlack-Scholes simulation.\\n\\nThis function generates the Monte Carlo paths for the stock price and gets the option\\nprice on each of the Monte Carlo paths. The calculation as shown is based on the log‐\\nnormal assumption of stock prices:\\n\\nSt +1 = Ste (μ–\\n\\n1\\n\\n2 σ 2)Δt +σ Δt Z\\n\\nwhere S is stock price, σ is volatility, μ is the drift, t is time, and Z  is a standard nor‐\\nmal variable.\\n\\ndef monte_carlo_paths(S_0, time_to_expiry, sigma, drift, seed, n_sims, \\\\\\n  n_timesteps):\\n    \"\"\"\\n    Create random paths of a stock price following a brownian geometric motion\\n    return:\\n\\n    a (n_timesteps x n_sims x 1) matrix\\n    \"\"\"\\n    if seed > 0:\\n            np.random.seed(seed)\\n    stdnorm_random_variates = np.random.randn(n_sims, n_timesteps)\\n    S = S_0\\n    dt = time_to_expiry / stdnorm_random_variates.shape[1]\\n    r = drift\\n    S_T = S * np.cumprod(np.exp((r-sigma**2/2)*dt+sigma*np.sqrt(dt)*\\\\\\n    stdnorm_random_variates), axis=1)\\n    return np.reshape(np.transpose(np.c_[np.ones(n_sims)*S_0, S_T]), \\\\\\n    (n_timesteps+1, n_sims, 1))\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n319\\n\\n\\x0cWe  generate  50,000  simulations  of  the  spot  price  over  a  period  of  one  month.  The\\ntotal number of time steps is 30. Hence, for each Monte Carlo scenario, there is one\\nobservation per day. The parameters needed for the simulation are defined below:\\n\\nS_0 = 100; K = 100; r = 0; vol = 0.2; T = 1/12\\ntimesteps = 30; seed = 42; n_sims = 5000\\n\\n# Generate the monte carlo paths\\npaths_train = monte_carlo_paths(S_0, T, vol, r, seed, n_sims, timesteps)\\n\\n3. Exploratory data analysis\\n\\nWe will look at descriptive statistics and data visualization in this section. Given that\\nthe  data  was  generated  by  the  simulation,  we  simply  inspect  one  path  as  a  sanity\\ncheck of the simulation algorithm:\\n\\n#Plot Paths for one simulation\\nplt.figure(figsize=(20, 10))\\nplt.plot(paths_train[1])\\nplt.xlabel(\\'Time Steps\\')\\nplt.title(\\'Stock Price Sample Paths\\')\\nplt.show()\\n\\nOutput\\n\\n320 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0c4. Evaluate algorithms and models\\n\\nIn  this  direct  policy  search  approach,  we  use  an  artificial  neural  network  (ANN)  to\\nmap the state to action. In a traditional ANN, we assume that all inputs (and outputs)\\nare independent of each other. However, the hedging decision at time t (represented\\nby δt) is path dependent and is influenced by the stock price and hedging decisions at\\nprevious time steps. Hence, using a traditional ANN is not feasible. RNN is a type of\\nANN  that  can  capture  the  time-varying  dynamics  of  the  underlying  system  and  is\\nmore appropriate in this context. RNNs have a memory, which captures information\\nabout what has been calculated so far. We used this property of the RNN model for\\ntime  series  modeling  in  Chapter  5.  LSTM  (also  discussed  in  Chapter  5)  is  a  special\\nkind  of  RNN  capable  of  learning  long-term  dependencies.  Past  state  information  is\\nmade available to the network when mapping to an action; the extraction of relevant\\npast data is then learned as part of the training process. We will use an LSTM model\\nto map the state to action and get the hedging strategy (i.e., δ1, δ2,…δT).\\n\\n4.1. Policy gradient script.    We will cover the implementation steps and model training\\nin this section. We provide the input variables—stock price path (S1, S2, ...ST ), strike,\\nand  risk  aversion  parameter,  α—to  the  trained  model  and  receive  the  hedging\\nstrategy (i.e., δ1, δ2, ...δT ) as the output. Figure 9-9 provides an overview of the train‐\\ning of the policy gradient for this case study.\\n\\nFigure 9-9. Policy gradient training for derivatives hedging\\n\\nWe  already  performed  step  1  in  section  2  of  this  case  study.  Steps  2  to  5  are  self-\\nexplanatory  and  are  implemented  in  the  agent  class  defined  later.  The  agent  holds\\nthe  variables  and  member  functions  that  perform  the  training.  An  object  of  the\\nagent  class  is  created  using  the  training  phase  and  is  used  for  training  the  model.\\nAfter  a  sufficient  number  of  iterations  of  steps  2  to  5,  an  optimal  policy  gradient\\nmodel is generated.\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n321\\n\\n\\x0cThe class consists of the following modules:\\n\\n• Constructor\\n• The function execute_graph_batchwise\\n• The functions training, predict, and restore\\n\\nLet us dig deeper into the Python code for each of the functions.\\n\\nThe Constructor is defined as an init function, where we define the model parame‐\\nters. We can pass the timesteps, batch_size, and number of nodes in each layer of\\nthe LSTM model to the constructor. We define the input variables of the model (i.e.,\\nstock  price  path,  strike,  and  risk  aversion  parameter)  as  TensorFlow  placeholders.\\nPlaceholders are used to feed in data from outside the computational graph, and we\\nfeed  the  data  of  these  input  variables  during  the  training  phase.  We  implement  an\\nLSTM  network  in  TensorFlow  by  using  the  tf.MultiRNNCell  function.  The  LSTM\\nmodel uses four layers with 62, 46, 46, and 1 nodes. The loss function is the CVaR,\\nwhich  is  minimized  when  tf.train  is  called  during  the  training  step.  We  sort  the\\nnegative realized PnLs of the trading strategy and calculate the mean of the (1−α) top\\nlosses:\\n\\nclass Agent(object):\\n    def __init__(self, time_steps, batch_size, features,\\\\\\n       nodes = [62, 46, 46, 1], name=\\'model\\'):\\n\\n        #1. Initialize the variables\\n        tf.reset_default_graph()\\n        self.batch_size = batch_size # Number of options in a batch\\n        self.S_t_input = tf.placeholder(tf.float32, [time_steps, batch_size, \\\\\\n          features]) #Spot\\n        self.K = tf.placeholder(tf.float32, batch_size) #Strike\\n        self.alpha = tf.placeholder(tf.float32) #alpha for cVaR\\n\\n        S_T = self.S_t_input[-1,:,0] #Spot at time T\\n        # Change in the Spot\\n        dS = self.S_t_input[1:, :, 0] - self.S_t_input[0:-1, :, 0]\\n        #dS = tf.reshape(dS, (time_steps, batch_size))\\n\\n        #2. Prepare S_t for use in the RNN remove the \\\\\\n        #last time step (at T the portfolio is zero)\\n        S_t = tf.unstack(self.S_t_input[:-1, :,:], axis=0)\\n\\n        # Build the lstm\\n        lstm = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(n) \\\\\\n        for n in nodes])\\n\\n        #3. So the state is a convenient tensor that holds the last\\n        #actual RNN state,ignoring the zeros.\\n\\n322 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0c        #The strategy tensor holds the outputs of all cells.\\n        self.strategy, state = tf.nn.static_rnn(lstm, S_t, initial_state=\\\\\\n          lstm.zero_state(batch_size, tf.float32), dtype=tf.float32)\\n\\n        self.strategy = tf.reshape(self.strategy, (time_steps-1, batch_size))\\n\\n        #4. Option Price\\n        self.option = tf.maximum(S_T-self.K, 0)\\n\\n        self.Hedging_PnL = - self.option + tf.reduce_sum(dS*self.strategy, \\\\\\n          axis=0)\\n\\n        #5. Total Hedged PnL of each path\\n        self.Hedging_PnL_Paths = - self.option + dS*self.strategy\\n\\n        # 6. Calculate the CVaR for a given confidence level alpha\\n        # Take the 1-alpha largest losses (top 1-alpha negative PnLs)\\n        #and calculate the mean\\n        CVaR, idx = tf.nn.top_k(-self.Hedging_PnL, tf.cast((1-self.alpha)*\\\\\\n        batch_size, tf.int32))\\n        CVaR = tf.reduce_mean(CVaR)\\n        #7. Minimize the CVaR\\n        self.train = tf.train.AdamOptimizer().minimize(CVaR)\\n        self.saver = tf.train.Saver()\\n        self.modelname = name\\n\\nThe function execute_graph_batchwise is the key function of the program, in which\\nwe train the neural network based on the observed experience. It takes a batch of the\\nstates as input and updates the policy gradient–based LSTM model weights by mini‐\\nmizing CVaR. This function trains the LSTM model to predict a hedging strategy by\\nlooping across the epochs and batches. First, it prepares a batch of market variables\\n(stock price, strike, and risk aversion) and uses sess.run function for training. This\\nsess.run  is  a  TensorFlow  function  to  run  any  operation  defined  within  it.  Here,  it\\ntakes the inputs and runs the tf.train function that was defined in the constructor.\\nAfter a sufficient number of iterations, an optimal policy gradient model is generated:\\n\\n    def _execute_graph_batchwise(self, paths, strikes, riskaversion, sess, \\\\\\n      epochs=1, train_flag=False):\\n        #1: Initialize the variables.\\n        sample_size = paths.shape[1]\\n        batch_size=self.batch_size\\n        idx = np.arange(sample_size)\\n        start = dt.datetime.now()\\n        #2:Loop across all the epochs\\n        for epoch in range(epochs):\\n            # Save the hedging Pnl for each batch\\n            pnls = []\\n            strategies = []\\n            if train_flag:\\n                np.random.shuffle(idx)\\n            #3. Loop across the observations\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n323\\n\\n\\x0c            for i in range(int(sample_size/batch_size)):\\n                indices = idx[i*batch_size : (i+1)*batch_size]\\n                batch = paths[:,indices,:]\\n\\n                #4. Train the LSTM\\n                if train_flag:#runs the train, hedging PnL and strategy.\\n                    _, pnl, strategy = sess.run([self.train, self.Hedging_PnL, \\\\\\n                      self.strategy], {self.S_t_input: batch,\\\\\\n                        self.K : strikes[indices],\\\\\\n                        self.alpha: riskaversion})\\n                        #5. Evaluation and no training\\n                else:\\n                    pnl, strategy = sess.run([self.Hedging_PnL, self.strategy], \\\\\\n                      {self.S_t_input: batch,\\\\\\n                      self.K : strikes[indices],\\n                      self.alpha: riskaversion})\\\\\\n\\n                pnls.append(pnl)\\n                strategies.append(strategy)\\n            #6. Calculate the option price # given the risk aversion level alpha\\n\\n            CVaR = np.mean(-np.sort(np.concatenate(pnls))\\\\\\n            [:int((1-riskaversion)*sample_size)])\\n            #7. Return training metrics, \\\\\\n            #if it is in the training phase\\n            if train_flag:\\n                if epoch % 10 == 0:\\n                    print(\\'Time elapsed:\\', dt.datetime.now()-start)\\n                    print(\\'Epoch\\', epoch, \\'CVaR\\', CVaR)\\n                    #Saving the model\\n                    self.saver.save(sess, \"model.ckpt\")\\n        self.saver.save(sess, \"model.ckpt\")\\n\\n        #8. return CVaR and other parameters\\n        return CVaR, np.concatenate(pnls), np.concatenate(strategies,axis=1)\\n\\nThe  training  function  simply  triggers  the  execute_graph_batchwise  function  and\\nprovides  all  the  inputs  required  for  training  to  this  function.  The  predict  function\\nreturns  the  action  (hedging  strategy)  given  a  state  (market  variables).  The  restore\\nfunction  restores  the  saved  trained  model,  to  be  used  further  for  training  or\\nprediction:\\n\\n    def training(self, paths, strikes, riskaversion, epochs, session, init=True):\\n        if init:\\n            sess.run(tf.global_variables_initializer())\\n        self._execute_graph_batchwise(paths, strikes, riskaversion, session, \\\\\\n          epochs, train_flag=True)\\n\\n    def predict(self, paths, strikes, riskaversion, session):\\n        return self._execute_graph_batchwise(paths, strikes, riskaversion,\\\\\\n          session,1, train_flag=False)\\n\\n324 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0c    def restore(self, session, checkpoint):\\n        self.saver.restore(session, checkpoint)\\n\\n4.2. Training the data.    The steps of training our policy-based model are:\\n1. Define  the  risk  aversion  parameter  for  CVaR,  number  of  features  (this  is  total\\nnumber of stocks, and in this case we just have one), strike price, and batch size.\\nThe  CVaR  represents  the  amount  of  loss  we  want  to  minimize.  For  example,  a\\nCVaR of 99% means that we want to avoid extreme loss, while a CVaR of 50%\\nminimizes average loss. We train with a CVaR of 50% to have smaller mean loss.\\n\\n2. Instantiate the policy gradient agent, which has the RNN based-policy with the\\n\\nloss function based on the CVaR.\\n\\n3. Iterate  through  the  batches;  the  strategy  is  defined  by  the  policy  output  of  the\\n\\nLSTM-based network.\\n\\n4. Finally, the trained model is saved.\\n\\nbatch_size = 1000\\nfeatures = 1\\nK = 100\\nalpha = 0.50 #risk aversion parameter for CVaR\\nepoch = 101 #It is set to 11, but should ideally be a high number\\nmodel_1 = Agent(paths_train.shape[0], batch_size, features, name=\\'rnn_final\\')\\n# Training the model takes a few minutes\\nstart = dt.datetime.now()\\nwith tf.Session() as sess:\\n    # Train Model\\n    model_1.training(paths_train, np.ones(paths_train.shape[1])*K, alpha,\\\\\\n     epoch, sess)\\nprint(\\'Training finished, Time elapsed:\\', dt.datetime.now()-start)\\n\\nOutput\\n\\nTime elapsed: 0:00:03.326560\\nEpoch 0 CVaR 4.0718956\\nEpoch 100 CVaR 2.853285\\nTraining finished, Time elapsed: 0:01:56.299444\\n\\n5. Testing the data\\n\\nTesting is an important step, especially for RL, as it is difficult for a model to provide\\nany  meaningful,  intuitive  relationships  between  input  and  their  corresponding  out‐\\nput that is easily understood. In the testing step, we will compare the effectiveness of\\nthe hedging strategy and compare it to a delta hedging strategy based on the Black-\\nScholes model. We first define the helper functions used in this step.\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n325\\n\\n\\x0c5.1. Helper functions for comparison against Black-Scholes.     In  this  module,  we  create\\nadditional  functions  that  are  used  for  comparison  against  the  traditional  Black-\\nScholes model.\\n\\n5.1.1. Black-Scholes price and delta.    The function BlackScholes_price implements the\\nanalytical formula for the call option price, and BS_delta implements the analytical\\nformula for the delta of a call option:\\n\\ndef BS_d1(S, dt, r, sigma, K):\\n    return (np.log(S/K) + (r+sigma**2/2)*dt) / (sigma*np.sqrt(dt))\\n\\ndef BlackScholes_price(S, T, r, sigma, K, t=0):\\n    dt = T-t\\n    Phi = stats.norm(loc=0, scale=1).cdf\\n    d1 = BS_d1(S, dt, r, sigma, K)\\n    d2 = d1 - sigma*np.sqrt(dt)\\n    return S*Phi(d1) - K*np.exp(-r*dt)*Phi(d2)\\n\\ndef BS_delta(S, T, r, sigma, K, t=0):\\n    dt = T-t\\n    d1 = BS_d1(S, dt, r, sigma, K)\\n    Phi = stats.norm(loc=0, scale=1).cdf\\n    return Phi(d1)\\n\\n5.1.2. Test results and plotting.     The  following  functions  are  used  to  compute  the  key\\nmetrics and related plots for evaluating the effectiveness of the hedge. The function\\ntest_hedging_strategy computes different types of PnL, including CVaR, PnL, and\\nHedge PnL. The function  plot_deltas plots the comparison of the RL delta versus\\nBlack-Scholes  hedging  at  different  time  points.  The  function  plot_strategy_pnl  is\\nused to plot the total PnL of the RL-based strategy versus Black-Scholes hedging:\\n\\ndef test_hedging_strategy(deltas, paths, K, price, alpha, output=True):\\n    S_returns = paths[1:,:,0]-paths[:-1,:,0]\\n    hedge_pnl = np.sum(deltas * S_returns, axis=0)\\n    option_payoff = np.maximum(paths[-1,:,0] - K, 0)\\n    replication_portfolio_pnls = -option_payoff + hedge_pnl + price\\n    mean_pnl = np.mean(replication_portfolio_pnls)\\n    cvar_pnl = -np.mean(np.sort(replication_portfolio_pnls)\\\\\\n    [:int((1-alpha)*replication_portfolio_pnls.shape[0])])\\n    if output:\\n        plt.hist(replication_portfolio_pnls)\\n        print(\\'BS price at t0:\\', price)\\n        print(\\'Mean Hedging PnL:\\', mean_pnl)\\n        print(\\'CVaR Hedging PnL:\\', cvar_pnl)\\n    return (mean_pnl, cvar_pnl, hedge_pnl, replication_portfolio_pnls, deltas)\\n\\ndef plot_deltas(paths, deltas_bs, deltas_rnn, times=[0, 1, 5, 10, 15, 29]):\\n    fig = plt.figure(figsize=(10,6))\\n    for i, t in enumerate(times):\\n\\n326 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0c        plt.subplot(2,3,i+1)\\n        xs =  paths[t,:,0]\\n        ys_bs = deltas_bs[t,:]\\n        ys_rnn = deltas_rnn[t,:]\\n        df = pd.DataFrame([xs, ys_bs, ys_rnn]).T\\n\\n        plt.plot(df[0], df[1], df[0], df[2], linestyle=\\'\\', marker=\\'x\\' )\\n        plt.legend([\\'BS delta\\', \\'RNN Delta\\'])\\n        plt.title(\\'Delta at Time %i\\' % t)\\n        plt.xlabel(\\'Spot\\')\\n        plt.ylabel(\\'$\\\\Delta$\\')\\n    plt.tight_layout()\\n\\ndef plot_strategy_pnl(portfolio_pnl_bs, portfolio_pnl_rnn):\\n    fig = plt.figure(figsize=(10,6))\\n    sns.boxplot(x=[\\'Black-Scholes\\', \\'RNN-LSTM-v1 \\'], y=[portfolio_pnl_bs, \\\\\\n    portfolio_pnl_rnn])\\n    plt.title(\\'Compare PnL Replication Strategy\\')\\n    plt.ylabel(\\'PnL\\')\\n\\n5.1.3. Hedging error for Black-Scholes replication.     The  following  function  is  used  to  get\\nthe  hedging  strategy  based  on  the  traditional  Black-Scholes  model,  which  is  further\\nused for comparison against the RL-based hedging strategy:\\n\\ndef black_scholes_hedge_strategy(S_0, K, r, vol, T, paths, alpha, output):\\n    bs_price = BlackScholes_price(S_0, T, r, vol, K, 0)\\n    times = np.zeros(paths.shape[0])\\n    times[1:] = T / (paths.shape[0]-1)\\n    times = np.cumsum(times)\\n    bs_deltas = np.zeros((paths.shape[0]-1, paths.shape[1]))\\n    for i in range(paths.shape[0]-1):\\n        t = times[i]\\n        bs_deltas[i,:] = BS_delta(paths[i,:,0], T, r, vol, K, t)\\n    return test_hedging_strategy(bs_deltas, paths, K, bs_price, alpha, output)\\n\\n5.2. Comparison between Black-Scholes and reinforcement learning.     We  will  compare  the\\neffectiveness  of  the  hedging  strategy  by  looking  at  the  influence  of  the  CVaR  risk\\naversion  parameter  and  inspect  how  well  the  RL-based  model  can  generalize  the\\nhedging strategy if we change the moneyness of the option, the drift, and the volatil‐\\nity of the underlying process.\\n\\n5.2.1. Test at 99% risk aversion.    As mentioned before, the CVaR represents the amount\\nof loss we want to minimize. We trained the model using a risk aversion of 50% to\\nminimize average loss. However, for testing purposes we increase the risk aversion to\\n99%, meaning that we want to avoid extreme loss. These results are compared against\\nthe Black-Scholes model:\\n\\nn_sims_test = 1000\\n# Monte Carlo Path for the test set\\nalpha = 0.99\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n327\\n\\n\\x0cpaths_test =  monte_carlo_paths(S_0, T, vol, r, seed_test, n_sims_test, \\\\\\n  timesteps)\\n\\nWe  use  the  trained  function  and  compare  the  Black-Scholes  and  RL  models  in  the\\nfollowing code:\\n\\nwith tf.Session() as sess:\\n    model_1.restore(sess, \\'model.ckpt\\')\\n    #Using the model_1 trained in the section above\\n    test1_results = model_1.predict(paths_test, np.ones(paths_test.shape[1])*K, \\\\\\n    alpha, sess)\\n\\n_,_,_,portfolio_pnl_bs, deltas_bs = black_scholes_hedge_strategy\\\\\\n(S_0,K, r, vol, T, paths_test, alpha, True)\\nplt.figure()\\n_,_,_,portfolio_pnl_rnn, deltas_rnn = test_hedging_strategy\\\\\\n(test1_results[2], paths_test, K, 2.302974467802428, alpha, True)\\nplot_deltas(paths_test, deltas_bs, deltas_rnn)\\nplot_strategy_pnl(portfolio_pnl_bs, portfolio_pnl_rnn)\\n\\nOutput\\n\\nBS price at t0: 2.3029744678024286\\nMean Hedging PnL: -0.0010458505607415178\\nCVaR Hedging PnL: 1.2447953011695538\\nRL based BS price at t0: 2.302974467802428\\nRL based Mean Hedging PnL: -0.0019250998451393934\\nRL based CVaR Hedging PnL: 1.3832611348053374\\n\\n328 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cFor the first test set (strike 100, same drift, same vol) with a risk aversion of 99%, the\\nresults look quite good. We see that the delta from both Black-Scholes and the RL-\\nbased approach converge over time from day 1 to 30. The CVaRs of both strategies\\nare  similar  and  lower  in  magnitude,  with  values  of  1.24  and  1.38  for  Black-Scholes\\nand RL, respectively. Also, the volatility of the two strategies is similar, as illustrated\\nin the second chart.\\n\\n5.2.2. Changing moneyness.    Let us now look at the comparison of the strategies, when\\nthe  moneyness,  defined  as  the  ratio  of  strike  to  spot  price,  is  changed.  In  order  to\\nchange the moneyness, we decrease the strike price by 10. The code snippet is similar\\nto the previous case, and the output is shown below:\\n\\nBS price at t0: 10.07339936955367\\nMean Hedging PnL: 0.0007508571761945107\\nCVaR Hedging PnL: 0.6977526775080665\\nRL based BS price at t0: 10.073\\nRL based Mean Hedging PnL: -0.038571546628968216\\nRL based CVaR Hedging PnL: 3.4732447615593975\\n\\nWith the change in the moneyness, we see that the PnL of the RL strategy is signifi‐\\ncantly worse than that of the Black-Scholes strategy. We see a significant deviation of\\nthe delta between the two across all the days. The CVaR and the volatility of the RL-\\nbased  strategy  is  much  higher.  The  results  indicate  that  we  should  be  careful  while\\ngeneralizing the model to different levels of moneyness and should train the model\\nwith the option of using a variety of strikes before implementing it in a production\\nenvironment.\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n329\\n\\n\\x0c5.2.3. Changing drift.     Let  us  now  look  at  the  comparison  of  the  strategies  when  the\\ndrift is changed. In order to change the drift, we assume the drift of the stock price is\\n4% per month, or 48% annualized. The output is shown below:\\n\\n330 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cOutput\\n\\nBS price at t0: 2.3029744678024286\\nMean Hedging PnL: -0.01723902964827388\\nCVaR Hedging PnL: 1.2141220199385756\\nRL based BS price at t0: 2.3029\\nRL based Mean Hedging PnL: -0.037668804359885316\\nRL based CVaR Hedging PnL: 1.357201635552361\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n331\\n\\n\\x0cThe  overall  results  look  good  for  the  change  in  drift.  The  conclusion  is  similar  to\\nresults  when  the  risk  aversion  was  changed,  with  the  deltas  for  the  two  approaches\\nconverging over time. Again, the CVaRs are similar in magnitude, with Black-Scholes\\nproducing a value of 1.21, and RL a value of 1.357.\\n\\n5.2.4. Shifted volatility.     Finally,  we  look  at  the  impact  of  shifting  the  volatility.  In\\norder to change the volatility, we increase it by 5%:\\n\\nOutput\\n\\nBS price at t0: 2.3029744678024286\\nMean Hedging PnL: -0.5787493248269506\\nCVaR Hedging PnL: 2.5583922824407566\\nRL based BS price at t0: 2.309\\nRL based Mean Hedging PnL: -0.5735181045192523\\nRL based CVaR Hedging PnL: 2.835487824499669\\n\\nLooking at the results, the delta, CVaR, and overall volatility of both models are simi‐\\nlar. Hence looking at the different comparisons overall, the performance of this RL-\\nbased hedging is on par with Black-Scholes–based hedging.\\n\\n332 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cConclusion\\n\\nIn  this  case  study,  we  compared  the  effectiveness  of  a  call  option  hedging  strategy\\nusing  RL.  The  RL-based  hedging  strategy  did  quite  well  even  when  certain  input\\nparameters were modified. However, this strategy was not able to generalize the strat‐\\negy for options at different moneyness levels. It underscores the fact that RL is a data-\\nintensive  approach,  and  it  is  important  to  train  the  model  with  different  scenarios,\\nwhich  becomes  more  important  if  the  model  is  intended  to  be  used  across  a  wide\\nvariety of derivatives.\\n\\nAlthough  we  found  the  RL  and  traditional  Black-Scholes  strategies  comparable,  the\\nRL  approach  offers  a  much  higher  ceiling  for  improvement.  The  RL  model  can  be\\nfurther  trained  using  a  wide  variety  of  instruments  with  different  hyperparameters,\\nleading  to  performance  enhancements.  It  would  be  interesting  to  explore  the  com‐\\nparison of these two hedging models for more exotic derivatives, given the trade-off\\nbetween these approaches.\\n\\nOverall, the RL-based approach is model independent and scalable, and it offers effi‐\\nciency boosts for many classical problems.\\n\\nCase Study 2: Derivatives Hedging \\n\\n| \\n\\n333\\n\\n\\x0cCase Study 3: Portfolio Allocation\\nAs discussed in prior case studies, the most commonly used technique for portfolio\\nallocation,  mean-variance  portfolio  optimization,  suffers  from  several  weaknesses,\\nincluding:\\n\\n• Estimation  errors  in  the  expected  returns  and  covariance  matrix  caused  by  the\\n\\nerratic nature of financial returns.\\n\\n• Unstable  quadratic  optimization  that  greatly  jeopardizes  the  optimality  of  the\\n\\nresulting portfolios.\\n\\nWe  addressed  some  of  these  weaknesses  in  “Case  Study  1:  Portfolio  Management:\\nFinding  an  Eigen  Portfolio”  on  page  202  in  Chapter  7,  and  in  “Case  Study  3:  Hier‐\\narchical Risk Parity” on page 267 in Chapter 8. Here, we approach this problem from\\nan RL perspective.\\n\\nReinforcement learning algorithms, with the ability to decide the policy on their own,\\nare  strong  models  for  performing  portfolio  allocation  in  an  automated  manner,\\nwithout  the  need  for  continuous  supervision.  Automation  of  the  manual  steps\\ninvolved  in  portfolio  allocation  can  prove  to  be  immensely  useful,  specifically  for\\nrobo-advisors.\\n\\nIn an RL-based framework, we treat portfolio allocation not just as a one-step optimi‐\\nzation problem but as continuous control of the portfolio with delayed rewards. We\\nmove  from  discrete  optimal  allocation  to  continuous  control  territory,  and  in  the\\nenvironment of a continuously changing market, RL algorithms can be leveraged to\\nsolve complex and dynamic portfolio allocation problems.\\n\\nIn  this  case  study,  we  will  use  a  Q-learning-based  approach  and  DQN  to  come  up\\nwith a policy for optimal portfolio allocation among a set of cryptocurrencies. Over‐\\nall,  the  approach  and  framework  in  terms  of  the  Python-based  implementation  is\\nsimilar to that in case study 1. Therefore, some repetitive sections or code explana‐\\ntion is skipped in this case study.\\n\\nIn this case study, we will focus on:\\n\\n• Defining the components of RL in a portfolio allocation problem.\\n\\n• Evaluating Q-learning in the context of portfolio allocation.\\n\\n• Creating a simulation environment to be used in the RL framework.\\n\\n• Extending  the  Q-learning  framework  used  for  trading  strategy  development  to\\n\\nportfolio management.\\n\\n334 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cBlueprint for Creating a Reinforcement Learning–Based\\nAlgorithm for Portfolio Allocation\\n\\n1. Problem definition\\n\\nIn  the  reinforcement  learning  framework  defined  for  this  case  study,  the  algorithm\\nperforms  an  action,  which  is  optimal  portfolio  allocation,  depending  on  the  current\\nstate  of  the  portfolio.  The  algorithm  is  trained  using  a  deep  Q-learning  framework,\\nand the components of the model are as follows:\\n\\nAgent\\n\\nA portfolio manager, a robo-advisor, or an individual investor.\\n\\nAction\\n\\nAssignment and rebalancing of the portfolio weights. The DQN model provides\\nthe Q-values, which are converted into portfolio weights.\\n\\nReward function\\n\\nThe Sharpe ratio. Although there can be a wide range of complex reward func‐\\ntions that provide a trade-off between profit and risk, such as percentage return\\nor maximum drawdown.\\n\\nState\\n\\nThe  state  is  the  correlation  matrix  of  the  instruments  based  on  a  specific  time\\nwindow. The correlation matrix is a suitable state variable for the portfolio allo‐\\ncation,  as  it  contains  the  information  about  the  relationships  between  different\\ninstruments and can be useful in performing portfolio allocation.\\n\\nEnvironment\\n\\nThe cryptocurrency exchange.\\n\\nThe dataset used in this case study is from the Kaggle platform. It contains the daily\\nprices of cryptocurrencies in 2018. The data contains some of the most liquid crypto‐\\ncurrencies, including Bitcoin, Ethereum, Ripple, Litecoin, and Dash.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  standard  Python  packages  are  loaded  in  this\\nstep. The details have already been presented in the previous case studies. Refer to the\\nJupyter notebook for this case study for more details.\\n\\nCase Study 3: Portfolio Allocation \\n\\n| \\n\\n335\\n\\n\\x0c2.2. Loading the data.    The fetched data is loaded in this step:\\n\\ndataset = read_csv(\\'data/crypto_portfolio.csv\\',index_col=0)\\n\\n3. Exploratory data analysis\\n\\n3.1. Descriptive statistics.    We will look at descriptive statistics and data visualizations\\nof the data in this section:\\n\\n# shape\\ndataset.shape\\n\\nOutput\\n\\n(375, 15)\\n\\n# peek at data\\nset_option(\\'display.width\\', 100)\\ndataset.head(5)\\n\\nOutput\\n\\nThe data has a total of 375 rows and 15 columns. These columns hold the daily prices\\nof 15 different cryptocurrencies in 2018.\\n\\n4. Evaluate algorithms and models\\n\\nThis is the key step of the reinforcement learning model development, where we will\\ndefine all the functions and classes and train the algorithm.\\n\\n4.1. Agent and cryptocurrency environment script.    We have an Agent class that holds the\\nvariables  and  member  functions  that  perform  the  Q-learning.  This  is  similar  to  the\\nAgent  class  defined  in  case  study  1,  with  an  additional  function  to  convert  the  Q-\\nvalue output from the deep neural network to portfolio weights and vice versa. The\\ntraining module implements iteration through several episodes and batches and saves\\nthe information of the state, action, reward, and next state to be used in training. We\\nskip the detailed description of the Python code of Agent class and the training mod‐\\nule in this case study. Readers can refer to the Jupyter notebook in the code reposi‐\\ntory for this book for more details.\\n\\n336 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cWe  implement  a  simulation  environment  for  cryptocurrencies  using  a  class  called\\nCryptoEnvironment. The concept of a simulation environment, or gym, is quite com‐\\nmon in RL problems. One of the challenges of reinforcement learning is the lack of\\navailable simulation environments on which to experiment. OpenAI gym is a toolkit\\nthat  provides  a  wide  variety  of  simulated  environments  (e.g.,  Atari  games,  2D/3D\\nphysical simulations), so we can train agents, compare them, or develop new RL algo‐\\nrithms. Additionally, it was developed with the aim of becoming a standardized envi‐\\nronment  and  benchmark  for  RL  research.  We  introduce  a  similar  concept  in  the\\nCryptoEnvironment class, where we create a simulation environment for cryptocur‐\\nrencies. This class has the following key functions:\\n\\ngetState\\n\\nThis  function  returns  the  state  as  well  as  the  historical  return  or  raw  historical\\ndata depending on the is_cov_matrix or is_raw_time_series flag\\n\\ngetReward\\n\\nThis  function  returns  the  reward  (i.e.,  Sharpe  ratio)  of  the  portfolio,  given  the\\nportfolio weights and lookback period\\n\\nclass CryptoEnvironment:\\n\\n    def __init__(self, prices = \\'./data/crypto_portfolio.csv\\', capital = 1e6):\\n        self.prices = prices\\n        self.capital = capital\\n        self.data = self.load_data()\\n\\n    def load_data(self):\\n        data =  pd.read_csv(self.prices)\\n        try:\\n            data.index = data[\\'Date\\']\\n            data = data.drop(columns = [\\'Date\\'])\\n        except:\\n            data.index = data[\\'date\\']\\n            data = data.drop(columns = [\\'date\\'])\\n        return data\\n\\n    def preprocess_state(self, state):\\n        return state\\n\\n    def get_state(self, t, lookback, is_cov_matrix=True\\\\\\n       is_raw_time_series=False):\\n\\n        assert lookback <= t\\n\\n        decision_making_state = self.data.iloc[t-lookback:t]\\n        decision_making_state = decision_making_state.pct_change().dropna()\\n\\n        if is_cov_matrix:\\n            x = decision_making_state.cov()\\n\\nCase Study 3: Portfolio Allocation \\n\\n| \\n\\n337\\n\\n\\x0c            return x\\n        else:\\n            if is_raw_time_series:\\n                decision_making_state = self.data.iloc[t-lookback:t]\\n            return self.preprocess_state(decision_making_state)\\n\\n    def get_reward(self, action, action_t, reward_t, alpha = 0.01):\\n\\n        def local_portfolio(returns, weights):\\n            weights = np.array(weights)\\n            rets = returns.mean() # * 252\\n            covs = returns.cov() # * 252\\n            P_ret = np.sum(rets * weights)\\n            P_vol = np.sqrt(np.dot(weights.T, np.dot(covs, weights)))\\n            P_sharpe = P_ret / P_vol\\n            return np.array([P_ret, P_vol, P_sharpe])\\n\\n        data_period = self.data[action_t:reward_t]\\n        weights = action\\n        returns = data_period.pct_change().dropna()\\n\\n        sharpe = local_portfolio(returns, weights)[-1]\\n        sharpe = np.array([sharpe] * len(self.data.columns))\\n        ret = (data_period.values[-1] - data_period.values[0]) / \\\\\\n        data_period.values[0]\\n\\n        return np.dot(returns, weights), ret\\n\\nLet’s explore the training of the RL model in the next step.\\n\\n4.3. Training the data.    As a first step, we initialize the Agent class and CryptoEnviron\\nment  class.  Then,  we  set  the  number  of  episodes  and  batch  size  for  the  training\\npurpose. Given the volatility of cryptocurrencies, we set the state window size to 180\\nand rebalancing frequency to 90 days:\\n\\nN_ASSETS = 15\\nagent = Agent(N_ASSETS)\\nenv = CryptoEnvironment()\\nwindow_size = 180\\nepisode_count = 50\\nbatch_size = 32\\nrebalance_period = 90\\n\\nFigure  9-10  provides  a  deep  dive  into  the  training  of  the  DQN  algorithm  used  for\\ndeveloping the RL-based portfolio allocation strategy. If we look carefully, the chart is\\nsimilar to the steps defined in Figure 9-8 in case study 1, with minor differences in\\nthe Q-Matrix, reward function, and action. Steps 1 to 7 describe the training and Cryp\\ntoEnvironment module; steps 8 to 10 show what happens in the replay buffer func‐\\ntion (i.e., exeReplay function) in the Agent module.\\n\\n338 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cFigure 9-10. DQN training for portfolio optimization\\n\\nThe details of steps 1 to 6 are:\\n\\n1. Get the current state using the helper function getState defined in the CryptoEn\\nvironment module. It returns a correlation matrix of the cryptocurrencies based\\non the window size.\\n\\n2. Get the action for the given state using the act function of the Agent class. The\\n\\naction is the weight of the cryptocurrency portfolio.\\n\\n3. Get  the  reward  for  the  given  action  using  the  getReward  function  in  the  Cryp\\n\\ntoEnvironment module.\\n\\n4. Get the next state using the getState function. The detail of the next state is fur‐\\n\\nther used in the Bellman equation for updating the Q-function.\\n\\n5. The  details  of  the  state,  next  state,  and  action  are  saved  in  the  memory  of  the\\n\\nAgent object. This memory is used further by the exeReply function.\\n\\n6. Check  if  the  batch  is  complete.  The  size  of  a  batch  is  defined  by  the  batch  size\\nvariable. If the batch is not complete, we move to the next time iteration. If the\\nbatch is complete, then we move to the Replay buffer function and update the\\nQ-function by minimizing the MSE between the Q-predicted and the Q-target in\\nsteps 8, 9, and 10.\\n\\nAs shown in the following charts, the code produces the final results along with two\\ncharts for each episode. The first chart shows the total cumulative return over time,\\nwhile the second chart shows the percentage of each cryptocurrency in the portfolio.\\n\\nCase Study 3: Portfolio Allocation \\n\\n| \\n\\n339\\n\\n\\x0cOutput\\n\\nEpisode 0/50 epsilon 1.0\\n\\nEpisode 1/50 epsilon 1.0\\n\\nEpisode 48/50 epsilon 1.0\\n\\n340 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cEpisode 49/50 epsilon 1.0\\n\\nThe charts outline the details of the portfolio allocation of the first two and last two\\nepisodes. The details of other episodes can be seen in the Jupyter notebook under the\\nGitHub repository for this book. The black line shows the performance of the portfo‐\\nlio,  and  the  dotted  grey  line  shows  the  performance  of  the  benchmark,  which  is  an\\nequally weighted portfolio of cryptocurrencies.\\n\\nIn  the  beginning  of  episodes  zero  and  one,  the  agent  has  no  preconception  of  the\\nconsequences of its actions, and it takes randomized actions to observe the returns,\\nwhich are quite volatile. Episode zero shows a clear example of erratic performance\\nbehavior. Episode one displays more stable movement but ultimately underperforms\\nthe  benchmark.  This  is  evidence  that  the  cumulative  reward  per  episode  fluctuates\\nsignificantly in the beginning of training.\\n\\nThe  last  two  charts  of  episodes  48  and  49  show  the  agent  starting  to  learn  from  its\\ntraining and discovering the optimal strategy. Overall returns are relatively stable and\\noutperform the benchmark. However, the overall portfolio weights are still quite vol‐\\natile due to the short time series and high volatility of the underlying cryptocurrency\\nassets. Ideally, we would be able to increase the number of training episodes and the\\nlength of historical data to enhance the training performance.\\n\\nCase Study 3: Portfolio Allocation \\n\\n| \\n\\n341\\n\\n\\x0cLet us look at the testing results.\\n\\n5. Testing the data\\n\\nRecall that the black line shows the performance of the portfolio, and the dotted grey\\nline is that of an equally weighted portfolio of cryptocurrencies:\\n\\nagent.is_eval = True\\n\\nactions_equal, actions_rl = [], []\\nresult_equal, result_rl = [], []\\n\\nfor t in range(window_size, len(env.data), rebalance_period):\\n\\n    date1 = t-rebalance_period\\n    s_ = env.get_state(t, window_size)\\n    action = agent.act(s_)\\n\\n    weighted_returns, reward = env.get_reward(action[0], date1, t)\\n    weighted_returns_equal, reward_equal = env.get_reward(\\n        np.ones(agent.portfolio_size) / agent.portfolio_size, date1, t)\\n\\n    result_equal.append(weighted_returns_equal.tolist())\\n    actions_equal.append(np.ones(agent.portfolio_size) / agent.portfolio_size)\\n\\n    result_rl.append(weighted_returns.tolist())\\n    actions_rl.append(action[0])\\n\\nresult_equal_vis = [item for sublist in result_equal for item in sublist]\\nresult_rl_vis = [item for sublist in result_rl for item in sublist]\\n\\nplt.figure()\\nplt.plot(np.array(result_equal_vis).cumsum(), label = \\'Benchmark\\', \\\\\\ncolor = \\'grey\\',ls = \\'--\\')\\nplt.plot(np.array(result_rl_vis).cumsum(), label = \\'Deep RL portfolio\\', \\\\\\ncolor = \\'black\\',ls = \\'-\\')\\nplt.xlabel(\\'Time Period\\')\\nplt.ylabel(\\'Cumulative Returnimage::images\\\\Chapter9-b82b2.png[]\\')\\nplt.show()\\n\\nDespite underperforming during the initial period, the model performance was better\\noverall,  primarily  due  to  avoiding  the  steep  decline  that  the  benchmark  portfolio\\nexperienced in the latter part of the test window. The returns appear very stable, per‐\\nhaps due to rotating away from the most volatile cryptocurrencies.\\n\\n342 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cOutput\\n\\nLet us inspect the return, volatility, Sharpe ratio, alpha, and beta of the portfolio and\\nbenchmark:\\n\\nimport statsmodels.api as sm\\nfrom statsmodels import regression\\ndef sharpe(R):\\n    r = np.diff(R)\\n    sr = r.mean()/r.std() * np.sqrt(252)\\n    return sr\\n\\ndef print_stats(result, benchmark):\\n\\n    sharpe_ratio = sharpe(np.array(result).cumsum())\\n    returns = np.mean(np.array(result))\\n    volatility = np.std(np.array(result))\\n\\n    X = benchmark\\n    y = result\\n    x = sm.add_constant(X)\\n    model = regression.linear_model.OLS(y, x).fit()\\n    alpha = model.params[0]\\n    beta = model.params[1]\\n\\n    return np.round(np.array([returns, volatility, sharpe_ratio, \\\\\\n      alpha, beta]), 4).tolist()\\n\\nprint(\\'EQUAL\\', print_stats(result_equal_vis, result_equal_vis))\\nprint(\\'RL AGENT\\', print_stats(result_rl_vis, result_equal_vis))\\n\\nCase Study 3: Portfolio Allocation \\n\\n| \\n\\n343\\n\\n\\x0cOutput\\n\\nEQUAL [-0.0013, 0.0468, -0.5016, 0.0, 1.0]\\nRL AGENT [0.0004, 0.0231, 0.4445, 0.0002, -0.1202]\\n\\nOverall,  the  RL  portfolio  performs  better  across  the  board,  with  a  higher  return,\\nhigher  Sharpe  ratio,  lower  volatility,  slight  alpha,  and  negative  correlation  to  the\\nbenchmark.\\n\\nConclusion\\n\\nIn this case study, we went beyond the classic efficient frontier for portfolio optimiza‐\\ntion  and  directly  learned  a  policy  of  dynamically  changing  portfolio  weights.  We\\ntrained  an  RL-based  model  by  setting  up  a  standardized  simulation  environment.\\nThis approach facilitated the training process and can be explored further for general\\nRL-based model training.\\n\\nThe  trained  RL-based  model  outperformed  an  equal-weight  benchmark  in  the  test\\nset. The performance of the RL-based model can be further improved by optimizing\\nthe  hyperparameters  or  using  a  longer  time  series  for  training.  However,  given  the\\nhigh complexity and low interpretability of an RL-based model, testing should occur\\nacross different time periods and market cycles before deploying the model for live\\ntrading. Also, as discussed in case study 1, we should carefully select the RL compo‐\\nnents, such as the reward function and state, and ensure we understand their impact\\non the overall model results.\\n\\nThe framework provided in this case study can enable financial practitioners to per‐\\nform  portfolio  allocation  and  rebalancing  with  a  very  flexible  and  automated\\napproach.\\n\\nChapter Summary\\nReward  maximization  is  one  of  the  key  principles  that  drives  algorithmic  trading,\\nportfolio management, derivative pricing, hedging, and trade execution. In this chap‐\\nter, we saw that when we use RL-based approaches, explicitly defining the strategy or\\npolicy for trading, derivative hedging, or portfolio management is unnecessary. The\\nalgorithm  determines  the  policy  itself,  which  can  lead  to  a  much  simpler  and  more\\nprincipled approach than other machine learning techniques.\\n\\nIn “Case Study 1: Reinforcement Learning–Based Trading Strategy” on page 298, we\\nsaw that RL makes algorithmic trading a simple game, which may or may not involve\\nunderstanding fundamental information. In “Case Study 2: Derivatives Hedging” on\\npage 316, we explored the use of reinforcement learning for a traditional derivative\\nhedging  problem.  This  exercise  demonstrated  that  we  can  leverage  the  efficient\\n\\n344 \\n\\n| \\n\\nChapter 9: Reinforcement Learning\\n\\n\\x0cnumerical calculation of RL in derivatives hedging to address some of the drawbacks\\nof the more traditional models. In “Case Study 3: Portfolio Allocation” on page 334,\\nwe performed portfolio allocation by learning a policy of changing portfolio weights\\ndynamically  in  a  continuously  changing  market  environment,  leading  to  further\\nautomation of the portfolio management process.\\n\\nAlthough RL comes with some challenges, such as being computationally expensive\\nand data intensive and lacking interpretability, it aligns perfectly with some areas in\\nfinance that are suited for policy frameworks based on reward maximization. Rein‐\\nforcement learning has managed to achieve superhuman performance in finite action\\nspaces, such as those in the games of Go, chess, and Atari. Looking ahead, with the\\navailability of more data, refined RL algorithms, and superior infrastructure, RL will\\ncontinue to prove to be immensely useful in finance.\\n\\nExercises\\n\\n• Using  the  ideas  and  concepts  presented  in  case  studies  1  and  2,  implement  a\\ntrading strategy based on a policy gradient algorithm for FX. Vary the key com‐\\nponents (i.e., reward function, state, etc.) for this implementation.\\n\\n• Implement the hedging of a fixed income derivative using the concepts presented\\n\\nin case study 2.\\n\\n• Incorporate a transaction cost in case study 2 and see the impact on the overall\\n\\nresults.\\n\\n• Based  on  the  ideas  presented  in  case  study  3,  implement  a  Q-learning-based\\nportfolio  allocation  strategy  on  a  portfolio  of  stocks,  FX,  or  fixed  income\\ninstruments.\\n\\nExercises \\n\\n| \\n\\n345\\n\\n\\x0c\\x0cCHAPTER 10\\nNatural Language Processing\\n\\nNatural  language  processing  (NLP)  is  a  subfield  of  artificial  intelligence  used  to  aid\\ncomputers in understanding natural human language. Most NLP techniques rely on\\nmachine  learning  to  derive  meaning  from  human  languages.  When  text  has  been\\nprovided, the computer utilizes algorithms to extract meaning associated with every\\nsentence and collect essential data from them. NLP manifests itself in different forms\\nacross many disciplines under various aliases, including (but not limited to) textual\\nanalysis, text mining, computational linguistics, and content analysis.\\n\\nIn the financial landscape, one of the earliest applications of NLP was implemented\\nby the US Securities and Exchange Commission (SEC). The group used text mining\\nand natural language processing to detect accounting fraud. The ability of NLP algo‐\\nrithms to scan and analyze legal and other documents at a high speed provides banks\\nand  other  financial  institutions  with  enormous  efficiency  gains  to  help  them  meet\\ncompliance regulations and combat fraud.\\n\\nIn the investment process, uncovering investment insights requires not only domain\\nknowledge  of  finance  but  also  a  strong  grasp  of  data  science  and  machine  learning\\nprinciples.  NLP  tools  may  help  detect,  measure,  predict,  and  anticipate  important\\nmarket characteristics and indicators, such as market volatility, liquidity risks, finan‐\\ncial stress, housing prices, and unemployment.\\n\\nNews has always been a key factor in investment decisions. It is well established that\\ncompany-specific,  macroeconomic,  and  political  news  strongly  influence  the  finan‐\\ncial markets. As technology advances, and market participants become more connec‐\\nted, the volume and frequency of news will continue to grow rapidly. Even today, the\\nvolume of daily text data being produced presents an untenable task for even a large\\nteam of fundamental researchers to navigate. Fundamental analysis assisted by NLP\\ntechniques  is  now  critical  to  unlock  the  complete  picture  of  how  experts  and  the\\nmasses feel about the market.\\n\\n347\\n\\n\\x0cIn banks and other organizations, teams of analysts are dedicated to poring over, ana‐\\nlyzing,  and  attempting  to  quantify  qualitative  data  from  news  and  SEC-mandated\\nreporting. Automation using NLP is well suited in this context. NLP can provide in-\\ndepth  support  in  the  analysis  and  interpretation  of  various  reports  and  documents.\\nThis  reduces  the  strain  that  repetitive,  low-value  tasks  put  on  human  employees.  It\\nalso provides a level of objectivity and consistency to otherwise subjective interpreta‐\\ntions; mistakes from human error are lessened. NLP can also allow a company to gar‐\\nner  insights  that  can  be  used  to  assess  a  creditor’s  risk  or  gauge  brand-related\\nsentiment from content across the web.\\n\\nWith  the  rise  in  popularity  of  live  chat  software  in  banking  and  finance  businesses,\\nNLP-based chatbots are a natural evolution. The combination of robo-advisors with\\nchatbots  is  expected  to  automate  the  entire  process  of  wealth  and  portfolio\\nmanagement.\\n\\nIn  this  chapter,  we  present  three  NLP-based  case  studies  that  cover  applications  of\\nNLP in algorithmic trading, chatbot creation, and document interpretation and auto‐\\nmation.  The  case  studies  follow  a  standardized  seven-step  model  development  pro‐\\ncess  presented  in  Chapter  2.  Key  model  steps  for  NLP-based  problems  are  data\\npreprocessing, feature representation, and inference. As such, these areas, along with\\nthe related concepts and Python-based examples, are outlined in this chapter.\\n\\n“Case Study 1: NLP and Sentiment Analysis–Based Trading Strategies” on page 362\\ndemonstrates  the  usage  of  sentiment  analysis  and  word  embedding  for  a  trading\\nstrategy. This case study highlights key focus areas for implementing an NLP-based\\ntrading strategy.\\n\\nIn  “Case  Study  2:  Chatbot  Digital  Assistant”  on  page  383,  we  create  a  chatbot  and\\ndemonstrate how NLP enables chatbots to understand messages and respond appro‐\\npriately. We leverage Python-based packages and modules to develop a chatbot in a\\nfew lines of code.\\n\\n“Case Study 3: Document Summarization” on page 393 illustrates the use of an NLP-\\nbased  topic  modeling  technique  to  discover  hidden  topics  or  themes  across  docu‐\\nments.  The  purpose  of  this  case  study  is  to  demonstrate  the  usage  of  NLP  to\\nautomatically  summarize  large  collections  of  documents  to  facilitate  organization\\nand management, as well as search and recommendations.\\n\\nIn addition to the points mentioned above, this chapter will cover:\\n\\n• How  to  perform  NLP  data  preprocessing,  including  steps  such  as  tokenization,\\npart-of-speech (PoS) tagging, or named entity recognition, in a few lines of code.\\n\\n• How  to  use  different  supervised  techniques,  including  LSTM,  for  sentiment\\n\\nanalysis.\\n\\n348 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0c• Understanding the main Python packages (i.e., NLTK, spaCy and TextBlob) and\\n\\nhow to use them for several NLP-related tasks.\\n\\n• How to build a data preprocessing pipeline using the spaCy package.\\n\\n• How to use pretrained models, such as word2vec, for feature representation.\\n\\n• How to use models such as LDA for topic modeling.\\n\\nThis Chapter’s Code Repository\\n\\nThe Python code for this chapter is included under the Chapter 10\\n- Natural Language Processing folder of the online GitHub reposi‐\\ntory  for  this  chapter.  For  any  new  NLP-based  case  study,  use  the\\ncommon  template  from  the  code  repository  and  modify  the  ele‐\\nments specific to the case study. The templates are designed to run\\non the cloud (i.e., Kaggle, Google Colab, and AWS).\\n\\nNatural Language Processing: Python Packages\\nPython is one of the best options to build an NLP-based expert system, and a large\\nvariety  of  open  source  NLP  libraries  are  available  for  Python  programmers.  These\\nlibraries  and  packages  contain  ready-to-use  modules  and  functions  to  incorporate\\ncomplex NLP steps and algorithms, making implementation fast, easy, and efficient.\\n\\nIn this section, we will describe three Python-based NLP libraries we’ve found to be\\nthe most useful and that we will be using in this chapter.\\n\\nNLTK\\nNLTK  is  the  most  famous  Python  NLP  library,  and  it  has  led  to  incredible  break‐\\nthroughs across several areas. Its modularized structure makes it excellent for learn‐\\ning  and  exploring  NLP  concepts.  However,  it  has  heavy  functionality  with  a  steep\\nlearning curve.\\n\\nNLTK  can  be  installed  using  the  typical  installation  procedure.  After  installing\\nNLTK, NLTK Data needs to be downloaded. The NLTK Data package includes a pre‐\\ntrained tokenizer punkt for English, which can be downloaded as well:\\n\\nimport nltk\\nimport nltk.data\\nnltk.download(\\'punkt\\')\\n\\nTextBlob\\nTextBlob is built on top of NLTK. This is one of the best libraries for fast prototyping\\nor  building  applications  with  minimal  performance  requirements.  TextBlob  makes\\n\\nNatural Language Processing: Python Packages \\n\\n| \\n\\n349\\n\\n\\x0ctext processing simple by providing an intuitive interface to NLTK. TextBlob can be\\nimported using the following command:\\n\\nfrom textblob import TextBlob\\n\\nspaCy\\nspaCy  is  an  NLP  library  designed  to  be  fast,  streamlined,  and  production-ready.  Its\\nphilosophy  is  to  present  only  one  algorithm  (the  best  one)  for  each  purpose.  We\\ndon’t  have  to  make  choices  and  can  focus  on  being  productive.  spaCy  uses  its  own\\npipeline to perform multiple preprocessing steps at the same time. We will demon‐\\nstrate it in a subsequent section.\\n\\nspaCy’s models can be installed as Python packages, just like any other module. To\\nload a model, use  spacy.load with the model’s shortcut link or package name or a\\npath to the data directory:\\n\\nimport spacy\\nnlp = spacy.load(\"en_core_web_lg\")\\n\\nIn  addition  to  these,  there  are  a  few  other  libraries,  such  as  gensim,  that  we  will\\nexplore for some of the examples in this chapter.\\n\\nNatural Language Processing: Theory and Concepts\\nAs we have already established, NLP is a subfield of artificial intelligence concerned\\nwith programming computers to process textual data in order to gain useful insights.\\nAll NLP applications go through common sequential steps, which include some com‐\\nbination of preprocessing textual data and representing the text as predictive features\\nbefore  feeding  them  into  a  statistical  inference  algorithm.  Figure  10-1  outlines  the\\nmajor steps in an NLP-based application.\\n\\nFigure 10-1. Natural language processing pipeline\\n\\n350 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cThe next section reviews these steps. For a thorough coverage of the topic, the reader\\nis referred to Natural Language Processing with Python by Steven Bird, Ewan Klein,\\nand Edward Loper (O’Reilly).\\n\\n1. Preprocessing\\nThere  are  usually  multiple  steps  involved  in  preprocessing  textual  data  for  NLP.\\nFigure 10-1 shows the key components of the preprocessing steps for NLP. These are\\ntokenization,  stop  words  removal,  stemming,  lemmatization,  PoS  (part-of-speech)\\ntagging, and NER (Name Entity Recognition).\\n\\n1.1. Tokenization\\n\\nTokenization  is  the  task  of  splitting  a  text  into  meaningful  segments,  called  tokens.\\nThese  segments  could  be  words,  punctuation,  numbers,  or  other  special  characters\\nthat are the building blocks of a sentence. A set of predetermined rules allows us to\\neffectively convert a sentence into a list of tokens. The following code snippets show\\nsample word tokenization using the NLTK and TextBlob packages:\\n\\n#Text to tokenize\\ntext = \"This is a tokenize test\"\\n\\nThe NLTK data package includes a pretrained Punkt tokenizer for English, which was\\npreviously loaded:\\n\\nfrom nltk.tokenize import word_tokenize\\nword_tokenize(text)\\n\\nOutput\\n\\n[\\'This\\', \\'is\\', \\'a\\', \\'tokenize\\', \\'test\\']\\n\\nLet’s look at tokenization using TextBlob:\\n\\nTextBlob(text).words\\n\\nOutput\\n\\nWordList([\\'This\\', \\'is\\', \\'a\\', \\'tokenize\\', \\'test\\'])\\n\\n1.2. Stop words removal\\n\\nAt  times,  extremely  common  words  that  offer  little  value  in  modeling  are  excluded\\nfrom the vocabulary. These words are called stop words. The code for removing stop\\nwords using the NLTK library is shown below:\\n\\ntext = \"S&P and NASDAQ are the two most popular indices in US\"\\n\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nstop_words = set(stopwords.words(\\'english\\'))\\ntext_tokens = word_tokenize(text)\\n\\nNatural Language Processing: Theory and Concepts \\n\\n| \\n\\n351\\n\\n\\x0ctokens_without_sw= [word for word in text_tokens if not word in stop_words]\\n\\nprint(tokens_without_sw)\\n\\nOutput\\n\\n[\\'S\\', \\'&\\', \\'P\\', \\'NASDAQ\\', \\'two\\', \\'popular\\', \\'indices\\', \\'US\\']\\n\\nWe  first  load  the  language  model  and  store  it  in  the  stop  words  variable.  The  stop\\nwords.words(\\'english\\')  is  a  set  of  default  stop  words  for  the  English  language\\nmodel in NLTK. Next, we simply iterate through each word in the input text, and if\\nthe  word  exists  in  the  stop  word  set  of  the  NLTK  language  model,  the  word  is\\nremoved. As we can see, stop words, such as are and most, are removed from the sen‐\\ntence.\\n\\n1.3. Stemming\\n\\nStemming is the process of reducing inflected (or sometimes derived) words to their\\nstem, base, or root form (generally a written word form). For example, if we were to\\nstem the words Stems, Stemming, Stemmed, and Stemitization, the result would be a\\nsingle word: Stem. The code for stemming using the NLTK library is shown here:\\n\\ntext = \"It\\'s a Stemming testing\"\\n\\nparsed_text = word_tokenize(text)\\n\\n# Initialize stemmer.\\nfrom nltk.stem.snowball import SnowballStemmer\\nstemmer = SnowballStemmer(\\'english\\')\\n\\n# Stem each word.\\n[(word, stemmer.stem(word)) for i, word in enumerate(parsed_text)\\n if word.lower() != stemmer.stem(parsed_text[i])]\\n\\nOutput\\n\\n[(\\'Stemming\\', \\'stem\\'), (\\'testing\\', \\'test\\')]\\n\\n1.4. Lemmatization\\n\\nA slight variant of stemming is lemmatization. The major difference between the two\\nprocesses is that stemming can often create nonexistent words, whereas lemmas are\\nactual words. An example of lemmatization is run as a base form for words like run‐\\nning and ran, or that the words better and good are considered the same lemma. The\\ncode for lemmatization using the TextBlob library is shown below:\\n\\ntext = \"This world has a lot of faces \"\\n\\nfrom textblob import Word\\nparsed_data= TextBlob(text).words\\n\\n352 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0c[(word, word.lemmatize()) for i, word in enumerate(parsed_data)\\n if word != parsed_data[i].lemmatize()]\\n\\nOutput\\n\\n[(\\'has\\', \\'ha\\'), (\\'faces\\', \\'face\\')]\\n\\n1.5. PoS tagging\\n\\nPart-of-speech (PoS) tagging is the process of assigning a token to its grammatical cat‐\\negory  (e.g.,  verb,  noun,  etc.)  in  order  to  understand  its  role  within  a  sentence.  PoS\\ntags  have  been  used  for  a  variety  of  NLP  tasks  and  are  extremely  useful  since  they\\nprovide a linguistic signal of how a word is being used within the scope of a phrase,\\nsentence, or document.\\n\\nAfter  a  sentence  is  split  into  tokens,  a  tagger,  or  PoS  tagger,  is  used  to  assign  each\\ntoken to a part-of-speech category. Historically, hidden Markov models (HMM) were\\nused to create such taggers. More recently, artificial neural networks have been lever‐\\naged. The code for PoS tagging using the TextBlob library is shown here:\\n\\ntext = \\'Google is looking at buying U.K. startup for $1 billion\\'\\nTextBlob(text).tags\\n\\nOutput\\n\\n[(\\'Google\\', \\'NNP\\'),\\n (\\'is\\', \\'VBZ\\'),\\n (\\'looking\\', \\'VBG\\'),\\n (\\'at\\', \\'IN\\'),\\n (\\'buying\\', \\'VBG\\'),\\n (\\'U.K.\\', \\'NNP\\'),\\n (\\'startup\\', \\'NN\\'),\\n (\\'for\\', \\'IN\\'),\\n (\\'1\\', \\'CD\\'),\\n (\\'billion\\', \\'CD\\')]\\n\\n1.6. Named entity recognition\\n\\nNamed  entity  recognition  (NER)  is  an  optional  next  step  in  data  preprocessing  that\\nseeks  to  locate  and  classify  named  entities  in  text  into  predefined  categories.  These\\ncategories  can  include  names  of  persons,  organizations,  locations,  expressions  of\\ntimes, quantities, monetary values, or percentages. The NER performed using spaCy\\nis shown below:\\n\\ntext = \\'Google is looking at buying U.K. startup for $1 billion\\'\\n\\nfor entity in nlp(text).ents:\\n    print(\"Entity: \", entity.text)\\n\\nNatural Language Processing: Theory and Concepts \\n\\n| \\n\\n353\\n\\n\\x0cOutput\\n\\nEntity:  Google\\nEntity:  U.K.\\nEntity:  $1 billion\\n\\nVisualizing  named  entities  in  text  using  the  displacy  module,  as  shown  in\\nFigure 10-2, can also be incredibly helpful in speeding up development and debug‐\\nging the code and training process:\\n\\nfrom spacy import displacy\\ndisplacy.render(nlp(text), style=\"ent\", jupyter = True)\\n\\nFigure 10-2. NER output\\n\\n1.7. spaCy: All of the above steps in one go.    All the preprocessing steps shown above can\\nbe performed in one step using spaCy. When we call nlp on a text, spaCy first tokeni‐\\nzes  the  text  to  produce  a  Doc  object.  The  Doc  is  then  processed  in  several  different\\nsteps.  This  is  also  referred  to  as  the  processing  pipeline.  The  pipeline  used  by  the\\ndefault models consists of a tagger, a parser, and an entity recognizer. Each pipeline\\ncomponent returns the processed Doc, which is then passed on to the next compo‐\\nnent, as demonstrated in Figure 10-3.\\n\\nFigure 10-3. spaCy pipeline (based on an image from the spaCy website.\\n\\nPython code text = \\'Google is looking at buying U.K. startup for $1 billion\\'\\ndoc = nlp(text)\\npd.DataFrame([[t.text, t.is_stop, t.lemma_, t.pos_]\\n              for t in doc],\\n             columns=[\\'Token\\', \\'is_stop_word\\', \\'lemma\\', \\'POS\\'])\\n\\nOutput\\n\\nToken\\nGoogle\\n\\nis_stop_word lemma POS\\nFalse\\n\\nGoogle\\n\\nPROPN\\n\\nis\\n\\nTrue\\n\\nlooking\\n\\nFalse\\n\\nat\\n\\nTrue\\n\\nbe\\n\\nlook\\n\\nat\\n\\nVERB\\n\\nVERB\\n\\nADP\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n354 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cToken\\nbuying\\n\\nU.K.\\n\\nstartup\\n\\nfor\\n\\n$\\n\\n1\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\nbillion\\n\\nis_stop_word lemma POS\\nVERB\\nFalse\\n\\nbuy\\n\\nFalse\\n\\nFalse\\n\\nTrue\\n\\nFalse\\n\\nFalse\\n\\nFalse\\n\\nU.K.\\n\\nPROPN\\n\\nstartup NOUN\\n\\nfor\\n\\n$\\n\\n1\\n\\nbillion\\n\\nADP\\n\\nSYM\\n\\nNUM\\n\\nNUM\\n\\nThe output for each of the preprocessing steps is shown in the preceding table. Given\\nthat spaCy performs a wide range of NLP-related tasks in a single step, it is a highly\\nrecommended  package.  As  such,  we  will  be  using  spaCy  extensively  in  our  case\\nstudies.\\n\\nIn addition to the above preprocessing steps, there are other frequently used prepro‐\\ncessing  steps,  such  as  lower  casing  or  nonalphanumeric  data  removing,  that  we  can\\nperform depending on the type of data. For example, data scraped from a website has\\nto be cleansed further, including the removal of HTML tags. Data from a PDF report\\nmust be converted into a text format.\\n\\nOther optional preprocessing steps include dependency parsing, coreference resolu‐\\ntion, triplet extraction, and relation extraction:\\n\\nDependency parsing\\n\\nAssigns a syntactic structure to sentences to make sense of how the words in the\\nsentence relate to each other.\\n\\nCoreference resolution\\n\\nThe process of connecting tokens that represent the same entity. It is common in\\nlanguages to introduce a subject with their name in one sentence and then refer\\nto them as him/her/it in subsequent sentences.\\n\\nTriplet extraction\\n\\nThe process of recording subject, verb, and object triplets when available in the\\nsentence structure.\\n\\nRelation extraction\\n\\nA  broader  form  of  triplet  extraction  in  which  entities  can  have  multiple\\ninteractions.\\n\\nThese  additional  steps  should  be  performed  only  if  they  will  help  with  the  task  at\\nhand. We will demonstrate examples of these preprocessing steps in the case studies\\nin this chapter.\\n\\nNatural Language Processing: Theory and Concepts \\n\\n| \\n\\n355\\n\\n\\x0c2. Feature Representation\\nThe vast majority of NLP-related data, such as news feed articles, PDF reports, social\\nmedia posts, and audio files, is created for human consumption. As such, it is often\\nstored in an unstructured format, which cannot be readily processed by computers.\\nIn order for the preprocessed information to be conveyed to the statistical inference\\nalgorithm, the tokens need to be translated into predictive features. A model is used\\nto embed raw text into a vector space.\\n\\nFeature representation involves two things:\\n\\n• A vocabulary of known words.\\n\\n• A measure of the presence of known words.\\n\\nSome of the feature representation methods are:\\n\\n• Bag of words\\n\\n• TF-IDF\\n\\n• Word embedding\\n\\n— Pretrained models (e.g., word2vec, GloVe, spaCy’s word embedding model)\\n— Customized deep learning–based feature representation1\\n\\nLet’s learn more about each of these methods.\\n\\n2.1. Bag of words—word count\\n\\nIn natural language processing, a common technique for extracting features from text\\nis to place all words that occur in the text in a bucket. This approach is called a bag of\\nwords  model.  It’s  referred  to  as  a  bag  of  words  because  any  information  about  the\\nstructure  of  the  sentence  is  lost.  In  this  technique,  we  build  a  single  matrix  from  a\\ncollection of texts, as shown in Figure 10-4, in which each row represents a token and\\neach  column  represents  a  document  or  sentence  in  our  corpus.  The  values  of  the\\nmatrix represent the count of the number of instances of the token appearing.\\n\\n1 A customized deep learning–based feature representation model is built in case study 1 of this chapter.\\n\\n356 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cFigure 10-4. Bag of words\\n\\nThe CountVectorizer from sklearn provides a simple way to both tokenize a collec‐\\ntion  of  text  documents  and  encode  new  documents  using  that  vocabulary.  The\\nfit_transform  function  learns  the  vocabulary  from  one  or  more  documents  and\\nencodes each document in the word as a vector:\\n\\nsentences = [\\n\\'The stock price of google jumps on the earning data today\\',\\n\\'Google plunge on China Data!\\'\\n]\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nvectorizer = CountVectorizer()\\nprint( vectorizer.fit_transform(sentences).todense() )\\nprint( vectorizer.vocabulary_ )\\n\\nOutput\\n\\n[[0 1 1 1 1 1 1 0 1 1 2 1]\\n [1 1 0 1 0 0 1 1 0 0 0 0]]\\n{\\'the\\': 10, \\'stock\\': 9, \\'price\\': 8, \\'of\\': 5, \\'google\\': 3, \\'jumps\\':\\\\\\n 4, \\'on\\': 6, \\'earning\\': 2, \\'data\\': 1, \\'today\\': 11, \\'plunge\\': 7,\\\\\\n \\'china\\': 0}\\n\\nWe can see an array version of the encoded vector showing a count of one occurrence\\nfor each word except the (index 10), which has an occurrence of two. Word counts\\nare a good starting point, but they are very basic. One issue with simple counts is that\\nsome words like the will appear many times, and their large counts will not be very\\nmeaningful  in  the  encoded  vectors.  These  bag  of  words  representations  are  sparse\\nbecause  the  vocabularies  are  vast,  and  a  given  word  or  document  would  be  repre‐\\nsented by a large vector comprised mostly of zero values.\\n\\nNatural Language Processing: Theory and Concepts \\n\\n| \\n\\n357\\n\\n\\x0c2.2. TF-IDF\\n\\nAn alternative is to calculate word frequencies, and by far the most popular method\\nfor that is TF-IDF, which stands for Term Frequency–Inverse Document Frequency:\\n\\nTerm Frequency\\n\\nThis summarizes how often a given word appears within a document.\\n\\nInverse Document Frequency\\n\\nThis downscales words that appear a lot across documents.\\n\\nPut simply, TF-IDF is a word frequency score that tries to highlight words that are\\nmore  interesting  (i.e.,  frequent  within  a  document,  but  not  across  documents).  The\\nTfidfVectorizer will tokenize documents, learn the vocabulary and the inverse docu‐\\nment frequency weightings, and allow you to encode new documents:\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nvectorizer = TfidfVectorizer(max_features=1000, stop_words=\\'english\\')\\nTFIDF = vectorizer.fit_transform(sentences)\\nprint(vectorizer.get_feature_names()[-10:])\\nprint(TFIDF.shape)\\nprint(TFIDF.toarray())\\n\\nOutput\\n\\n[\\'china\\', \\'data\\', \\'earning\\', \\'google\\', \\'jumps\\', \\'plunge\\', \\'price\\', \\'stock\\', \\\\\\n\\'today\\']\\n(2, 9)\\n[[0.         0.29017021 0.4078241  0.29017021 0.4078241  0.\\n  0.4078241  0.4078241  0.4078241 ]\\n [0.57615236 0.40993715 0.         0.40993715 0.         0.57615236\\n  0.         0.         0.        ]]\\n\\nIn the provided code snippet, a vocabulary of nine words is learned from the docu‐\\nments. Each word is assigned a unique integer index in the output vector. The sen‐\\ntences  are  encoded  as  a  nine-element  sparse  array,  and  we  can  review  the  final\\nscorings of each word with different values from the other words in the vocabulary.\\n\\n2.3. Word embedding\\n\\nA word embedding represents words and documents using a dense vector representa‐\\ntion. In an embedding, words are represented by dense vectors in which a vector rep‐\\nresents the projection of the word into a continuous vector space. The position of a\\nword within the vector space is learned from text and is based on the words that sur‐\\nround the word when it is used. The position of a word in the learned vector space is\\nreferred to as its embedding.\\n\\nSome  of  the  models  of  learning  word  embeddings  from  text  include  word2Vec,\\nspaCy’s pretrained word embedding model, and GloVe. In addition to these carefully\\ndesigned  methods,  a  word  embedding  can  be  learned  as  part  of  a  deep  learning\\n\\n358 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cmodel. This can be a slower approach, but it tailors the model to a specific training\\ndataset.\\n\\n2.3.1. Pretrained model: Via spaCy\\n\\nspaCy comes with built-in representation of text as vectors at different levels of word,\\nsentence,  and  document.  The  underlying  vector  representations  come  from  a  word\\nembedding  model,  which  generally  produces  a  dense,  multidimensional  semantic\\nrepresentation of words (as shown in the following example). The word embedding\\nmodel includes 20,000 unique vectors with 300 dimensions. Using this vector repre‐\\nsentation,  we  can  calculate  similarities  and  dissimilarities  between  tokens,  named\\nentities, noun phrases, sentences, and documents.\\n\\nThe word embedding in spaCy is performed by first loading the model and then pro‐\\ncessing text. The vectors can be accessed directly using the .vector attribute of each\\nprocessed token (i.e., word). The mean vector for the entire sentence is also calcula‐\\nted simply by using the vector, providing a very convenient input for machine learn‐\\ning models based on sentences:\\n\\ndoc = nlp(\"Apple orange cats dogs\")\\nprint(\"Vector representation of the sentence for first 10 features: \\\\n\", \\\\\\ndoc.vector[0:10])\\n\\nOutput:\\\\\\n\\nVector representation of the sentence for first 10 features:\\n [ -0.30732775 0.22351399 -0.110111   -0.367025   -0.13430001\\n   0.13790375 -0.24379876 -0.10736975  0.2715925   1.3117325 ]\\n\\nThe  vector  representation  of  the  sentence  for  the  first  10  features  of  the  pretrained\\nmodel is shown in the output.\\n\\n2.3.2. Pretrained model: Word2Vec using gensim package\\n\\nThe Python-based implementation of the word2vec model using the gensim package\\nis demonstrated here:\\n\\nfrom gensim.models import Word2Vec\\n\\nsentences = [\\n[\\'The\\',\\'stock\\',\\'price\\', \\'of\\', \\'Google\\', \\'increases\\'],\\n[\\'Google\\',\\'plunge\\',\\' on\\',\\'China\\',\\' Data!\\']]\\n\\n# train model\\nmodel = Word2Vec(sentences, min_count=1)\\n\\n# summarize the loaded model\\nwords = list(model.wv.vocab)\\nprint(words)\\nprint(model[\\'Google\\'][1:5])\\n\\nNatural Language Processing: Theory and Concepts \\n\\n| \\n\\n359\\n\\n\\x0cOutput\\n\\n[\\'The\\', \\'stock\\', \\'price\\', \\'of\\', \\'Google\\', \\'increases\\', \\'plunge\\', \\' on\\', \\'China\\',\\\\\\n\\' Data!\\']\\n[-1.7868265e-03 -7.6242397e-04  6.0105987e-05  3.5568199e-03\\n]\\n\\nThe vector representation of the sentence for the first five features of the pretrained\\nword2vec model is shown above.\\n\\n3. Inference\\nAs with other artificial intelligence tasks, an inference generated by an NLP applica‐\\ntion usually needs to be translated into a decision in order to be actionable. Inference\\nfalls  under  three  machine  learning  categories  covered  in  the  previous  chapters  (i.e.,\\nsupervised,  unsupervised,  and  reinforcement  learning).  While  the  type  of  inference\\nrequired  depends  on  the  business  problem  and  the  type  of  training  data,  the  most\\ncommonly used algorithms are supervised and unsupervised.\\n\\nOne of the most frequently used supervised methodologies in NLP is the Naive Bayes\\nmodel,  as  it  can  produce  reasonable  accuracy  using  simple  assumptions.  A  more\\ncomplex supervised methodology is using artificial neural network architectures. In\\npast  years,  these  architectures,  such  as  recurrent  neural  networks  (RNNs),  have\\ndominated NLP-based inference.\\n\\nMost of the existing literature in NLP focuses on supervised learning. As such, unsu‐\\npervised  learning  applications  constitute  a  relatively  less  developed  subdomain  in\\nwhich measuring document similarity is among the most common tasks. A popular\\nunsupervised technique applied in NLP is Latent Semantic Analysis (LSA). LSA looks\\nat relationships between a set of documents and the words they contain by producing\\na set of latent concepts related to the documents and terms. LSA has paved the way\\nfor  a  more  sophisticated  approach  called  Latent  Dirichlet  Allocation  (LDA),  under\\nwhich documents are modeled as a finite mixture of topics. These topics in turn are\\nmodeled as a finite mixture over words in the vocabulary. LDA has been extensively\\nused for topic modeling—a growing area of research in which NLP practitioners build\\nprobabilistic generative models to reveal likely topic attributions for words.\\n\\nSince  we  have  reviewed  many  supervised  and  unsupervised  learning  models  in  the\\nprevious  chapters,  we  will  provide  details  only  on  Naive  Bayes  and  LDA  models  in\\nthe next sections. These are used extensively in NLP and were not covered in the pre‐\\nvious chapters.\\n\\n3.1. Supervised learning example—Naive Bayes\\n\\nNaive Bayes is a family of algorithms based on applying Bayes’s theorem with a strong\\n(naive) assumption that every feature used to predict the category of a given sample is\\nindependent  of  the  others.  They  are  probabilistic  classifiers  and  therefore  will\\n\\n360 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0ccalculate  the  probability  of  each  category  using  Bayes’s  theorem.  The  category  with\\nthe highest probability will be output.\\n\\nIn  NLP,  a  Naive  Bayes  approach  assumes  that  all  word  features  are  independent  of\\neach other given the class labels. Due to this simplifying assumption, Naive Bayes is\\nvery  compatible  with  a  bag-of-words  word  representation,  and  it  has  been  demon‐\\nstrated to be fast, reliable, and accurate in a number of NLP applications. Moreover,\\ndespite its simplifying assumptions, it is competitive with (and at times even outper‐\\nforms) more complicated classifiers.\\n\\nLet us look at the usage of Naive Bayes for the inference in a sentiment analysis prob‐\\nlem. We take a dataframe in which there are two sentences with sentiments assigned\\nto each. In the next step, we convert the sentences into a feature representation using\\nCountVectorizer. The features and sentiments are used to train and test the model\\nusing Naive Bayes:\\n\\nsentences = [\\n\\'The stock price of google jumps on the earning data today\\',\\n\\'Google plunge on China Data!\\']\\nsentiment = (1, 0)\\ndata = pd.DataFrame({\\'Sentence\\':sentences,\\n        \\'sentiment\\':sentiment})\\n\\n# feature extraction\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nvect = CountVectorizer().fit(data[\\'Sentence\\'])\\nX_train_vectorized = vect.transform(data[\\'Sentence\\'])\\n\\n# Running naive bayes model\\nfrom sklearn.naive_bayes import MultinomialNB\\nclfrNB = MultinomialNB(alpha=0.1)\\nclfrNB.fit(X_train_vectorized, data[\\'sentiment\\'])\\n\\n#Testing the model\\npreds = clfrNB.predict(vect.transform([\\'Apple price plunge\\',\\\\\\n \\'Amazon price jumps\\']))\\npreds\\n\\nOutput\\n\\narray([0, 1])\\n\\nAs we can see, the Naive Bayes trains the model fairly well from the two sentences.\\nThe  model  gives  a  sentiment  of  zero  and  one  for  the  test  sentences  “Apple  price\\nplunge” and “Amazon price jumps,” respectively, given the sentences used for train‐\\ning  also  had  the  keywords  “plunge”  and  “jumps,”  with  corresponding  sentiment\\nassignments.\\n\\nNatural Language Processing: Theory and Concepts \\n\\n| \\n\\n361\\n\\n\\x0c3.2. Unsupervised learning example: LDA\\n\\nLDA  is  extensively  used  for  topic  modeling  because  it  tends  to  produce  meaningful\\ntopics that humans can interpret, assigns topics to new documents, and is extensible.\\nIt works by first making a key assumption: documents are generated by first selecting\\ntopics, and then, for each topic, a set of words. The algorithm then reverse engineers\\nthis process to find the topics in a document.\\n\\nIn the following code snippet, we show an implementation of LDA for topic model‐\\ning.  We  take  two  sentences  and  convert  the  sentences  into  a  feature  representation\\nusing  CountVectorizer.  These  features  and  the  sentiments  are  used  to  train  the\\nmodel and produce two smaller matrices representing the topics:\\n\\nsentences = [\\n\\'The stock price of google jumps on the earning data today\\',\\n\\'Google plunge on China Data!\\'\\n]\\n\\n#Getting the bag of words\\nfrom sklearn.decomposition import LatentDirichletAllocation\\nvect=CountVectorizer(ngram_range=(1, 1),stop_words=\\'english\\')\\nsentences_vec=vect.fit_transform(sentences)\\n\\n#Running LDA on the bag of words.\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nlda=LatentDirichletAllocation(n_components=3)\\nlda.fit_transform(sentences_vec)\\n\\nOutput\\n\\narray([[0.04283242, 0.91209846, 0.04506912],\\n       [0.06793339, 0.07059533, 0.86147128]])\\n\\nWe will be using LDA for topic modeling in the third case study of this chapter and\\nwill discuss the concepts and interpretation in detail.\\n\\nTo review, in order to approach any NLP-based problem, we need to follow the pre‐\\nprocessing,  feature  extraction,  and  inference  steps.  Now,  let’s  dive  into  the  case\\nstudies.\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading\\nStrategies\\nNatural language processing offers the ability to quantify text. One can begin to ask\\nquestions such as: How positive or negative is this news? and How can we quantify\\nwords?\\n\\nPerhaps  the  most  notable  application  of  NLP  is  its  use  in  algorithmic  trading.  NLP\\nprovides  an  efficient  means  of  monitoring  market  sentiments.  By  applying\\n\\n362 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cNLP-based  sentiment  analysis  techniques  to  news  articles,  reports,  social  media,  or\\nother web content, one can effectively determine whether those sources have a posi‐\\ntive or negative senitment score. Sentiment scores can be used as a directional signal\\nto buy stocks with positive scores and sell stocks with negative ones.\\n\\nTrading  strategies  based  on  text  data  are  becoming  more  popular  as  the  amount  of\\nunstructured data increases. In this case study we are going to look at how one can\\nuse NLP-based sentiments to build a trading strategy.\\n\\nIn this case study, we will focus on:\\n\\n• Producing news sentiments using supervised and unsupervised algorithms.\\n\\n• Enhancing sentiment analysis by using a deep learning model, such as LSTM.\\n\\n• Comparison of different sentiment generation methodologies for the purpose of\\n\\nbuilding a trading strategy.\\n\\n• Using sentiments and word vectors effectively as features in a trading strategy.\\n\\n• Collecting data from different sources and preprocessing it for sentiment analy‐\\n\\nsis.\\n\\n• Using NLP Python packages for sentiment analysis.\\n\\n• Building a framework for backtesting results of a trading strategy using available\\n\\nPython packages.\\n\\nThis case study combines concepts presented in previous chapters. The overall model\\ndevelopment steps of this case study are similar to the seven-step model development\\nin prior case studies, with slight modifications.\\n\\nBlueprint for Building a Trading Strategy Based on\\nSentiment Analysis\\n\\n1. Problem definition\\n\\nOur goal is to (1) use NLP to extract information from news headlines, (2) assign a\\nsentiment to that information, and (3) use sentiment analysis to build a trading strat‐\\negy.\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n363\\n\\n\\x0cThe data used for this case study will be from the following sources:\\n\\nNews headlines data compiled from the RSS feeds of several news websites\\n\\nFor the purpose of this study, we will look only at the headlines, not at the full\\ntext of the stories. Our dataset contains around 82,000 headlines from May 2011\\nthrough December 2018.2\\n\\nYahoo Finance website for stock data\\n\\nThe return data for stocks used in this case study is derived from Yahoo Finance\\nprice data.\\n\\nKaggle\\n\\nWe will use the labeled data of news sentiments for a classification-based senti‐\\nment analysis model. Note that this data may not be fully applicable to the case at\\nhand and is used here for demonstration purposes.\\n\\nStock market lexicon\\n\\nLexicon  refers  to  the  component  of  an  NLP  system  that  contains  information\\n(semantic, grammatical) about individual words or word strings. This is created\\nbased on stock market conversations in microblogging services.3\\n\\nThe key steps of this case study are outlined in Figure 10-5.\\n\\nFigure 10-5. Steps in a sentiment analysis–based trading strategy\\n\\nOnce we are done with preprocessing, we will look at the different sentiment analysis\\nmodels. The results from the sentiment analysis step are used to develop the trading\\nstrategy.\\n\\n2 The news can be downloaded by a simple web-scraping program in Python using packages such as Beautiful\\n\\nSoup. Readers should talk to the website or follow its terms of service in order to use the news for commercial\\npurpose.\\n\\n3 The source of this lexicon is Nuno Oliveira, Paulo Cortez, and Nelson Areal, “Stock Market Sentiment Lexi‐\\ncon Acquisition Using Microblogging Data and Statistical Measures,” Decision Support Systems 85 (March\\n2016): 62–73.\\n\\n364 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0c2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     The  first  set  of  libraries  to  be  loaded  are  the  NLP-\\nspecific libraries discussed above. Refer to the Jupyter notebook of this case study for\\ndetails of the other libraries.\\n\\nfrom textblob import TextBlob\\nimport spacy\\nimport nltk\\nimport warnings\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\\nnltk.download(\\'vader_lexicon\\')\\nnlp = spacy.load(\"en_core_web_lg\")\\n\\n2.2. Loading the data.    In this step, we load the stock price data from Yahoo Finance.\\nWe select 10 stocks for this case study. These stocks are some of the largest stocks in\\nthe S&P 500 by market share:\\n\\ntickers = [\\'AAPL\\',\\'MSFT\\',\\'AMZN\\',\\'GOOG\\',\\'FB\\',\\'WMT\\',\\'JPM\\',\\'TSLA\\',\\'NFLX\\',\\'ADBE\\']\\nstart = \\'2010-01-01\\'\\nend = \\'2018-12-31\\'\\ndf_ticker_return = pd.DataFrame()\\nfor ticker in tickers:\\n    ticker_yf = yf.Ticker(ticker)\\n    if df_ticker_return.empty:\\n        df_ticker_return = ticker_yf.history(start = start, end = end)\\n        df_ticker_return[\\'ticker\\']= ticker\\n    else:\\n        data_temp = ticker_yf.history(start = start, end = end)\\n        data_temp[\\'ticker\\']= ticker\\n        df_ticker_return = df_ticker_return.append(data_temp)\\ndf_ticker_return.to_csv(r\\'Data\\\\Step3.2_ReturnData.csv\\')\\n\\ndf_ticker_return.head(2)\\n\\nThe  data  contains  the  price  and  volume  data  of  the  stocks  along  with  their  ticker\\nname. In the next step, we look at the news data.\\n\\n3. Data preparation\\n\\nIn this step, we load and preprocess the news data, followed by combining the news\\ndata  with  the  stock  return  data.  This  combined  dataset  will  be  used  for  the  model\\ndevelopment.\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n365\\n\\n\\x0c3.1. Preprocessing news data.    The news data is downloaded from the News RSS feed,\\nand the file is available in JSON format. The JSON files for different dates are kept\\nunder  a  zipped  folder.  The  data  is  downloaded  using  the  standard  web-scraping\\nPython package Beautiful Soup, which is an open source framework. Let us look at\\nthe content of the downloaded JSON file:\\n\\nz = zipfile.ZipFile(\"Data/Raw Headline Data.zip\", \"r\")\\ntestFile=z.namelist()[10]\\nfileData= z.open(testFile).read()\\nfileDataSample = json.loads(fileData)[\\'content\\'][1:500]\\nfileDataSample\\n\\nOutput\\n\\n\\'li class=\"n-box-item date-title\" data-end=\"1305172799\" data-start=\"1305086400\"\\ndata-txt=\"Tuesday, December 17, 2019\">Wednesday, May 11,2011</li><li\\nclass=\"n-box-item sa-box-item\" data-id=\"76179\" data-ts=\"1305149244\"><div\\nclass=\"media media-overflow-fix\"><div class-\"media-left\"><a class=\"box-ticker\"\\nhref=\"/symbol/CSCO\" target=\"blank\">CSCO</a></div><div class=\"media-body\"<h4\\nclass=\"media-heading\"><a href=\"/news/76179\" sasource=\"on_the_move_news_\\nfidelity\" target=\"_blank\">Cisco (NASDAQ:CSCO): Pr\\'\\n\\nWe can see that the JSON format is not suitable for the algorithm. We need to get the\\nnews from the JSONs. Regex becomes the vital part of this step. Regex can find a pat‐\\ntern in the raw, messy text and perform actions accordingly. The following function\\nparses HTML by using information encoded in the JSON file:\\n\\ndef jsonParser(json_data):\\n    xml_data = json_data[\\'content\\']\\n\\n    tree = etree.parse(StringIO(xml_data), parser=etree.HTMLParser())\\n\\n    headlines = tree.xpath(\"//h4[contains(@class, \\'media-heading\\')]/a/text()\")\\n    assert len(headlines) == json_data[\\'count\\']\\n\\n    main_tickers = list(map(lambda x: x.replace(\\'/symbol/\\', \\'\\'),\\\\\\n           tree.xpath(\"//div[contains(@class, \\'media-left\\')]//a/@href\")))\\n    assert len(main_tickers) == json_data[\\'count\\']\\n    final_headlines = [\\'\\'.join(f.xpath(\\'.//text()\\')) for f in\\\\\\n           tree.xpath(\"//div[contains(@class, \\'media-body\\')]/ul/li[1]\")]\\n    if len(final_headlines) == 0:\\n        final_headlines = [\\'\\'.join(f.xpath(\\'.//text()\\')) for f in\\\\\\n           tree.xpath(\"//div[contains(@class, \\'media-body\\')]\")]\\n        final_headlines = [f.replace(h, \\'\\').split(\\'\\\\xa0\\')[0].strip()\\\\\\n                           for f,h in zip (final_headlines, headlines)]\\n    return main_tickers, final_headlines\\n\\nLet us see how the output looks like after running the JSON parser:\\n\\njsonParser(json.loads(fileData))[1][1]\\n\\n366 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cOutput\\n\\n\\'Cisco Systems (NASDAQ:CSCO) falls further into the red on FQ4\\n guidance of $0.37-0.39 vs. $0.42 Street consensus. Sales seen flat\\n to +2% vs. 8% Street view. CSCO recently -2.1%.\\'\\n\\nAs  we  can  see,  the  output  is  converted  into  a  more  readable  format  after  JSON\\nparsing.\\n\\nWhile  evaluating  the  sentiment  analysis  models,  we  also  analyze  the  relationship\\nbetween  the  sentiments  and  subsequent  stock  performance.  In  order  to  understand\\nthe  relationship,  we  use  event  return,  which  is  the  return  that  corresponds  to  the\\nevent. We do this because at times the news is reported late (i.e., after market partici‐\\npants are aware of the announcement) or after market close. Having a slightly wider\\nwindow ensures that we capture the essence of the event. Event return is defined as:\\n\\nRt –1 + Rt + Rt +1\\n\\nwhere Rt –1, Rt +1 are the returns before and after the news data, and Rt is the return on\\nthe day of the news (i.e., time t).\\n\\nLet us extract the event return from the data:\\n\\n#Computing the return\\ndf_ticker_return[\\'ret_curr\\'] = df_ticker_return[\\'Close\\'].pct_change()\\n#Computing the event return\\ndf_ticker_return[\\'eventRet\\'] = df_ticker_return[\\'ret_curr\\']\\\\\\n + df_ticker_return[\\'ret_curr\\'].shift(-1) + df_ticker_return[\\'ret_curr\\'].shift(1)\\n\\nNow we have all the data in place. We will prepare a combined dataframe, which will\\nhave  the  news  headlines  mapped  to  the  date,  the  returns  (event  return,  current\\nreturn, and next day’s return), and stock ticker. This dataframe will be used for build‐\\ning the sentiment analysis model and the trading strategy:\\n\\ncombinedDataFrame = pd.merge(data_df_news, df_ticker_return, how=\\'left\\', \\\\\\nleft_on=[\\'date\\',\\'ticker\\'], right_on=[\\'date\\',\\'ticker\\'])\\ncombinedDataFrame = combinedDataFrame[combinedDataFrame[\\'ticker\\'].isin(tickers)]\\ndata_df = combinedDataFrame[[\\'ticker\\',\\'headline\\',\\'date\\',\\'eventRet\\',\\'Close\\']]\\ndata_df = data_df.dropna()\\ndata_df.head(2)\\n\\nOutput\\n\\nticker headline\\nAMZN Whole Foods (WFMI) –5.2% following a downgrade… 2011-05-02\\n\\ndate\\n\\neventRet\\n0.017650\\n\\nClose\\n201.19\\n\\n5\\n\\n11 NFLX\\n\\nNetflix (NFLX +1.1%) shares post early gains a…\\n\\n2011-05-02 –0.013003\\n\\n33.88\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n367\\n\\n\\x0cLet us look at the overall shape of the data:\\n\\nprint(data_df.shape, data_df.ticker.unique().shape)\\n\\nOutput\\n\\n(2759, 5) (10,)\\n\\nIn  this  step,  we  prepared  a  clean  dataframe  that  has  ticker,  headline,  event  return,\\nreturn for a given day, and future return for 10 stock tickers, totaling 2,759 rows of\\ndata. Let us evaluate the models for sentiment analysis in the next step.\\n\\n4. Evaluate models for sentiment analysis\\n\\nIn this section, we will go through the following three approaches of computing sen‐\\ntiments for the news:\\n\\n• Predefined model—TextBlob package\\n\\n• Tuned model—classification algorithms and LSTM\\n\\n• Model based on financial lexicon\\n\\nLet us go through the steps.\\n\\n4.1. Predefined model—TextBlob package.    The TextBlob sentiment function is a pre‐\\ntrained model based on the Naive Bayes classification algorithm. The function maps\\nadjectives  that  are  frequently  found  in  movie  reviews4  to  sentiment  polarity  scores\\nranging  from  –1  to  +1  (negative  to  positive),  converting  a  sentence  to  a  numerical\\nvalue. We apply this on all headline articles. An example of getting the sentiment for\\na news text is shown below:\\n\\ntext = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share in Frankfurt, \\\\\\ntouching their\\nhighest level in 14 months, after the U.S. government said \\\\\\n a $25M glyphosate decision against the\\ncompany should be reversed.\"\\n\\nTextBlob(text).sentiment.polarity\\n\\nOutput\\n\\n0.5\\n\\n4 We also train a sentiment analysis model on the financial data in the subsequent section and compare the\\n\\nresults against the TextBlob model.\\n\\n368 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cThe sentiment for the statement is 0.5. We apply this on all headlines we have in the\\ndata:\\n\\ndata_df[\\'sentiment_textblob\\'] = [TextBlob(s).sentiment.polarity for s in \\\\\\ndata_df[\\'headline\\']]\\n\\nLet us inspect the scatterplot of the sentiments and returns to examine the correlation\\nbetween the two for all 10 stocks.\\n\\nA plot for a single stock (APPL) is also shown in the following chart (see the code in\\nthe Jupyter notebook in the GitHub repository for this book for more details on the\\ncode):\\n\\nFrom the scatterplots, we can see that there is not a strong relationship between the\\nnews and the sentiments. The correlation between return and sentiments is positive\\n(4.27%), which means that news with positive sentiments leads to positive return and\\nis  expected.  However,  the  correlation  is  not  very  high.  Even  looking  at  the  overall\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n369\\n\\n\\x0cscatterplot,  we  see  the  majority  of  the  sentiments  concentrated  around  zero.  This\\nraises the question of whether a sentiment score trained on movie reviews is appro‐\\npriate for stock prices. The sentiment_assessments attribute lists the underlying val‐\\nues for each token and can help us understand the reason for the overall sentiment of\\na sentence:\\n\\ntext = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share\\\\\\nin Frankfurt, touching their highest level in 14 months, after the\\\\\\nU.S. government said a $25M glyphosate decision against the company\\\\\\nshould be reversed.\"\\nTextBlob(text).sentiment_assessments\\n\\nOutput\\n\\nSentiment(polarity=0.5, subjectivity=0.5, assessments=[([\\'touching\\'], 0.5, 0.5, \\\\\\nNone)])\\n\\nWe  see  that  the  statement  has  a  positive  sentiment  of  0.5,  but  it  appears  the  word\\n“touching” gave rise to the positive sentiment. More intuitive words, such as “high,”\\ndo not. This example shows that the context of the training data is important for the\\nsentiment score to be meaningful. There are many predefined packages and functions\\navailable for sentiment analysis, but it is important to be careful and have a thorough\\nunderstanding of the problem’s context before using a function or an algorithm for\\nsentiment analysis.\\n\\nFor this case study, we may need sentiments trained on the financial news. Let us take\\na look at that in the next step.\\n\\n4.2. Supervised learning—classification algorithms and LSTM\\n\\nIn this step, we develop a customized model for sentiment analysis based on available\\nlabeled data. The label data for this is obtained from the Kaggle website:\\n\\nsentiments_data = pd.read_csv(r\\'Data\\\\LabelledNewsData.csv\\', \\\\\\nencoding=\"ISO-8859-1\")\\nsentiments_data.head(1)\\n\\nOutput\\n\\ndatetime\\n1/16/2020 5:25\\n\\nheadline\\n$MMM fell on hard times but could be set to re…\\n\\nsentiment\\n\\nticker\\nMMM 0\\n\\n1/11/2020 6:43 Wolfe Research Upgrades 3M $MMM to ¡§Peer Perf… MMM 1\\n\\n0\\n\\n1\\n\\nThe  data  has  headlines  for  the  news  across  30  different  stocks,  totaling  9,470  rows,\\nand  has  sentiments  labeled  zero  and  one.  We  perform  the  classification  steps  using\\nthe classification model development template presented in Chapter 6.\\n\\nIn order to run a supervised learning model, we first need to convert the news head‐\\nlines  into  a  feature  representation.  For  this  exercise,  the  underlying  vector\\n\\n370 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0crepresentations come from a spaCy word embedding model, which generally produces\\na dense, multidimensional semantic representation of words (as shown in the exam‐\\nple  below).  The  word  embedding  model  includes  20,000  unique  vectors  with  300\\ndimensions. We apply this on all headlines in the data processed in the previous step:\\n\\nall_vectors = pd.np.array([pd.np.array([token.vector for token in nlp(s) ]).\\\\\\nmean(axis=0)*pd.np.ones((300))\\\\\\n for s in sentiments_data[\\'headline\\']])\\n\\nNow  that  we  have  prepared  the  independent  variable,  we  train  the  classification\\nmodel in a similar manner as discussed in Chapter 6. We have the sentiments label\\nzero or one as the dependent variable. We first divide the data into training and test\\nsets and run the key classification models (i.e., logistic regression, CART, SVM, ran‐\\ndom forest, and artificial neural network).\\n\\nWe will also include LSTM, which is an RNN-based model,5 in the list of models con‐\\nsidered. An RNN-based model performs well for NLP, because it stores the informa‐\\ntion  for  current  features  as  well  neighboring  ones  for  prediction.  It  maintains  a\\nmemory based on past information, which enables the model to predict the current\\noutput conditioned on long distance features and looks at the words in the context of\\nthe entire sentence, rather than simply looking at the individual words.\\n\\nFor us to be able to feed the data into our LSTM model, all input documents must\\nhave the same length. We use the Keras  tokenizer function to tokenize the strings\\nand then use texts_to_sequences to make sequences of words. More details can be\\nfound on the Keras website. We will limit the maximum review length to max_words\\nby truncating longer reviews and pad shorter reviews with a null value (0). We can\\naccomplish this using the pad_sequences function, also in Keras. The third parame‐\\nter is the input_length (set to 50), which is the length of each comment sequence:\\n\\n### Create sequence\\nvocabulary_size = 20000\\ntokenizer = Tokenizer(num_words= vocabulary_size)\\ntokenizer.fit_on_texts(sentiments_data[\\'headline\\'])\\nsequences = tokenizer.texts_to_sequences(sentiments_data[\\'headline\\'])\\nX_LSTM = pad_sequences(sequences, maxlen=50)\\n\\nIn  the  following  code  snippet,  we  use  the  Keras  library  to  build  an  artificial  neural\\nnetwork classifier based on an underlying LSTM model. The network starts with an\\nembedding  layer.  This  layer  lets  the  system  expand  each  token  to  a  larger  vector,\\nallowing the network to represent a word in a meaningful way. The layer takes 20,000\\nas  the  first  argument  (i.e.,  the  size  of  our  vocabulary)  and  300  as  the  second  input\\nparameter (i.e., the dimension of the embedding). Finally, given that this is a classifi‐\\nlabeled  as  zero  or  one,  the\\ncation  problem  and  the  output  needs  to  be \\n\\n5 Refer to Chapter 5 for more details on RNN models.\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n371\\n\\n\\x0cKerasClassifier function is used as a wrapper over the LSTM model to produce a\\nbinary (zero or one) output:\\n\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\ndef create_model(input_length=50):\\n    model = Sequential()\\n    model.add(Embedding(20000, 300, input_length=50))\\n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\\n    model.add(Dense(1, activation=\\'sigmoid\\'))\\n    model.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\', \\\\\\n    metrics=[\\'accuracy\\'])\\n    return model\\nmodel_LSTM = KerasClassifier(build_fn=create_model, epochs=3, verbose=1, \\\\\\n  validation_split=0.4)\\nmodel_LSTM.fit(X_train_LSTM, Y_train_LSTM)\\n\\nThe comparison of all the machine learning models is as follows:\\n\\nAs expected, the LSTM model has the best performance in the test set (accuracy of\\n96.7%) as compared to all other models. The performance of the ANN, with a train‐\\ning set accuracy of 99% and a test set accuracy of 93.8%, is comparable to the LSTM-\\nbased model. The performances of random forest (RF), SVM, and logistic regression\\n(LR) are reasonable as well. CART and KNN do not perform as well as other models.\\nCART shows high overfitting. Let us use the LSTM model for the computation of the\\nsentiments in the data in the following steps.\\n\\n4.3. Unsupervised—model based on a financial lexicon\\n\\nIn this case study, we update the VADER lexicon with words and sentiments from a\\nlexicon adapted to stock market conversations in microblogging services:\\n\\n372 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cLexicons\\n\\nSpecial  dictionaries  or  vocabularies  that  have  been  created  for  analyzing  senti‐\\nments. Most lexicons have a list of positive and negative polar words with some\\nscore  associated  with  them.  Using  various  techniques,  such  as  the  position  of\\nwords, the surrounding words, context, parts of speech, and phrases, scores are\\nassigned  to  the  text  documents  for  which  we  want  to  compute  the  sentiment.\\nAfter aggregating these scores, we get the final sentiment:\\n\\nVADER (Valence Aware Dictionary for Sentiment Reasoning)\\n\\nA prebuilt sentiment analysis model included in the NLTK package. It can give\\nboth positive and negative polarity scores as well as the strength of the emotion\\nof a text sample. It is rule-based and relies heavily on human-rated texts. These\\nare  words  or  any  textual  form  of  communication  labeled  according  to  their\\nsemantic orientation as either positive or negative.\\n\\nThis lexical resource was automatically created using diverse statistical measures and\\na  large  set  of  labeled  messages  from  StockTwits,  which  is  a  social  media  platform\\ndesigned  for  sharing  ideas  among  investors,  traders,  and  entrepreneurs.6  The  senti‐\\nments are between –1 and 1, similar to the sentiments from TextBlob. In the follow‐\\ning code snippet, we train the model based on the financial sentiments:\\n\\n# stock market lexicon\\nsia = SentimentIntensityAnalyzer()\\nstock_lex = pd.read_csv(\\'Data/lexicon_data/stock_lex.csv\\')\\nstock_lex[\\'sentiment\\'] = (stock_lex[\\'Aff_Score\\'] + stock_lex[\\'Neg_Score\\'])/2\\nstock_lex = dict(zip(stock_lex.Item, stock_lex.sentiment))\\nstock_lex = {k:v for k,v in stock_lex.items() if len(k.split(\\' \\'))==1}\\nstock_lex_scaled = {}\\nfor k, v in stock_lex.items():\\n    if v > 0:\\n        stock_lex_scaled[k] = v / max(stock_lex.values()) * 4\\n    else:\\n        stock_lex_scaled[k] = v / min(stock_lex.values()) * -4\\n\\nfinal_lex = {}\\nfinal_lex.update(stock_lex_scaled)\\nfinal_lex.update(sia.lexicon)\\nsia.lexicon = final_lex\\n\\n6 The source of this lexicon is Nuno Oliveira, Paulo Cortez, and Nelson Areal, “Stock Market Sentiment Lexi‐\\ncon Acquisition Using Microblogging Data and Statistical Measures,” Decision Support Systems 85 (March\\n2016): 62–73.\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n373\\n\\n\\x0cLet us check the sentiment of a news item:\\n\\ntext = \"AAPL is trading higher after reporting its October sales\\\\\\nrose 12.6% M/M. It has seen a 20%+ jump in orders\"\\n\\nsia.polarity_scores(text)[\\'compound\\']\\n\\nOutput\\n\\n0.4535\\n\\nWe get the sentiments for all the news headlines based in our dataset:\\n\\nvader_sentiments = pd.np.array([sia.polarity_scores(s)[\\'compound\\']\\\\\\n for s in data_df[\\'headline\\']])\\n\\nLet us look at the relationship between the returns and sentiments, which is compu‐\\nted using the lexicon-based methodology for the entire dataset.\\n\\nThere are not many instances of high returns for lower sentiment scores, but the data\\nmay not be very clear. We will look deeper into the comparison of different types of\\nsentiment analysis in the next section.\\n\\n4.4. Exploratory data analysis and comparison\\n\\nIn this section, we compare the sentiments computed using the different techniques\\npresented above. Let us look at the sample headlines and the sentiments from three\\ndifferent methodologies, followed by a visual analysis:\\n\\n374 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0csentiment_textblob sentiment_LSTM sentiment_lex\\n0.036667\\n\\n0.5478\\n\\n1\\n\\n4620\\n\\nticker headline\\nTSM\\n\\nTSMC (TSM +1.8%) is trading higher after\\nreporting its October sales rose 12.6% M/M.\\nDigiTimes adds TSMC has seen a 20%+ jump\\nin orders from QCOM, NVDA, SPRD, and\\nMediatek. The numbers suggest TSMC could\\nbeat its Q4 guidance (though December tends\\nto be weak), and that chip demand could be\\nstabilizing after getting hit hard by inventory\\ncorrections. (earlier) (UMC sales)\\n\\nLooking at one of the headlines, the sentiment from this sentence is positive. How‐\\never, the TextBlob sentiment result is smaller in magnitude, suggesting that the senti‐\\nment  is  more  neutral.  This  points  back  to  the  previous  assumption  that  the  model\\ntrained  on  movie  sentiments  likely  will  not  be  accurate  for  stock  sentiments.  The\\nclassification-based  model  correctly  suggests  the  sentiment  is  positive,  but  it  is\\nbinary.  Sentiment_lex  has  a  more  intuitive  output  with  a  significantly  positive\\nsentiment.\\n\\nLet us review the correlation of all the sentiments from different methodologies ver‐\\nsus returns:\\n\\nAll  sentiments  have  positive  relationships  with  the  returns,  which  is  intuitive  and\\nexpected.  The  sentiments  from  the  lexicon  methodology  are  highest,  which  means\\nthe  stock’s  event  return  can  be  predicted  the  best  using  the  lexicon  methodology.\\nRecall that this methodology leverages financial terms in the model. The LSTM-based\\nmethod  also  performs  better  than  the  TextBlob  approach,  but  the  performance  is\\nslightly worse compared to the lexicon-based methodology.\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n375\\n\\n\\x0cLet us look at the performance of the methodology at the ticker level. We chose a few\\ntickers with the highest market cap for the analysis:\\n\\nLooking at the chart, the correlation from the lexicon methodology is highest across\\nall  stock  tickers,  which  corroborates  the  conclusion  from  the  previous  analysis.  It\\nmeans  the  returns  can  be  predicted  the  best  using  the  lexicon  methodology.  The\\nTextBlob-based sentiments show unintuitive results in some cases, such as with JPM.\\n\\n376 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cLet us look at the scatterplot for lexicon versus TextBlob methodologies for AMZN\\nand GOOG. We will set the LSTM-based method aside since the binary sentiments\\nwill not be meaningful in the scatterplot:\\n\\nThe  lexicon-based  sentiments  on  the  left  show  a  positive  relationship  between  the\\nsentiments  and  returns.  Some  of  the  points  with  the  highest  returns  are  associated\\nwith the most positive news. Also, the scatterplot is more uniformly distributed in the\\ncase of lexicon as compared to TextBlob. The sentiments for TextBlob are concentra‐\\nted around zero, probably because the model is not able to categorize financial senti‐\\nments well. For the trading strategy, we will be using the lexicon-based sentiments, as\\nthese are the most appropriate based on the analysis in this section. The LSTM-based\\nsentiments are good as well, but they are labeled either zero or one. The more granu‐\\nlar lexicon-based sentiments are preferred.\\n\\n5. Models evaluation—building a trading strategy\\n\\nThe sentiment data can be used in several ways for building a trading strategy. The\\nsentiments  can  be  used  as  a  stand-alone  signal  to  decide  buy,  sell,  or  hold  actions.\\nThe  sentiment  score  or  the  word  vectors  can  also  be  used  to  predict  the  return  or\\nprice of a stock. That prediction can be used to build a trading strategy.\\n\\nIn  this  section,  we  demonstrate  a  trading  strategy  in  which  we  buy  or  sell  a  stock\\nbased on the following approach:\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n377\\n\\n\\x0c• Buy a stock when the change in sentiment score (current sentiment score/previ‐\\nous  sentiment  score)  is  greater  than  0.5.  Sell  a  stock  when  the  change  in  senti‐\\nment  score  is  less  than  –0.5.  The  sentiment  score  used  here  is  based  on  the\\nlexicon-based sentiments computed in the previous step.\\n\\n• In addition to the sentiments, we use moving average (based on the last 15 days)\\n\\nwhile making a buy or sell decision.\\n\\n• Trades (i.e., buy or sell) are in 100 shares. The initial amount available for trading\\n\\nis set to $100,000.\\n\\nThe strategy threshold, the lot size, and the initial capital can be tweaked depending\\non the performance of the strategy.\\n\\n5.1. Setting up a strategy.    To set up the trading strategy, we use backtrader, which is a\\nconvenient  Python-based  framework  for  implementing  and  backtesting  trading\\nstrategies.  Backtrader  allows  us  to  write  reusable  trading  strategies,  indicators,  and\\nanalyzers instead of having to spend time building infrastructure. We use the Quick‐\\nstart code in the backtrader documentation as a base and adapt it to our sentiment-\\nbased trading strategy.\\n\\nThe following code snippet summarizes the buy and sell logic for the strategy. Refer\\nto the Jupyter notebook of this case study for the detailed implementation:\\n\\n# buy if current close more than simple moving average (sma)\\n# AND sentiment increased by >= 0.5\\nif self.dataclose[0] > self.sma[0] and self.sentiment - prev_sentiment >= 0.5:\\n  self.order = self.buy()\\n\\n# sell if current close less than simple moving average(sma)\\n# AND sentiment decreased by >= 0.5\\nif self.dataclose[0] < self.sma[0] and self.sentiment - prev_sentiment <= -0.5:\\n  self.order = self.sell()\\n\\n5.2. Results for individual stocks.    First, we run our strategy on GOOG and look at the\\nresults:\\n\\nticker = \\'GOOG\\'\\nrun_strategy(ticker, start = \\'2012-01-01\\', end = \\'2018-12-12\\')\\n\\nThe output shows the trading log for some of the days and the final return:\\n\\nOutput\\n\\nStarting Portfolio Value: 100000.00\\n2013-01-10, Previous Sentiment 0.08, New Sentiment 0.80 BUY CREATE, 369.36\\n2014-07-17, Previous Sentiment 0.73, New Sentiment -0.22 SELL CREATE, 572.16\\n2014-07-18, OPERATION PROFIT, GROSS 22177.00, NET 22177.00\\n2014-07-18, Previous Sentiment -0.22, New Sentiment 0.77 BUY CREATE, 593.45\\n2014-09-12, Previous Sentiment 0.66, New Sentiment -0.05 SELL CREATE, 574.04\\n\\n378 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0c2014-09-15, OPERATION PROFIT, GROSS -1876.00, NET -1876.00\\n2015-07-17, Previous Sentiment 0.01, New Sentiment 0.90 BUY CREATE, 672.93\\n.\\n.\\n.\\n2018-12-11, Ending Value 149719.00\\n\\nWe  analyze  the  backtesting  result  in  the  following  plot  produced  by  the  backtrader\\npackage. Refer to the Jupyter notebook of this case study for the detailed version of\\nthis chart.\\n\\nThe results show an overall profit of $49,719. The chart is a typical chart7 produced\\nby the backtrader package and is divided into four panels:\\n\\nTop panel\\n\\nThe top panel is the cash value observer. It keeps track of the cash and the total\\nportolio value during the life of the backtesting run. In this run, we started with\\n$100,000 and ended with $149,719.\\n\\nSecond panel\\n\\nThis panel is the trade observer. It shows the realized profit/loss of each trade. A\\ntrade  is  defined  as  opening  a  position  and  taking  the  position  back  to  zero\\n\\n7 Refer to the plotting section of the backtrader website for more details on the backtrader’s charts and the\\n\\npanels.\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n379\\n\\n\\x0c(directly  or  crossing  over  from  long  to  short  or  short  to  long).  Looking  at  this\\npanel, five out of eight trades are profitable for the strategy.\\n\\nThird panel\\n\\nThis  panel  is  buy  sell  observer.  It  indicates  where  buy  and  sell  operations  have\\ntaken  place.  In  general,  we  see  that  the  buy  action  takes  place  when  the  stock\\nprice is increasing, and the sell action takes place when the stock price has started\\ndeclining.\\n\\nBottom panel\\n\\nThis panel shows the sentiment score, varying between –1 and 1.\\n\\nNow  we  choose  one  of  the  days  (2015-07-17)  when  a  buy  action  was  triggered  and\\nanalyze the news for Google on that and the previous day:\\n\\nGOOG_ticker= data_df[data_df[\\'ticker\\'].isin([ticker])]\\nNew= list(GOOG_ticker[GOOG_ticker[\\'date\\'] ==  \\'2015-07-17\\'][\\'headline\\'])\\nOld= list(GOOG_ticker[GOOG_ticker[\\'date\\'] ==  \\'2015-07-16\\'][\\'headline\\'])\\nprint(\"Current News:\",New,\"\\\\n\\\\n\",\"Previous News:\", Old)\\n\\nOutput\\n\\nCurrent News: [\"Axiom Securities has upgraded Google (GOOG +13.4%, GOOGL +14.8%)\\nto Buy following the company\\'s Q2 beat and investor-pleasing comments about\\nspending discipline, potential capital returns, and YouTube/mobile growth. MKM\\nhas launched coverage at Buy, and plenty of other firms have hiked their targets.\\nGoogle\\'s market cap is now above $450B.\"]\\n\\nPrevious News: [\"While Google\\'s (GOOG, GOOGL) Q2 revenue slightly missed\\nestimates when factoring traffic acquisitions costs (TAC), its ex-TAC revenue of\\n$14.35B was slightly above a $14.3B consensus. The reason: TAC fell to 21% of ad\\nrevenue from Q1\\'s 22% and Q2 2014\\'s 23%. That also, of course, helped EPS beat\\nestimates.\", \\'Google (NASDAQ:GOOG): QC2 EPS of $6.99 beats by $0.28.\\']\\n\\nClearly, the news on the selected day mentions the upgrade of Google, a piece of posi‐\\ntive news. The previous day mentions the revenue missing estimates, which is nega‐\\ntive  news.  Hence,  there  was  a  significant  change  of  the  news  sentiment  on  the\\nselected day, resulting in a buy action triggered by the trading algorithm.\\n\\nNext, we run the strategy for FB:\\n\\nticker = \\'FB\\'\\nrun_strategy(ticker, start = \\'2012-01-01\\', end = \\'2018-12-12\\')\\n\\nOutput\\n\\nStart Portfolio value: 100000.00\\nFinal Portfolio Value: 108041.00\\nProfit: 8041.00\\n\\n380 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cThe details of the backtesting results of the strategy are as follows:\\n\\nTop panel\\n\\nThe cash value panel shows an overall profit of $8,041.\\n\\nSecond panel\\n\\nThe trade observer panel shows that six out of seven actions were profitable.\\n\\nThird panel\\n\\nThe buy/sell observer shows that in general the buy (sell) action took place when\\nthe stock price was increasing (decreasing).\\n\\nBottom panel\\n\\nIt  shows  a  high  number  of  positive  sentiments  for  FB  around  the  2013–2014\\nperiod.\\n\\n5.3. Results for multiple stocks.     In  the  previous  step,  we  executed  the  trading  strategy\\non  individual  stocks.  Here,  we  run  it  on  all  10  stocks  for  which  we  computed  the\\nsentiments:\\n\\nresults_tickers = {}\\nfor ticker in tickers:\\n    results_tickers[ticker] = run_strategy(ticker, start = \\'2012-01-01\\', \\\\\\n    end = \\'2018-12-12\\')\\npd.DataFrame.from_dict(results_tickers).set_index(\\\\\\n  [pd.Index([\"PerUnitStartPrice\", StrategyProfit\\'])])\\n\\nCase Study 1: NLP and Sentiment Analysis–Based Trading Strategies \\n\\n| \\n\\n381\\n\\n\\x0cOutput\\n\\nThe  strategy  performs  quite  well  and  yields  an  overall  profit  for  all  the  stocks.  As\\nmentioned before, the buy and sell actions are performed in a lot size of 100. Hence,\\nthe dollar amount used is proportional to the stock price. We see the highest nominal\\nprofit  from  AMZN  and  GOOG,  which  is  primarily  attributed  to  the  high  dollar\\namounts  invested  for  these  stocks  given  their  high  stock  price.  Other  than  overall\\nprofit, several other metrics, such as Sharpe ratio and maximum drawdown, can be\\nused to analyze the performance.\\n\\n5.4. Varying the strategy time period\\n\\nIn the previous analysis, we used the time period from 2011 to 2018 for our backtest‐\\ning. In this step, to further analyze the effectiveness of our strategy, we vary the time\\nperiod of the backtesting and analyze the results. First, we run the strategy for all the\\nstocks for the time period between 2012 and 2014:\\n\\nresults_tickers = {}\\nfor ticker in tickers:\\n    results_tickers[ticker] = run_strategy(ticker, start = \\'2012-01-01\\', \\\\\\n    end = \\'2014-12-31\\')\\n\\nOutput\\n\\nThe strategy yields an overall profit for all the stocks except AMZN and WMT. Now\\nwe run the strategy between 2016 and 2018:\\n\\nresults_tickers = {}\\nfor ticker in tickers:\\n    results_tickers[ticker] = run_strategy(ticker, start = \\'2016-01-01\\', \\\\\\n    end = \\'2018-12-31\\')\\n\\nOutput\\n\\n382 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cWe  see  a  good  performance  of  the  sentiment-based  strategy  across  all  the  stocks\\nexcept AAPL, and we can conclude that it performs quite well on different time peri‐\\nods. The strategy can be adjusted by modifying the trading rules or lot sizes. Addi‐\\ntional  metrics  can  also  be  used  to  understand  the  performance  of  the  strategy.  The\\nsentiments can also be used along with the other features, such as correlated variables\\nand technical indicators for prediction.\\n\\nConclusion\\n\\nIn this case study, we looked at various ways in which unstructured data can be con‐\\nverted  to  structured  data  and  then  used  for  analysis  and  prediction  using  tools  for\\nNLP.  We  have  demonstrated  three  different  approaches,  including  deep  learning\\nmodels to develop a model for computing the sentiments. We performed a compari‐\\nson of the models and concluded that one of the most important steps in training the\\nmodel for sentiment analysis is using a domain-specific vocabulary.\\n\\nWe also used a pretrained English model by spaCy to convert a sentence into senti‐\\nments  and  used  the  sentiments  as  signals  to  develop  a  trading  strategy.  The  initial\\nresults suggested that the model trained on a financial lexicon–based sentiment could\\nprove  to  be  a  viable  model  for  a  trading  strategy.  Additional  improvements  to  this\\ncan be made by using more complex pretrained sentiment analysis models, such as\\nBERT by Google, or different pretrained NLP models available in open source plat‐\\nforms. Existing NLP libraries fill in some of the preprocessing and encoding steps to\\nallow us to focus on the inference step.\\n\\nWe could build on the trading strategy by including more correlated variables, tech‐\\nnical  indicators,  or  even  improved  sentiment  analysis  by  using  more  sophisticated\\npreprocessing steps and models based on more relevant financial text data.\\n\\nCase Study 2: Chatbot Digital Assistant\\nChatbots are computer programs that maintain a conversation with a user in natural\\nlanguage.  They  can  understand  the  user’s  intent  and  send  responses  based  on  an\\norganization’s business rules and data. These chatbots use deep learning and NLP to\\nprocess language, enabling them to understand human speech.\\n\\nChatbots  are  increasingly  being  implemented  across  many  domains  for  financial\\nservices. Banking bots enable consumers to check their balance, transfer money, pay\\nbills, and more. Brokering bots enable consumers to find investment options, make\\ninvestments,  and  track  balances.  Customer  support  bots  provide  instant  responses,\\ndramatically increasing customer satisfaction. News bots deliver personalized current\\nevents  information,  while  enterprise  bots  enable  employees  to  check  leave  balance,\\nfile expenses, check their inventory balance, and approve transactions. In addition to\\n\\nCase Study 2: Chatbot Digital Assistant \\n\\n| \\n\\n383\\n\\n\\x0cautomating  the  process  of  assisting  customers  and  employees,  chatbots  can  help\\nfinancial institutions gain information about their customers. The bot phenomenon\\nhas the potential to cause broad disruption in many areas within the finance sector.\\n\\nDepending  on  the  way  bots  are  programmed,  we  can  categorize  chatbots  into  two\\nvariants:\\n\\nRule-based\\n\\nThis variety of chatbots is trained according to rules. These chatbots do not learn\\nthrough interactions and may sometimes fail to answer complex queries outside\\nof the defined rules.\\n\\nSelf-learning\\n\\nThis  variety  of  bots  relies  on  ML  and  AI  technologies  to  converse  with  users.\\nSelf-learning chatbots are further divided into retrieval-based and generative:\\n\\nRetrieval-based\\n\\nThese chatbot are trained to rank the best response from a finite set of prede‐\\nfined responses.\\n\\nGenerative\\n\\nThese  chatbots  are  not  built  with  predefined  responses.  Instead,  they  are\\ntrained using a large number of previous conversations. They require a very\\nlarge amount of conversational data to train.\\n\\nIn this case study, we will prototype a self-learning chatbot that can answer financial\\nquestions.\\n\\nThis case study focuses on:\\n\\n• Building a customized logic and rules parser using NLP for a chatbot.\\n\\n• Understanding the data preparation required for building a chatbot.\\n\\n• Understanding the basic building blocks of a chatbot.\\n\\n• Leveraging  available  Python  packages  and  corpuses  to  train  a  chatbot  in  a  few\\n\\nlines of code.\\n\\n384 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cBlueprint for Creating a Custom Chatbot Using NLP\\n\\n1. Problem definition\\n\\nThe goal of this case study is to build a basic prototype of a conversational chatbot\\npowered  by  NLP.  The  primary  purpose  of  this  chatbot  is  to  help  a  user  retrieve  a\\nfinancial  ratio  about  a  particular  company.  Such  chatbots  are  designed  to  quickly\\nretrieve  the  details  about  a  stock  or  an  instrument  that  may  help  the  user  make  a\\ntrading decision.\\n\\nIn addition to retrieving a financial ratio, the chatbot could also engage in casual con‐\\nversations with a user, perform basic mathematical calculations, and provide answers\\nto questions from a list used to train it. We intend to use Python packages and func‐\\ntions for chatbot creation and to customize several components of the chatbot archi‐\\ntecture to adapt to our requirements.\\n\\nThe  chatbot  prototype  created  in  this  case  study  is  designed  to  understand  user\\ninputs and intention and retrieve the information they are seeking. It is a small proto‐\\ntype that could be enhanced for use as an information retrieval bot in banking, bro‐\\nkering, or customer support.\\n\\n2. Getting started—loading the libraries\\n\\nFor this case study, we will use two text-based libraries: spaCy and ChatterBot. spaCy\\nhas been previously introduced; ChatterBot is a Python library used to create simple\\nchatbots with minimal programming required.\\n\\nAn untrained instance of ChatterBot starts off with no knowledge of how to commu‐\\nnicate. Each time a user enters a statement, the library saves the input and response\\ntext.  As  ChatterBot  receives  more  inputs,  the  number  of  responses  it  can  offer  and\\nthe accuracy of those responses increase. The program selects the response by search‐\\ning for the closest matching known statement to the input. It then returns the most\\nlikely response to that statement based on how frequently each response is issued by\\nthe people the bot communicates with.\\n\\n2.1. Load libraries.    We import spaCy using the following Python code:\\n\\nimport spacy #Custom NER model.\\nfrom spacy.util import minibatch, compounding\\n\\nCase Study 2: Chatbot Digital Assistant \\n\\n| \\n\\n385\\n\\n\\x0cThe  ChatterBot  library  has  the  modules  LogicAdapter,  ChatterBotCorpusTrainer,\\nand ListTrainer. These modules are used by our bot in order to construct responses\\nto user queries. We begin by importing the following:\\n\\nfrom chatterbot import ChatBot\\nfrom chatterbot.logic import LogicAdapter\\nfrom chatterbot.trainers import ChatterBotCorpusTrainer\\nfrom chatterbot.trainers import ListTrainer\\n\\nOther libraries used in this exercise are as follows:\\n\\nimport random\\nfrom itertools import product\\n\\nBefore we move to the customized chatbot, let us develop a chatbot using the default\\nfeatures of the ChatterBot package.\\n\\n3. Training a default chatbot\\n\\nChatterBot  and  many  other  chatbot  packages  come  with  a  data  utility  module  that\\ncan be used to train chatbots. Here are the ChatterBot components we will be using:\\n\\nLogic adapters\\n\\nLogic  adapters  determine  the  logic  for  how  ChatterBot  selects  a  response  to  a\\ngiven  input  statement.  It  is  possible  to  enter  any  number  of  logic  adapters  for\\nyour  bot  to  use.  In  the  example  below,  we  are  using  two  inbuilt  adapters:  Best‐\\nMatch,  which  returns  the  best  known  responses,  and  MathematicalEvaluation,\\nwhich performs mathematical operations.\\n\\nPreprocessors\\n\\nChatterBot’s preprocessors are simple functions that modify the input statement\\na chatbot receives before the statement gets processed by the logic adapter. The\\npreprocessors can be customized to perform different preprocessing steps, such\\nas tokenization and lemmatization, in order to have clean and processed data. In\\nthe  example  below,  the  default  preprocessor  for  cleaning  white  spaces,\\nclean_whitespace, is used.\\n\\nCorpus training\\n\\nChatterBot  comes  with  a  corpus  data  and  utility  module  that  makes  it  easy  to\\nquickly  train  the  bot  to  communicate.  We  use  the  already  existing  corpuses\\nenglish, english.greetings, and english.conversations for training the chatbot.\\n\\nList training\\n\\nJust like the corpus training, we train the chatbot with the conversations that can\\nbe used for training using ListTrainer. In the example below, we have trained the\\nchatbot using some sample commands. The chatbot can be trained using a sig‐\\nnificant amount of conversation data.\\n\\n386 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cchatB = ChatBot(\"Trader\",\\n                preprocessors=[\\'chatterbot.preprocessors.clean_whitespace\\'],\\n                logic_adapters=[\\'chatterbot.logic.BestMatch\\',\\n                                \\'chatterbot.logic.MathematicalEvaluation\\'])\\n\\n# Corpus Training\\ntrainerCorpus = ChatterBotCorpusTrainer(chatB)\\n\\n# Train based on English Corpus\\ntrainerCorpus.train(\\n    \"chatterbot.corpus.english\"\\n)\\n# Train based on english greetings corpus\\ntrainerCorpus.train(\"chatterbot.corpus.english.greetings\")\\n\\n# Train based on the english conversations corpus\\ntrainerCorpus.train(\"chatterbot.corpus.english.conversations\")\\n\\ntrainerConversation = ListTrainer(chatB)\\n# Train based on conversations\\n\\n# List training\\ntrainerConversation.train([\\n    \\'Help!\\',\\n    \\'Please go to google.com\\',\\n    \\'What is Bitcoin?\\',\\n    \\'It is a decentralized digital currency\\'\\n])\\n\\n# You can train with a second list of data to add response variations\\ntrainerConversation.train([\\n    \\'What is Bitcoin?\\',\\n    \\'Bitcoin is a cryptocurrency.\\'\\n])\\n\\nOnce the chatbot is trained, we can test the trained chatbot by having the following\\nconversation:\\n\\n>Hi\\nHow are you doing?\\n\\n>I am doing well.\\nThat is good to hear\\n\\n>What is 78964 plus 5970\\n78964 plus 5970 = 84934\\n\\n>what is a dollar\\ndollar: unit of currency in the united states.\\n\\n>What is Bitcoin?\\nIt is a decentralized digital currency\\n\\nCase Study 2: Chatbot Digital Assistant \\n\\n| \\n\\n387\\n\\n\\x0c>Help!\\nPlease go to google.com\\n\\n>Tell me a joke\\nDid you hear the one about the mountain goats in the andes? It was \"ba a a a d\".\\n\\n>What is Bitcoin?\\nBitcoin is a cryptocurrency.\\n\\nIn this example, we see a chatbot that gives an intuitive reply in response to the input.\\nThe first two responses are due to the training on the English greetings and English\\nconversation  corpuses.  Additionally,  the  responses  to  Tell  me  a  joke  and  what  is  a\\ndollar are due to the training on the English corpus. The computation in the fourth\\nline is the result of the chatbot being trained on the MathematicalEvaluation logical\\nadapter. The responses to Help! and What is Bitcoin? are the result of the customized\\nlist trainers. Additionally, we see two different replies to What is Bitcoin?, given that\\nwe trained it using the list trainers.\\n\\nNext, we move on to creating a chatbot designed to use a customized logical adapter\\nto give financial ratios.\\n\\n4. Data preparation: Customized chatbot\\n\\nWe want our chatbot to be able to recognize and group subtly different inquiries. For\\nexample, one might want to ask about the company Apple Inc. by simply referring to\\nit as Apple, and we would want to map it to a ticker—AAPL, in this case. Construct‐\\ning commonly used phrases in order to refer to firms can be built by using a dictio‐\\nnary as follows:\\n\\ncompanies = {\\n    \\'AAPL\\':  [\\'Apple\\', \\'Apple Inc\\'],\\n    \\'BAC\\': [\\'BAML\\', \\'BofA\\', \\'Bank of America\\'],\\n    \\'C\\': [\\'Citi\\', \\'Citibank\\'],\\n    \\'DAL\\': [\\'Delta\\', \\'Delta Airlines\\']\\n}\\n\\nSimilarly, we want to build a map for financial ratios:\\n\\nratios = {\\n    \\'return-on-equity-ttm\\': [\\'ROE\\', \\'Return on Equity\\'],\\n    \\'cash-from-operations-quarterly\\': [\\'CFO\\', \\'Cash Flow from Operations\\'],\\n    \\'pe-ratio-ttm\\': [\\'PE\\', \\'Price to equity\\', \\'pe ratio\\'],\\n    \\'revenue-ttm\\': [\\'Sales\\', \\'Revenue\\'],\\n}\\n\\nThe keys of this dictionary can be used to map to an internal system or API. Finally,\\nwe want the user to be able to request the phrase in multiple formats. Saying Get me\\nthe [RATIO] for [COMPANY] should be treated similarly to What is the [RATIO] for\\n[COMPANY]? We build these sentence templates for our model to train on by build‐\\ning a list as follows:\\n\\n388 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cstring_templates = [\\'Get me the {ratio} for {company}\\',\\n                   \\'What is the {ratio} for {company}?\\',\\n                   \\'Tell me the {ratio} for {company}\\',\\n                  ]\\n\\n4.1. Data construction.     We  begin  constructing  our  model  by  creating  reverse\\ndictionaries:\\n\\ncompanies_rev = {}\\nfor k, v in companies.items():\\n  for ve in v:\\n      companies_rev[ve] = k\\n  ratios_rev = {}\\n  for k, v in ratios.items():\\n        for ve in v:\\n             ratios_rev[ve] = k\\n  companies_list = list(companies_rev.keys())\\n  ratios_list = list(ratios_rev.keys())\\n\\nNext, we create sample statements for our model. We build a function that gives us a\\nrandom  sentence  structure,  inquiring  about  a  random  financial  ratio  for  a  random\\ncompany.  We  will  be  creating  a  custom  named  entity  recognition_  model  in  the\\nspaCy framework. This requires training the model to pick up the word or phrase in\\na sample sentence. To train the spaCy model, we need to provide it with an example,\\nsuch  as  (Get  me  the  ROE  for  Citi,  {\"entities”:  [(11,  14,  RATIO),  (19,  23,  COM‐\\nPANY) ]}).\\n\\n4.2. Training data.    The first part of the training example is the sentence. The second is\\na dictionary that consists of entities and the starting and ending index of the label:\\n\\nN_training_samples = 100\\ndef get_training_sample(string_templates, ratios_list, companies_list):\\n  string_template=string_templates[random.randint(0, len(string_templates)-1)]\\n      ratio = ratios_list[random.randint(0, len(ratios_list)-1)]\\n      company = companies_list[random.randint(0, len(companies_list)-1)]\\n      sent = string_template.format(ratio=ratio,company=company)\\n      ents = {\"entities\": [(sent.index(ratio), sent.index(ratio)+\\\\\\n  len(ratio), \\'RATIO\\'),\\n                    (sent.index(company), sent.index(company)+len(company), \\\\\\n                    \\'COMPANY\\')]}\\n       return (sent, ents)\\n\\nLet us define the training data:\\n\\nTRAIN_DATA = [\\nget_training_sample(string_templates, ratios_list, companies_list) \\\\\\nfor i in range(N_training_samples)\\n]\\n\\nCase Study 2: Chatbot Digital Assistant \\n\\n| \\n\\n389\\n\\n\\x0c5. Model creation and training.     Once  we  have  the  training  data,  we  construct  a  blank\\nmodel  in  spaCy.  spaCy’s  models  are  statistical,  and  every  decision  they  make—for\\nexample, which part-of-speech tag to assign, or whether a word is a named entity—is\\na  prediction.  This  prediction  is  based  on  the  examples  the  model  has  seen  during\\ntraining.  To  train  a  model,  you  first  need  training  data—examples  of  text  and  the\\nlabels  you  want  the  model  to  predict.  This  could  be  a  part-of-speech  tag,  a  named\\nentity,  or  any  other  information.  The  model  is  then  shown  the  unlabeled  text  and\\nmakes a prediction. Because we know the correct answer, we can give the model feed‐\\nback on its prediction in the form of an error gradient of the loss function. This calcu‐\\nlates the difference between the training example and the expected output, as shown\\nin Figure 10-6. The greater the difference, the more significant the gradient, and the\\nmore updates we need to make to our model.\\n\\nFigure 10-6. Machine learning–based training in spaCy\\n\\nnlp = spacy.blank(\"en\")\\n\\nNext, we create an NER pipeline to our model:\\n\\nner = nlp.create_pipe(\"ner\")\\nnlp.add_pipe(ner)\\n\\nThen we add the training labels that we use:\\n\\nner.add_label(\\'RATIO\\')\\nner.add_label(\\'COMPANY\\')\\n\\n5.1. Model optimization function\\n\\nNow we start optimization of our models:\\n\\noptimizer = nlp.begin_training()\\nmove_names = list(ner.move_names)\\npipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\\nwith nlp.disable_pipes(*other_pipes):  # only train NER\\n     sizes = compounding(1.0, 4.0, 1.001)\\n     # batch up the examples using spaCy\\'s minibatch\\n     for itn in range(30):\\n        random.shuffle(TRAIN_DATA)\\n        batches = minibatch(TRAIN_DATA, size=sizes)\\n        losses = {}\\n\\n390 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0c        for batch in batches:\\n           texts, annotations = zip(*batch)\\n           nlp.update(texts, annotations, sgd=optimizer,\\n           drop=0.35, losses=losses)\\n        print(\"Losses\", losses)\\n\\nTraining  the  NER  model  is  akin  to  updating  the  weights  for  each  token.  The  most\\nimportant  step  is  to  use  a  good  optimizer.  The  more  examples  of  our  training  data\\nthat we provide spaCy, the better it will be at recognizing generalized results.\\n\\n5.2. Custom logic adapter\\n\\nNext, we build our custom logic adapter:\\n\\nfrom chatterbot.conversation import Statement\\nclass FinancialRatioAdapter(LogicAdapter):\\n     def __init__(self, chatbot, **kwargs):\\n          super(FinancialRatioAdapter, self).__init__(chatbot, **kwargs)\\n     def process(self, statement, additional_response_selection_parameters):\\n        user_input = statement.text\\n        doc = nlp(user_input)\\n        company = None\\n        ratio = None\\n        confidence = 0\\n        # We need exactly 1 company and one ratio\\n        if len(doc.ents) == 2:\\n         for ent in doc.ents:\\n             if ent.label_ == \"RATIO\":\\n                  ratio = ent.text\\n                 if ratio in ratios_rev:\\n                      confidence += 0.5\\n             if ent.label_ == \"COMPANY\":\\n                  company = ent.text\\n                  if company in companies_rev:\\n                       confidence += 0.5\\n        if confidence > 0.99: (its found a ratio and company)\\n         outtext = \\'\\'\\'https://www.zacks.com/stock/chart\\\\\\n                /{comanpy}/fundamental/{ratio} \\'\\'\\'.format(ratio=ratios_rev[ratio]\\\\\\n                  , company=companies_rev[company])\\n         confidence = 1\\n        else:\\n         outtext = \\'Sorry! Could not figure out what the user wants\\'\\n         confidence = 0\\n        output_statement = Statement(text=outtext)\\n        output_statement.confidence = confidence\\n        return output_statement\\n\\nWith this custom logic adapter, our chatbot will take each input statement and try to\\nrecognize  a  RATIO  and/or  COMPANY  using  our  NER  model.  If  the  model  finds\\nexactly  one  COMPANY  and  exactly  one  RATIO,  it  constructs  a  URL  to  guide  the\\nuser.\\n\\nCase Study 2: Chatbot Digital Assistant \\n\\n| \\n\\n391\\n\\n\\x0c5.3. Model usage—training and testing\\n\\nNow we begin using our chatbot by using the following import:\\n\\nfrom chatterbot import ChatBot\\n\\nWe construct our chatbot by adding the FinancialRatioAdapter logical adapter that\\nwe created above to the chatbot. Although the following code snippet only shows us\\nadding  the  FinancialRatioAdapter,  note  that  other  logical  adapters,  lists,  and  cor‐\\npuses used in the prior training of the chatbot are also included. Please refer to the\\nJupyter notebook of the case study for more details.\\n\\nchatbot = ChatBot(\\n       \"My ChatterBot\",\\n       logic_adapters=[\\n        \\'financial_ratio_adapter.FinancialRatioAdapter\\'\\n    ]\\n)\\n\\nNow we test our chatbot using the following statements:\\n\\nconverse()\\n\\n>What is ROE for Citibank?\\nhttps://www.zacks.com/stock/chart/C/fundamental/return-on-equity-ttm\\n\\n>Tell me PE for Delta?\\nhttps://www.zacks.com/stock/chart/DAL/fundamental/pe-ratio-ttm\\n\\n>What is Bitcoin?\\nIt is a decentralized digital currency\\n\\n>Help!\\nPlease go to google.com\\n\\n>What is 786940 plus 75869\\n786940 plus 75869 = 862809\\n\\n>Do you like dogs?\\nSorry! Could not figure out what the user wants\\n\\nAs  shown  above,  the  custom  logic  adapter  for  our  chatbot  finds  a  RATIO  and/or\\nCOMPANY  in  the  sentence  using  our  NLP  model.  If  an  exact  pair  is  detected,  the\\nmodel constructs a URL to guide the user to the answer. Additionally, other logical\\nadapters, such as mathematical evaluation, work as expected.\\n\\nConclusion\\n\\nOverall,  this  case  study  provides  an  introduction  to  a  number  of  aspects  of  chatbot\\ndevelopment.\\n\\n392 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cUsing the ChatterBot library in Python allows us to build a simple interface to resolve\\nuser inputs. To train a blank model, one must have a substantial training dataset. In\\nthis  case  study,  we  looked  at  patterns  available  to  us  and  used  them  to  generate\\ntraining samples. Getting the right amount of training data is usually the hardest part\\nof constructing a custom chatbot.\\n\\nThis case study is a demo project, and significant enhancements can be made to each\\ncomponent to extend it to a wide variety of tasks. Additional preprocessing steps can\\nbe added to have cleaner data to work with. To generate a response from our bot for\\ninput questions, the logic can be refined further to incorporate better similarity meas‐\\nures  and  embeddings.  The  chatbot  can  be  trained  on  a  bigger  dataset  using  more\\nadvanced ML techniques. A series of custom logic adapters can be used to construct a\\nmore  sophisticated  ChatterBot.  This  can  be  generalized  to  more  interesting  tasks,\\nsuch  as  retrieving  information  from  a  database  or  asking  for  more  input  from  the\\nuser.\\n\\nCase Study 3: Document Summarization\\nDocument  summarization  refers  to  the  selection  of  the  most  important  points  and\\ntopics in a document and arranging them in a comprehensive manner. As discussed\\nearlier, analysts at banks and other financial service organizations pore over, analyze,\\nand attempt to quantify qualitative data from news, reports, and documents. Docu‐\\nment summarization using NLP can provide in-depth support in this analyzing and\\ninterpretation.  When  tailored  to  financial  documents,  such  as  earning  reports  and\\nfinancial news, it can help analysts quickly derive key topics and market signals from\\ncontent. Document summarization can also be used to improve reporting efforts and\\ncan provide timely updates on key matters.\\n\\nIn NLP, topic models (such as LDA, introduced earlier in the chapter) are the most\\nfrequently  used  tools  for  the  extraction  of  sophisticated,  interpretable  text  features.\\nThese  models  can  surface  key  topics,  themes,  or  signals  from  large  collections  of\\ndocuments and can be effectively used for document summarization.\\n\\nIn this case study, we will focus on:\\n\\n• Implementing the LDA model for topic modeling.\\n\\n• Understanding  the  necessary  data  preparation  (i.e.,  converting  a  PDF  for  an\\n\\nNLP-related problem).\\n\\n• Topic visualization.\\n\\nCase Study 3: Document Summarization \\n\\n| \\n\\n393\\n\\n\\x0cBlueprint for Using NLP for Document Summarization\\n\\n1. Problem definition\\n\\nThe goal of this case study is to effectively discover common topics from earnings call\\ntranscripts of publicly traded companies using LDA. A core advantage of this techni‐\\nque  compared  to  other  approaches,  is  that  no  prior  knowledge  of  the  topics  is\\nneeded.\\n\\n2. Getting started—loading the data and Python packages\\n\\n2.1. Loading the Python packages.     For  this  case  study,  we  will  extract  the  text  from  a\\nPDF. Hence, the Python library pdf-miner is used for processing the PDF files into a\\ntext format. Libraries for feature extraction and topic modeling are also loaded. The\\nlibraries for the visualization will be loaded later in the case study:\\n\\nLibraries for pdf conversion\\n\\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\\nfrom pdfminer.converter import TextConverter\\nfrom pdfminer.layout import LAParams\\nfrom pdfminer.pdfpage import PDFPage\\nimport re\\nfrom io import StringIO\\n\\nLibraries for feature extraction and topic modeling\\n\\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\\nfrom sklearn.decomposition import LatentDirichletAllocation\\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\\n\\nOther libraries\\n\\nimport numpy as np\\nimport pandas as pd\\n\\n3. Data preparation\\n\\nThe convert_pdf_to_txt function defined below pulls out all characters from a PDF\\ndocument  except  the  images.  The  function  simply  takes  in  the  PDF  document,\\nextracts all characters from the document, and outputs the extracted text as a Python\\nlist of strings:\\n\\ndef convert_pdf_to_txt(path):\\n    rsrcmgr = PDFResourceManager()\\n\\n394 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0c    retstr = StringIO()\\n    laparams = LAParams()\\n    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\\n    fp = open(path, \\'rb\\')\\n    interpreter = PDFPageInterpreter(rsrcmgr, device)\\n    password = \"\"\\n    maxpages = 0\\n    caching = True\\n    pagenos=set()\\n\\n    for page in PDFPage.get_pages(fp, pagenos,\\\\\\n            maxpages=maxpages, password=password,caching=caching,\\\\\\n            check_extractable=True):\\n        interpreter.process_page(page)\\n\\n    text = retstr.getvalue()\\n\\n    fp.close()\\n    device.close()\\n    retstr.close()\\n    return text\\n\\nIn the next step, the PDF is converted to text using the above function and saved in a\\ntext file:\\n\\nDocument=convert_pdf_to_txt(\\'10K.pdf\\')\\nf=open(\\'Finance10k.txt\\',\\'w\\')\\nf.write(Document)\\nf.close()\\nwith open(\\'Finance10k.txt\\') as f:\\n    clean_cont = f.read().splitlines()\\n\\nLet us look at the raw document:\\n\\nclean_cont[1:15]\\n\\nOutput\\n\\n[\\' \\',\\n \\'\\',\\n \\'SECURITIES AND EXCHANGE COMMISSION\\',\\n \\' \\',\\n \\'\\',\\n \\'Washington, D.C. 20549\\',\\n \\' \\',\\n \\'\\',\\n \\'\\\\xa0\\',\\n \\'FORM \\',\\n \\'\\\\xa0\\',\\n \\'\\',\\n \\'QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF\\',\\n \\' \\']\\n\\nCase Study 3: Document Summarization \\n\\n| \\n\\n395\\n\\n\\x0cThe  text  extracted  from  the  PDF  document  contains  uninformative  characters  that\\nneed to be removed. These characters reduce the effectiveness of our models as they\\nprovide  unnecessary  count  ratios.  The  following  function  uses  a  series  of  regular\\nexpression  (regex)  searches  as  well  as  list  comprehension  to  replace  uninformative\\ncharacters with a blank space:\\n\\ndoc=[i.replace(\\'\\\\xe2\\\\x80\\\\x9c\\', \\'\\') for i in clean_cont ]\\ndoc=[i.replace(\\'\\\\xe2\\\\x80\\\\x9d\\', \\'\\') for i in doc ]\\ndoc=[i.replace(\\'\\\\xe2\\\\x80\\\\x99s\\', \\'\\') for i in doc ]\\n\\ndocs = [x for x in doc if x != \\' \\']\\ndocss = [x for x in docs if x != \\'\\']\\nfinancedoc=[re.sub(\"[^a-zA-Z]+\", \" \", s) for s in docss]\\n\\n4. Model construction and training\\n\\nThe  CountVectorizer  function  from  the  sklearn  module  is  used  with  minimal\\nparameter tuning to represent the clean document as a document term matrix. This is\\nperformed because our modeling requires that strings be represented as integers. The\\nCountVectorizer  shows  the  number  of  times  a  word  occurs  in  the  list  after  the\\nremoval of stop words. The document term matrix was formatted into a Pandas data‐\\nframe  in  order  to  inspect  the  dataset.  This  dataframe  shows  the  word-occurrence\\ncount of each term in the document:\\n\\nvect=CountVectorizer(ngram_range=(1, 1),stop_words=\\'english\\')\\nfin=vect.fit_transform(financedoc)\\n\\nIn the next step, the document term matrix will be used as the input data to the LDA\\nalgorithm  for  topic  modeling.  The  algorithm  was  fitted  to  isolate  five  distinct  topic\\ncontexts, as shown by the following code. This value can be adjusted depending on\\nthe level of granularity one intends to obtain from the modeling:\\n\\nlda=LatentDirichletAllocation(n_components=5)\\nlda.fit_transform(fin)\\nlda_dtf=lda.fit_transform(fin)\\n\\nsorting=np.argsort(lda.components_)[:, ::-1]\\nfeatures=np.array(vect.get_feature_names())\\n\\nThe following code uses the mglearn library to display the top 10 words within each\\nspecific topic model:\\n\\nimport mglearn\\nmglearn.tools.print_topics(topics=range(5), feature_names=features,\\nsorting=sorting, topics_per_chunk=5, n_words=10)\\n\\nOutput\\n\\ntopic 1       topic 2       topic 3       topic 4       topic 5\\n--------      --------      --------      --------      --------\\nassets        quarter       loans         securities    value\\n\\n396 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cbalance       million       mortgage      rate          total\\nlosses        risk          loan          investment    income\\ncredit        capital       commercial    contracts     net\\nperiod        months        total         credit        fair\\nderivatives   financial     real          market        billion\\nliabilities   management    estate        federal       equity\\nderivative    billion       securities    stock         september\\nallowance     ended         consumer      debt          december\\naverage       september     backed        sales         table\\n\\nEach topic in the table is expected to represent a broader theme. However, given that\\nwe  trained  the  model  on  only  a  single  document,  the  themes  across  the  topics  may\\nnot be very distinct from each other.\\n\\nLooking at the broader theme, topic 2 discusses quarters, months, and currency units\\nrelated  to  asset  valuation.  Topic  3  reveals  information  on  income  from  real  estate,\\nmortgages, and related instrument. Topic 5 also has terms related to asset valuation.\\nThe first topic references balance sheet items and derivatives. Topic 4 is slightly simi‐\\nlar to topic 1 and has words related to an investment process.\\n\\nIn  terms  of  overall  theme,  topics  2  and  5  are  quite  distinct  from  the  others.  There\\nmay also be some similarity between topics 1 and 4, based on the top words. In the\\nnext section, we will try to understand the separation between these topics using the\\nPython library pyLDAvis.\\n\\n5. Visualization of topics\\n\\nIn this section, we visualize the topics using different techniques.\\n\\n5.1. Topic visualization.     Topic  visualization  facilitates  the  evaluation  of  topic  quality\\nusing  human  judgment.  pyLDAvis  is  a  library  that  displays  the  global  relationships\\nbetween  topics  while  also  facilitating  their  semantic  evaluation  by  inspecting  the\\nterms  most  closely  associated  with  each  topic  and,  inversely,  the  topics  associated\\nwith  each  term.  It  also  addresses  the  challenge  in  which  frequently  used  terms  in  a\\ndocument tend to dominate the distribution over words that define a topic.\\n\\nBelow, the pyLDAvis_ library is used to visualize the topic models:\\n\\nfrom __future__ import  print_function\\nimport pyLDAvis\\nimport pyLDAvis.sklearn\\n\\nzit=pyLDAvis.sklearn.prepare(lda,fin,vect)\\npyLDAvis.show(zit)\\n\\nCase Study 3: Document Summarization \\n\\n| \\n\\n397\\n\\n\\x0cOutput\\n\\nWe  notice  that  topics  2  and  5  are  quite  distant  from  each  other.  This  is  what  we\\nobserved in the section above from the overall theme and list of words under these\\ntopics.  Topics  1  and  4  are  quite  close,  which  validates  our  observation  above.  Such\\nclose  topics  should  be  analyzed  more  intricately  and  might  be  combined  if  needed.\\nThe relevance of the terms under each topic, as shown in the right panel of the chart,\\ncan also be used to understand the differences. Topics 3 and 4 are relatively close as\\nwell, although topic 3 is quite distant from the others.\\n\\n5.2. Word cloud.     In  this  step,  a  word  cloud  is  generated  for  the  entire  document  to\\nnote the most recurrent terms in the document:\\n\\n#Loading the additional packages for word cloud\\nfrom os import path\\nfrom PIL import Image\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom wordcloud import WordCloud,STOPWORDS\\n\\n#Loading the document and generating the word cloud\\nd = path.dirname(__name__)\\ntext = open(path.join(d, \\'Finance10k.txt\\')).read()\\n\\nstopwords = set(STOPWORDS)\\nwc = WordCloud(background_color=\"black\", max_words=2000, stopwords=stopwords)\\nwc.generate(text)\\n\\n398 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cplt.figure(figsize=(16,13))\\nplt.imshow(wc, interpolation=\\'bilinear\\')\\nplt.axis(\"off\")\\nplt.show()\\n\\nOutput\\n\\nThe word cloud generally agrees with the results from the topic modeling, as recur‐\\nrent  words,  such  as  loan,  real  estate,  third  quarter,  and  fair  value,  are  larger  and\\nbolder.\\n\\nBy integrating the information from the steps above, we may come up with the list of\\ntopics represented by the document. For the document in our case study, we see that\\nwords  like  third  quarter,  first  nine,  and  nine  months  occur  quite  frequently.  In  the\\nword  list,  there  are  several  topics  related  to  balance  sheet  items.  So  the  document\\nmight  be  a  third-quarter  financial  balance  sheet  with  all  credit  and  assets  values  in\\nthat quarter.\\n\\nConclusion\\n\\nIn  this  case  study,  we  explored  the  use  of  topic  modeling  to  gain  insights  into  the\\ncontent of a document. We demonstrated the use of the LDA model, which extracts\\nplausible topics and allows us to gain a high-level understanding of large amounts of\\ntext in an automated way.\\n\\nWe performed extraction of the text from a document in PDF format and performed\\nfurther  data  preprocessing.  The  results,  alongside  the  visualizations,  demonstrated\\nthat the topics are intuitive and meaningful.\\n\\nCase Study 3: Document Summarization \\n\\n| \\n\\n399\\n\\n\\x0cOverall, the case study shows how machine learning and NLP can be applied across\\nmany domains—such as investment analysis, asset modeling, risk management, and\\nregulatory compliance—to summarize documents, news, and reports in order to sig‐\\nnificantly  reduce  manual  processing.  Given  this  ability  to  quickly  access  and  verify\\nrelevant, filtered information, analysts may be able to provide more comprehensive\\nand informative reports on which management can base their decisions.\\n\\nChapter Summary\\nThe  field  of  NLP  has  made  significant  progress,  resulting  in  technologies  that  have\\nand  will  continue  to  revolutionize  how  financial  institutions  operate.  In  the  near\\nterm,  we  are  likely  to  see  an  increase  in  NLP-based  technologies  across  different\\ndomains  of  finance,  including  asset  management,  risk  management,  and  process\\nautomation.  The  adoption  and  understanding  of  NLP  methodologies  and  related\\ninfrastructure are very important for financial institutions.\\n\\nOverall,  the  concepts  in  Python,  machine  learning,  and  finance  presented  in  this\\nchapter through the case studies can be used as a blueprint for any other NLP-based\\nproblem in finance.\\n\\nExercises\\n\\n• Using  the  concepts  from  case  study  1,  use  NLP-based  techniques  to  develop  a\\n\\ntrading strategy using Twitter data.\\n\\n• In case study 1, use the word2vec word embedding method to generate the word\\n\\nvectors and incorporate it into the trading strategy.\\n\\n• Using  the  concepts  from  case  study  2,  test  a  few  more  logical  adapters  to  the\\n\\nchatbot.\\n\\n• Using the concepts from case study 3, perform topic modeling on a set of finan‐\\n\\ncial news articles for a given day and retrieve the key themes of the day.\\n\\n400 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cIndex\\n\\nA\\naccounting fraud, early use of NLP to detect,\\n\\n347\\n\\naccuracy (evaluation metric), 77\\naction\\n\\nin reinforcement learning, 284\\nin trading, 287\\n\\naction-value function (Q-value), 286\\nAdaGrad (adaptive gradient algorithm), 39\\nAdam (adaptive moment estimation), 39\\nadaptive boosting (AdaBoost), 68\\n\\nadvantages and disadvantages, 69\\nhyperparameters, 69\\nimplementation in Python, 69\\n\\nadaptive gradient algorithm (AdaGrad), 39\\nadjusted R² metric, 76\\naffinity propagation clustering, 242\\n\\ngrouping investors, 264\\npairs trading, 254\\n\\nagent (RL system)\\n\\ndefinition, 9, 284\\ntrading and, 287\\nagent class, 305-307\\nagglomerative hierarchical clustering, 241\\n\\n(see also hierarchical clustering)\\nAI (artificial intelligence), defined, 6\\nalgorithmic trading, 2\\nalgorithms, comparing, 24\\nAmazon Web Services (AWS), 44\\nAnaconda, 15\\nANNs (see artificial neural networks)\\narea under ROC curve (AUC), 78\\nARIMA (autoregressive integrated moving\\n\\naverage) model, 91, 107, 110\\n\\nartificial intelligence (AI), defined, 6\\nartificial neural networks (ANNs), 31-45\\nalternatives to running on CPU, 43\\nANN-based supervised learning models, 71\\narchitecture, 32-34\\ncreating in Python, 40-44\\nderivative pricing with, 122\\nderivatives hedging with, 321-325\\nhyperparameters, 36-40\\nlayers, 33\\nneurons, 32\\nreinforcement learning and, 292\\nrunning on cloud services, 44\\nrunning on GPU, 44\\ntraining, 34-36\\n\\nasset price prediction, 4, 49\\nAUC (area under ROC curve), 78\\nautocorrelation, 88\\nautomated trading systems (see algorithmic\\n\\ntrading)\\n\\nautomation, finance and, 3\\nautoregressive integrated moving average\\n\\n(ARIMA) model (see ARIMA)\\n\\nB\\nbackpropagation, 35-36\\nbacktesting\\n\\ndefined, 298\\neigen portfolio, 214-216\\nhierarchical risk parity, 275-276\\n\\nbacktrader, 378\\nbag of words model, 356\\nbagging (bootstrap aggregation), 65\\nbatch size, 40\\n\\nIndex \\n\\n| \\n\\n401\\n\\n\\x0cBellman equations, 288\\nBellman optimality equation, 288\\nbias, defined, 73\\nbias-variance trade-off, 73\\nbinary (decision) tree, 63\\nbitcoin trading: enhancing speed and accuracy,\\n\\n227-236\\ndata preparation, 228-230\\nevaluation of algorithms and models,\\n\\n230-235\\n\\nloading data and Python packages, 228\\n\\nbitcoin trading: strategy, 179-190\\ndata preparation, 181-185\\ndata visualization, 184\\nevaluation of algorithms and models,\\n\\n185-187\\n\\nexploratory data analysis, 181\\nfeature engineering, 182-184\\nfinalizing the model, 187-190\\nloading data and Python packages, 180\\nmodel tuning and grid search, 187\\n\\nBlack–Scholes formula, 114\\nBlack–Scholes model\\n\\nhyperparameters, 65\\nimplementation in Python, 65\\nlearning a CART model, 63\\npruning the tress, 64\\nrepresentation, 63\\nstopping criterion, 64\\n\\nclustering, 9, 237-277\\n\\naffinity propagation, 242\\ndefined, 9\\ndimensionality reduction versus, 237\\nhierarchical, 240-242\\nhierarchical risk parity (see hierarchical risk\\n\\nparity)\\n\\ninvestors (see clustering investors)\\nk-means, 239\\npairs trading and (see pairs trading)\\ntechniques, 239-243\\nclustering investors, 259-267\\naffinity propagation, 264\\ncluster intuition, 265\\ndata preparation, 262\\nevaluation of algorithms and models,\\n\\n263-265\\n\\nfor call option price, 115\\nreinforcement learning-based derivatives\\n\\nhedging compared to, 325-332\\n\\nexploratory data analysis, 261\\nk-means clustering, 263-264\\nloading data and Python packages, 261\\n\\nbond market, yield curve and, 141\\nboosting, 65\\n\\nC\\nCART (see classification and regression trees)\\ncharge-off, 167\\nchatbots, 4, 383-393\\n\\ndata preparation for customized chatbot,\\n\\n388\\n\\nloading libraries, 385\\nmodel creation and training, 390-392\\nNLP case study, 383-393\\nrule-based versus self-learning, 384\\ntraining a default chatbot, 386-388\\n\\nChatterBot, 386\\nclassification\\ndefined, 8\\nevaluation metrics, 77\\nregression versus, 8, 49\\n\\nclassification and regression trees (CART),\\n\\n63-65\\nadvantages and disadvantages, 65\\nderivative pricing with, 122\\n\\n402 \\n\\n| \\n\\nIndex\\n\\ncointegration, 257-259\\ncomparison of algorithms, 24\\ncomparison of models, 24\\nConda, 15\\nconfusion matrix, 78\\ncost functions, 38, 52\\ncredit (see loan default probability)\\ncredit card underwriting, 3\\ncross validation, 74\\ncross-entropy (log loss), 39\\ncumulative discounted reward, 285\\n\\nD\\ndashboard, for robo-advisor, 138-140\\ndata cleaning, steps in, 21\\ndata preparation, steps in, 21\\ndata science, defined, 7\\ndata transformation, in model development, 23\\ndata visualization, 19\\ndata, sample code for loading, 17\\ndatasets, splitting training/testing, 24\\ndecision (binary) tree, 63\\n\\n\\x0cdecision tree classifiers (see classification and\\n\\nregression trees (CART))\\n\\ndeep learning\\n\\nANNs and, 31\\ndefined, 6\\nreinforcement learning and, 292\\nRNNs and, 92\\ntime series modeling with, 92-94\\n\\ndeep neural networks\\n\\nANN-based supervised learning models, 72\\ndefined, 34\\n\\ndeep Q-network (DQN), 296\\ndendrograms, 241\\ndeploying a model, 28\\nderivative pricing, 4, 114-125\\ndata generation, 117\\ndata preparation/analysis, 120\\ndefining functions and parameters, 116\\nevaluation of models, 120-125\\nexploratory data analysis, 118\\nloading data and Python packages, 116-118\\nremoving volatility data, 123\\ntuning/finalizing model, 122\\n\\nderivatives hedging, 316-333\\n\\nevaluation of algorithms and models,\\n\\n321-325\\n\\nexploratory data analysis, 320\\ngenerating data, 319\\nloading Python packages, 319\\npolicy gradient script, 321-324\\ntesting the data, 325-332\\ntraining the data, 325\\n\\ndescriptive statistics, in model development, 18\\ndeterministic policy, 285\\ndimensionality reduction, 195-236\\n\\nbitcoin trading (see bitcoin trading: enhanc‐\\n\\ning speed and accuracy)\\n\\nclustering versus, 237\\ndefined, 8, 195\\nkernel PCA, 201\\nprincipal component analysis (PCA),\\n\\n198-201\\n\\nt-distributed stochastic neighbor embed‐\\n\\nding, 202\\n\\ntechniques, 197-202\\nyield curve construction (see yield curve\\nconstruction/interest rate modeling)\\n\\ndirect policy search, 293\\ndiscounting factor, 285\\n\\ndivisive hierarchical clustering, 241\\ndocument summarization, 393-400\\n\\ndata preparation, 394-396\\nloading data and Python packages, 394\\nmodel construction and training, 396\\nvisualization of topics, 397-399\\n\\ndocument term matrix, 396\\nDQN (deep Q-network), 296\\ndrift, 330\\n\\nE\\nEigen decomposition, 199\\neigen portfolio, 202-217\\nbacktesting, 214-216\\ndata preparation, 205\\nevaluation of algorithms and models,\\n\\n207-216\\n\\nexploratory data analysis, 204\\nloading data and Python packages, 203\\nSharpe ratio to determine best portfolio,\\n\\n210-213\\nelastic net (EN), 56, 106\\nelbow method (k-means clustering), 247\\nensemble models (supervised learning), 65-71\\n\\nadaptive boosting, 68\\nextra trees, 68\\ngradient boosting method, 70\\nrandom forest, 66-67\\n\\nenvironment\\n\\nreinforcement learning, 284\\ntrading and, 287\\n\\nepoch, 40\\nepsilon greedy algorithm, 307\\nepsilon variable, 313\\nerror gradient, 390\\nEuclidean distance, 60\\nevaluation metrics, identifying, 24\\nevaluation of models, steps in, 23\\nevent return, 367\\nexperience replay, 296, 307\\nexploratory data analysis, in model develop‐\\n\\nment, 18-20\\n\\nextra trees (extremely randomized trees), 68\\n\\nF\\nfeature engineering, 179\\nfeature representation in NLP, 356-360\\n\\nbag of words model, 356\\nTF-IDF, 358\\n\\nIndex \\n\\n| \\n\\n403\\n\\n\\x0cword embedding, 358\\n\\nfeature selection, in model development, 22\\nfinalizing a model, steps in, 27-28\\nfinancial news, sentiment analysis of, 5\\nfixed income market, yield curve and, 141\\nforward propagation, 34\\nfraud detection, 3, 153-165\\ndata preparation, 156\\nevaluation of models, 156-159\\nexploratory data analysis, 155\\nloading data and Python packages, 154\\nmodel tuning, 159-165\\n\\nfuture reward, 285\\n\\nG\\ngamma, 313\\ngates, 93\\ngenerative chatbots, 384\\ngensim, 359\\ngeometric Brownian motion (GBM), 114\\nGini cost function, 64\\nGoogle Colaboratory, 44\\ngradient ascent, 298\\ngradient boosting method (GBM), 70\\nadvantages and disadvantages, 71\\nhyperparameters, 70\\nimplementation in Python, 70\\n\\ngradient descent, 35\\ngrid search, 25, 53\\ngym (simulation environment), 337\\n\\nH\\nhelper functions, 308\\nhidden layers, 34, 36\\nhierarchical clustering, 240-242, 251-254\\nhierarchical risk parity (HRP), 267-277\\n\\nbacktesting, 275-276\\nbuilding hierarchy graph/dendrogram, 270\\ncomparison against other asset allocation\\n\\nmethods, 273\\ndata preparation, 269\\nevaluation of algorithms and models,\\n\\n270-274\\n\\ngetting portfolio weights for all types of\\n\\nasset allocation, 274\\n\\nloading data and Python packages, 269\\nquasi-diagonalization, 271\\nrecursive bisection, 272\\nstages of HRP algorithm, 271-274\\n\\n404 \\n\\n| \\n\\nIndex\\n\\nHRP (see hierarchical risk parity)\\nhyperparameters\\n\\nactivation functions, 37\\nANNs and, 36-40\\nbatch size, 40\\ncost functions, 38\\nepoch, 40\\nlearning rate, 36\\nmodel tuning and, 25\\nnumber of hidden layers and nodes, 36\\noptimizers, 39\\n\\nI\\ninertia, 239\\ninference (natural language processing),\\n\\n360-362\\nLDA implementation, 362\\nNaive Bayes approach, 360\\n\\ninput layer, ANN, 33\\ninsurance underwriting, 3\\nintelligent investors, 128\\nintuition, model/variable, 28\\ninvestors, clustering (see clustering investors)\\n\\nK\\nk-folds cross validation, 134\\nk-means clustering, 239\\n\\nfor pairs trading, 247-251\\ngrouping investors, 263-264\\nhyperparameters, 240\\nimplementation in Python, 240\\n\\nk-nearest neighbors (KNN), 60\\n\\nadvantages and disadvantages, 61\\nhyperparameters, 61\\nKaggle, running ANNs on, 44\\nKeras, 15, 40\\nkernel, 59\\nkernel PCA (KPCA), 201\\nKNN (see k-nearest neighbors)\\n\\nL\\nL1 regularization, 55\\nL2 regularization, 56\\nlasso regression (LASSO), 55, 106\\nlatent Dirichlet allocation (LDA), 360-362\\nlatent semantic analysis (LSA), 360\\nlayers, ANN, 33\\nLDA (latent Dirichlet allocation), 360-362\\n\\n\\x0cLDA (linear discriminant analysis), 62\\nlearning rate, 36\\nlemmatization, 352\\nlexicons, defined, 373\\nlinear (identity) function, 37\\nlinear discriminant analysis (LDA), 62\\nlinear regression, 52-54\\n\\nadvantages and disadvantages, 54\\ngrid search, 53\\nimplementation in Python, 52\\nregularized regression versus, 55\\ntraining a model, 52\\n\\nlinkage method, 251\\nloan default probability, 166-179\\ndata preparation, 167-169\\nevaluation of algorithms and models, 175\\nfeature selection, 169-175\\nfinalizing the model, 177\\nloading data and Python packages, 167\\nmodel tuning and grid search, 177\\n\\nloan underwriting, 3\\nlog loss (cross-entropy), 39\\nlogistic regression, 57\\n\\nadvantages and disadvantages, 58\\nhyperparameters, 58\\n\\nlong short-term memory (LSTM), 93\\n\\nsentiment analysis, 370-372\\nstock price prediction, 108\\n\\nloss (cost) functions, 38, 52\\nLSA (latent semantic analysis), 360\\n\\nM\\nmachine learning\\ndefined, 6\\nsupervised, 7\\ntypes of, 7-10\\n\\nMAE (mean absolute error), 76\\nManhattan distance, 61\\nMarkov decision processes (MDPs), 289-292\\nMarkowitz mean-variance portfolio optimiza‐\\ntion (see mean-variance portfolio (MVP)\\noptimization)\\n\\nMatplotlib, 14\\nmatrix seriation, 271\\nMDPs (Markov decision processes), 289-292\\nmean absolute error (MAE), 76\\nmean reversion, 243\\nmean squared error (MSE), 38, 76\\nmean-variance portfolio (MVP) optimization\\n\\nhierarchical risk parity versus, 267-277\\nweaknesses of, 334\\n\\nmodel development\\n\\ndata preparation, 21\\ndata transformation, 23\\nexploratory data analysis, 18-20\\nfeature selection, 22\\nfinalizing the model, 27-28\\nidentifying evaluation metrics, 24\\nloading data and packages, 17\\nmodel tuning, 25\\nperformance on test set, 27\\nsaving and deploying, 28\\nsplitting training/testing datasets, 24\\n\\nmodel evaluation, steps in, 23\\nmodel finalization, steps in, 27-28\\nmodel intuition, 28\\nmodel tuning, 25\\nmodel-based algorithms, 293\\nmodel-free algorithms, 293\\nmomentum (MOM), 183\\nmomentum optimizers, 39\\nmomentum technical indicators, 182\\nmoney laundering, 5\\nmoneyness, 116, 329\\nmoving average, 182\\nMSE (mean squared error), 38, 76\\nmultivariate plot types, Python code for, 20\\nMVP optimization (see mean-variance portfo‐\\n\\nlio optimization)\\n\\nN\\nnaive Bayes\\n\\nNLP and, 360\\nTextBlob and, 368\\n\\nnamed entity recognition (NER), 353\\nnatural language processing (NLP), 347-400\\nchatbot digital assistant (see chatbots)\\ndefined, 10\\nfeature representation, 356-360\\ninference, 360-362\\nlemmatization, 352\\nnamed entity recognition (NER), 353\\nNLTK library, 349\\nPoS tagging, 353\\npreprocessing, 351-355\\nPython packages for, 349\\nspaCy library, 350, 354-355\\nstemming, 352\\n\\nIndex \\n\\n| \\n\\n405\\n\\n\\x0cstop words removal, 351\\nTextBlob library, 349\\ntheory and concepts, 350-362\\ntokenization, 351\\n\\nNER (named entity recognition), 353\\nneuron weights, 34\\nneurons, 32\\nNLP (see natural language processing)\\nNLTK (Natural Language Took Kit), 349\\n\\nstemming code, 352\\nstop words removal, 351\\ntokenizer, 351\\n\\nnonparametric models, 84\\nnormalization, 23\\nNumPy, 14\\n\\nO\\nOpenAI gym, 337\\noptimizers, 39\\nordinary least squares (OLS) regression, 52-54\\n\\n(see also linear regression)\\n\\noutput layer, ANN, 34\\noverfitting, 54, 73\\n\\nP\\nP (transition probability function), 286\\npairs trading, 243-259\\n\\naffinity propagation, 254\\ncluster evaluation, 255-257\\ndata preparation, 245\\ndefined, 237\\nevaluation of algorithms and models,\\n\\n247-257\\n\\nexploratory data analysis, 245\\nhierarchical clustering, 251-254\\nk-means clustering, 247-251\\nloading data and Python packages, 244\\npairs selection, 257-259\\n\\nPandas, 14\\nparameter optimization, 298\\nparametric models, 84\\npart-of-speech (PoS) tagging, 353\\npartially observable Markov decision process\\n\\n(POMDP), 288\\n\\nPCA (see principal component analysis)\\npickle module, 28\\npip, 15\\npolicy development, 298\\npolicy gradient, 297, 321-324\\n\\n406 \\n\\n| \\n\\nIndex\\n\\npolicy, defined, 285\\npolicy-based algorithms, 293\\nPOMDP (partially observable Markov decision\\n\\nprocess), 288\\n\\nportfolio allocation, 334-344\\n\\nagent and cryptocurrency environment\\n\\nscript, 336\\n\\nevaluation of algorithms and models,\\n\\n336-341\\n\\nexploratory data analysis, 336\\nloading data and Python packages, 335\\ntesting the data, 342-344\\ntraining the data, 338-341\\n\\nportfolio management\\n\\nclustering investors (see clustering invest‐\\n\\nors)\\n\\nhierarchical risk parity (see hierarchical risk\\n\\nparity)\\n\\nrobo-advisors and, 2\\nportfolio weights, 208-210\\nPoS (part-of-speech) tagging, 353\\nprecision (evaluation metric), 77\\npredicted value, 34\\nprincipal component analysis (PCA), 198-201\\n\\nasset allocation, 207-216\\nbitcoin trading speed/accuracy enhance‐\\n\\nment, 231-233\\n\\nEigen decomposition, 199\\nsingular value decomposition, 200\\nyield curve construction, 222-226\\n\\nprocessing pipeline, 354\\nPython (generally), 13-29\\nadvantages of using, 13\\ncreating an artificial neural network model\\n\\nin, 40-44\\n\\nmachine learning packages, 14\\nmodel development blueprint, 16-28\\npackage installation, 15\\nsteps for model development in Python eco‐\\n\\nsystem, 15-28\\n\\nPython, creating an ANN model in, 40-44\\n\\ncompiling the model, 42\\nevaluating the model, 43\\nfitting the model, 43\\ninstalling Keras and machine learning pack‐\\n\\nages, 40\\n\\nmodel construction, 41\\n\\n\\x0cQ\\nQ-learning, 294\\nQ-value, 286\\nqualitative data, 348\\nquasi-diagonalization, 271\\n\\nR\\nradial basis function (rbf), 201\\nrandom forest, 66-67\\n\\nadvantages and disadvantages, 67\\nhyperparameters, 67\\nimplementation in Python, 67\\nrandom under-sampling, 161-165\\nrate of change (ROC), 182\\nrbf (radial basis function), 201\\nrecall (evaluation metric), 78\\nrectified linear unit (ReLU) function, 37\\nrecurrent neural networks (RNNs), 92\\nderivatives hedging with, 321\\nLSTM, 93\\n\\nrecursive binary splitting, 63\\nrecursive bisection, 272\\nregex, 366\\nregression, 83-150\\n\\nclassification versus, 8, 49\\ndefined, 8\\nsupervised (see supervised regression)\\ntime series models versus, 84\\n\\nregularization, defined, 55\\nregularized regression, 55-57\\nreinforcement learning (RL), 281-345\\n\\nartificial neural networks/deep learning, 292\\nBellman equations, 288\\ncomponents, 284-288\\ncomponents in trading context, 287\\ndeep Q-network, 296\\ndefined, 9\\nderivatives hedging (see derivatives hedg‐\\n\\ning)\\n\\nMarkov decision processes, 289-292\\nmodel definition, 286\\nmodel-based algorithms, 293\\nmodel-free algorithms, 293\\nmodeling framework, 288-293\\nmodels, 293-298\\npolicy, 285\\npolicy gradient method, 297\\nportfolio allocation (see portfolio allocation)\\nQ-learning, 294\\n\\nQ-value, 286\\nSARSA, 295\\nshortcomings, 298\\ntemporal difference learning, 292\\ntheory and concepts, 283-283\\ntrading strategy (see trading strategy,\\nreinforcement-learning based)\\n\\nvalue function, 285\\n\\nrelative strength index (RSI), 182\\nReLU (rectified linear unit) function, 37\\nrescaling, 23\\nresidual sum of squares (RSS), 52\\nretrieval-based chatbots, 384\\nreward function, 287, 301\\nreward, defined, 284\\nridge regression, 56\\nrisk management, 4\\nrisk tolerance and robo-advisor case study,\\n\\n125-141\\ndata preparation, 128-131\\nevaluation of models, 134\\nfeature selection, 131-134\\nfinalizing the model, 136\\nloading data and Python packages, 127\\nmodel tuning and grid search, 135\\nrobo-advisor dashboard, 138-140\\n\\nrisk-free assets, 128\\nrisky assets, 128\\nRL (see reinforcement learning)\\nRMSProp (root mean square propagation), 39\\nRNNs (see recurrent neural networks)\\nrobo-advisors, 2\\n\\n(see also risk tolerance and robo-advisor\\n\\ncase study)\\nROC (rate of change), 182\\nroot mean square propagation (RMSProp), 39\\nroot mean squared error (RMSE), 24, 76\\nRSI (relative strength index), 182\\nRSS (residual sum of squares), 52\\nrule-based chatbots, 384\\nR² metric, 76\\n\\nS\\nSARSA, 295\\nsaving a model, 28\\nScikit-learn, 14\\nSciPy, 14\\nSeaborn, 15\\nseasonal variation, 87\\n\\nIndex \\n\\n| \\n\\n407\\n\\n\\x0cSecurities and Exchange Commission (SEC),\\n\\n347\\n\\nself-learning chatbots, 384\\nsensitivity (recall), 78\\nsentiment analysis, 5\\nsentiment analysis-based trading strategies,\\n\\n362-383\\ndata preparation, 365-368\\nevaluating models for sentiment analysis,\\n\\n368-377\\n\\nexploratory data analysis and comparison,\\n\\n374-377\\n\\nloading data and Python packages, 365\\nmodels evaluation, 377-383\\nresults for individual stocks, 378-381\\nresults for multiple stocks, 381\\nsetting up a strategy, 378\\nsupervised learning: classification algo‐\\n\\nrithms and LSTM, 370-372\\n\\nunsupervised learning: model based on\\n\\nfinancial lexicon, 373-374\\n\\nvarying the strategy time period, 382\\n\\nSequential model (Keras), 41\\nSharpe radio, 210-213\\nsigmoid function, 37, 300\\nsilhouette method (k-means clustering), 248\\nsimulation environment, 337\\nsingular value decomposition (SVD), 200,\\n\\n231-233\\n\\nsklearn, 14, 60, 67\\nspaCy, 350, 354-355\\nimporting, 385\\nword embedding via, 359\\n\\nstandardization, 23\\nstate (definition), 284, 300\\nstate (value) function, 285\\nstationary time series, 89\\nStatsModels, 15\\nstemming, 352\\nstochastic oscillators, 182\\nstochastic policy, 285\\nstock price prediction, 95-113\\ndata preparation, 103\\ndata visualization, 100-102\\ndescriptive statistics, 100\\nevaluation of models, 103-110\\nexploratory data analysis, 100-103\\nfeatures useful for, 96\\nfinalizing the model, 112\\n\\n408 \\n\\n| \\n\\nIndex\\n\\nloading data, 99\\nloading Python packages, 98\\nmodel tuning and grid search, 110-112\\ntime series analysis, 102\\n\\nsum of squared residuals (RSS), 52\\nsum squared error, 64\\nsupervised classification, 151-191\\n\\nbitcoin trading strategy (see bitcoin trading:\\n\\nstrategy)\\n\\nfraud detection (see fraud detection)\\nloan default probability (see loan default\\n\\nprobability)\\n\\nselecting an evaluation metric for, 79\\n\\nsupervised learning, 49-82\\nANN-based models, 71\\nclassification, 151-191\\n\\n(see also classification)\\n\\nclassification and regression trees, 63-65\\nclassification metrics, 77\\ncross validation, 74\\ndefined, 7, 49\\nensemble models, 65-71\\nevaluation metrics, 75-79\\nfactors for model selection, 79\\nhyperparameters, 62\\nk-nearest neighbors, 60\\nlinear discriminant analysis, 62\\nlinear regression, 52-54\\nlogistic regression, 57\\nmodel performance, 73-79\\nmodel selection, 79-82\\nmodel trade-off, 81\\noverfitting/underfitting, 73\\noverview of models, 51-73\\nregression, 8\\nregularized regression, 55-57\\nselecting evaluation metric for supervised\\n\\nregression, 76\\n\\nsupport vector machine, 58-60\\n\\nsupervised regression\\n\\ntime series models versus, 84\\nyield curve prediction (see yield curve pre‐\\n\\ndiction)\\n\\nsupport vector machine (SVM), 58-60\\nadvantages and disadvantages, 60\\nhyperparameters, 60\\n\\nSVD (singular value decomposition), 200,\\n\\n231-233\\n\\n\\x0cT\\nt-distributed stochastic neighbor embedding (t-\\n\\nSNE), 202, 233\\ntanh function, 37\\ntemporal difference (TD) learning, 292\\n\\nQ-learning and, 294\\nSARSA, 295\\nTensorFlow, 15\\ntesting datasets, 24, 27\\nTextBlob, 349, 368-370\\nTF–IDF (term frequency–inverse document\\n\\nfrequency), 358\\n\\nTheano, 15\\ntime series (defined), 86\\ntime series analysis (defined), 84\\ntime series models\\nARIMA, 91\\nautocorrelation, 88\\nbasics, 86-95\\ndeep learning approach to modeling, 92-94\\ndifferencing, 90\\nmodifying data for supervised learning\\n\\nmodel, 95\\nstationarity, 89\\nsupervised regression versus, 84\\ntime series components, 87\\ntraditional models, 90-92\\n\\ntime series prediction, 72\\ntokenization, 351\\ntopic modeling, LDA for, 362\\ntopic visualization, 397\\ntrade settlement, 5\\ntrading strategy, reinforcement-learning based,\\n\\n298-316\\nagent class, 305-307\\ndata preparation, 303\\nevaluation of algorithms and models,\\n\\n303-314\\n\\nexploratory data analysis, 302\\nhelper functions, 308\\nimplementation steps and modules, 304\\nloading data and Python packages, 301\\nmodel tuning, 313\\ntesting the data, 314-315\\ntraining the model, 308-313\\n\\ntraining datasets, 24\\ntraining, for ANNs, 34-36\\n\\nbackpropagation, 35-36\\nforward propagation, 34\\n\\ntransition probability function (P), 286\\ntrend, defined, 87\\ntrue positive rate (recall), 78\\ntruncated SVD, 200, 231-233\\ntuning a model, 25\\n\\nU\\nunder-sampling, 161-165\\nunderfitting, 73\\nunderwriting, 3\\nunivariate plot types, Python code for, 19\\nunsupervised learning\\ndefined, 8, 195\\nLDA for topic modeling, 362\\n\\nV\\nVADER (Valence Aware Dictionary for Senti‐\\n\\nment Reasoning), 373\\n\\nvalue function, 285\\n\\nQ-value and, 286\\nvariable intuition, 28\\nvariance, defined, 73\\nvisualization of data, 19\\nvolatility, 116\\n\\nW\\nweights, neuron, 34\\nword cloud, 398\\nword embedding, 358\\nvia spaCy, 359\\nvia word2vec model, 359\\n\\nY\\nyield curve construction/interest rate modeling\\n\\ncase study, 217-227\\ndata preparation, 220\\ndata visualization, 219-220\\nevaluation of algorithms and models,\\n\\n222-226\\n\\nexploratory data analysis, 219-220\\nloading data and Python packages, 218\\n\\nyield curve prediction, 141-149\\ndata visualization, 144-146\\nevaluation of models, 146\\nexploratory data analysis, 144-146\\nloading data and Python packages, 143\\nmodel tuning and grid search, 147-149\\n\\nIndex \\n\\n| \\n\\n409\\n\\n\\x0cAbout the Authors\\n\\nHariom  Tatsat  currently  works  as  a  vice  president  in  the  Quantitative  Analytics\\nDivision of an investment bank in New York. Hariom has extensive experience as a\\nquant in the areas of predictive modeling, financial instrument pricing, and risk man‐\\nagement in several global investment banks and financial organizations. He comple‐\\nted  his  MS  at  UC  Berkeley  and  his  BE  at  IIT  Kharagpur  (India).  Hariom  has  also\\ncompleted  FRM  (Financial  Risk  Manager)  certification  and  CQF  (Certificate  in\\nQuantitative Finance) and is a candidate for CFA Level 3.\\n\\nSahil  Puri  works  as  a  quantitative  researcher.  His  work  involves  testing  model\\nassumptions and finding strategies for multiple asset classes. Sahil has applied multi‐\\nple statistical and machine learning–based techniques to a wide variety of problems.\\nExamples  include  generating  text  features,  labeling  curve  anomalies,  nonlinear  risk\\nfactor detection, and time series prediction. He completed his MS at UC Berkeley and\\nhis BE at Delhi College of Engineering (India).\\n\\nBrad Lookabaugh works as a VP of Portfolio Management at Union Home Owner‐\\nship  Investors,  a  real  estate  investment  startup  based  in  San  Francisco.  His  work\\nfocuses on the implementation of machine learning and investment decision models\\nin business processes, internal systems, and consumer-facing products. Similar to his\\ncoauthors, Brad completed his MS in financial engineering at UC Berkeley.\\n\\n\\x0cColophon\\n\\nThe  animal  on  the  cover  of  Machine  Learning  and  Data  Science  Blueprints  for\\nFinance  is  the  common  quail  (Coturnix  coturnix),  a  migratory  bird  that  breeds  in\\nEurope, Turkey, and central Asia to China, wintering in parts of southeast Asia and\\nacross the continent of Africa.\\n\\nThe common quail is small and round, streaked brown with a white eyestripe and, in\\nmales, a white chin. It has long wings to benefit its migratory nature, and is several\\ninches in length. Its size and plumage allow it to blend well into its environment. Its\\nappearance  in  combination  with  its  secretive  nature  means  the  common  quail  is\\nrarely seen and more often detected by the male’s emphatic whistle.\\n\\nCommon  quail  consume  mostly  seeds,  grains,  and  nuts,  but  females  require  a  high\\nprotein  diet  for  breeding,  dining  on  beetles,  ants,  and  earwigs  among  other  small\\ninsects. Whether pecking at wind-scattered seeds or insects, the quail feeds mainly on\\nthe ground. Though it travels great migratory distances, the common quail is reluc‐\\ntant to fly even when disturbed.\\n\\nThe  common  quail  has  been  depicted  in  Egyptian  hieroglyphs  dating  back  to  circa\\n5000 B.C.E. and have been raised for human consumption since the construction of\\nthe  Great  Pyramids.  Common  quail  in  Europe  lay  up  to  13  eggs  per  clutch,  and\\nchicks can fly when they are just 11 days old.\\n\\nWhile the common quail’s conservation status is currently listed as of least concern,\\nindustrial scale trapping is driving the species into decline. Many of the animals on\\nO’Reilly covers are endangered; all of them are important to the world.\\n\\nThe cover illustration is by Karen Montgomery, based on a black and white engrav‐\\ning  from  Shaw’s  Zoology.  The  cover  fonts  are  Gilroy  Semibold  and  Guardian  Sans.\\nThe  text  font  is  Adobe  Minion  Pro;  the  heading  font  is  Adobe  Myriad  Condensed;\\nand the code font is Dalton Maag’s Ubuntu Mono.\\n\\n\\x0cThere’s much more  \\nwhere this came from.\\n\\nExperience books, videos, live online  \\ntraining courses, and more from O’Reilly  \\nand our 200+ partners—all in one place.\\n\\nLearn more at oreilly.com/online-learning\\n\\n5\\n7\\n1\\n\\n|\\n\\n.\\n\\nc\\nn\\n\\nI\\n\\n,\\n\\ni\\n\\na\\nd\\ne\\nM\\ny\\n\\nl\\nl\\ni\\n\\ne\\nR\\nO\\n\\n’\\n\\nf\\no\\nk\\nr\\na\\nm\\ne\\nd\\na\\nr\\nt\\nd\\ne\\nr\\ne\\nt\\ns\\ng\\ne\\nr\\n\\ni\\n\\na\\n\\ns\\n\\ni\\n\\ny\\n\\nl\\nl\\ni\\n\\ne\\nR\\nO\\n\\n’\\n\\n.\\n\\nc\\nn\\n\\nI\\n\\n,\\n\\ni\\n\\na\\nd\\ne\\nM\\ny\\n\\nl\\nl\\ni\\n\\n’\\n\\ne\\nR\\nO\\n9\\n1\\n\\n0\\n2\\n©\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c', metadata={'source': 'source_documents/hom.pdf'}), Document(page_content='Automated Planning Tool (APT): A mixed integer non-linear programming\\nproblem solver for Workorder scheduling\\n\\nAnupam Purwar (RME Science, Amazon) * 1\\n\\nAbstract\\nWorkorder scheduling is a type of Resource allo-\\ncation problem which is NP-complete. Workorder\\nscheduling related to maintenance work becomes\\nmore challenging because of the inherent com-\\nplexity of e-commerce facilities owing to vari-\\nous local requirements and variety of equipments\\nviz.electrical, mechanical and electro-mechanical\\netc. Hence, we formulated the problem by using\\nmultiple decision variables to accommodate all\\nthese requirements. Next, we decomposed the\\nproblem into two smaller sub-problems to ensure\\na fast yet accurate solution with load balancing of\\nworkorder hours as the main objective function.\\nThrough this objective function, we are address-\\ning the business requirement of reducing overtime\\nand ensuring uniformity in workorders to be ex-\\necuted across the planning horizon. We have a\\nmix of continuous and binary decision variables\\nwith multiple linear constraints, hence, this prob-\\nlem has been formulated as a Mixed Integer Non-\\nlinear programming problem (MINLP) with non-\\nlinear objection function. Being a NP-hard hard\\nproblem the runtime increases as the problem size\\nincreases (problem size is proportional to num-\\nber of planning days and number of workorders).\\nHowever, through problem decomposition, code\\nvectorization and use of AWS EC2 instances, we\\nhave achieved a run time in the range of 5-20 min-\\nutes. Through this article, we have covered the\\nvital details of optimization model, cloud compute\\ninfrastructure used to solve this NP-hard problem\\nand salient points of MINLP model formulation.\\n\\n1. Introduction\\n\\nWorkorder scheduling is a popular problem which has been\\nsolved using optimization models for production planning\\n[1], facility maintenance [2] and online scheduling in smart\\nfactories [3]. In this paper, we have discussed our ﬁndings\\nfrom development of an optimization model for scheduling\\nmaintenance activities in e-commerce facilities. Present\\nday e-commerce facility process large volume of customer\\n\\norders everyday, only possible because of high reliability of\\nequipment supporting these facilities. The health of these\\nequipments is taken care by site maintenance teams by exe-\\ncuting and planning/scheduling workorders (WOs) for thou-\\nsands of equipments of different types every day. Planning\\nand scheduling carried out by the sites is key to maximize\\nequipment uptime through load-balancing and prioritiza-\\ntion of critical maintenance activities at these facilities/sites.\\nGenerally, Workorder scheduling is done manually using\\nexcel spreadsheets requiring manually updating information\\non ERP system to produce the daily plan. This process is\\ntime consuming, sub-optimal and done for short time win-\\ndows( 5-7 days). This takes away any scope of centraliza-\\ntion as well as associated opportunities related to centralized\\nplanning, cluster level work aggregation and realize associ-\\nated efﬁciencies. Centralized planning using a data driven\\nsolution can help create multiple plans for different scenar-\\nios by minimizing variance in workorder allocation across\\nplanning period. Also, it makes workorder scheduling more\\naccurate, less time consuming, easily explainable to the\\nsite maintenance teams and bring uniformity in scheduling\\nacross all the sites. Hence, we propose a data driven solu-\\ntion namely Automated Planning Tool (APT) to serve as\\na recommendation engine for the e-commerce facilities to\\nefﬁciently and effectively execute day to day scheduling as\\nwell as long-term planning.\\n\\n2. Problem Statement\\n\\nThe problem at hand is to make sure all the open worko-\\nrders are scheduled to the available technicians in a uniform\\nmanner. Every Workorder (WO) needs to be mapped to a\\ntechnician/technicians depending who carry the right quali-\\nﬁcation keeping in mind their availability. The availability\\nof a technician is governed by their availability information\\nand technician-shift mapping information. The purpose of\\nthe model is to schedule every WO to a given shift/day and\\na technician pool, while achieving the following objectives :\\n\\n• The sum total of hours assigned to technicians at facil-\\nity per day across planning horizon shall be as uniform\\nas possible\\n\\n• The percentage of workorders getting scheduled by the\\n\\n\\x0cAutomated Planning Tool (APT): A mixed integer non-linear programming problem solver for Workorder scheduling\\n\\nmodel shall be as high as possible\\n\\n• Model shall abide by site level planning requirements\\nviz. geographic co-location of equipment and techni-\\ncians, equipment nature (high criticality equipments\\nare serviced in Downtime only), shift timings of facility\\nas well as variable shift-technician mapping\\n\\nTo meet the ﬁrst objective, we need to arrive at a distribution\\nof WOs in a way that the number of hours allocated across\\nthe “n” day planning horizon (PH) remains as uniform as\\npossible. (n is a hyper parameter entered by planner). Ide-\\nally, total hours allocated per day (y-axis) when plotted with\\nrespect to PH (x-axis), the plot should represent WOs being\\nspread out as evenly as possible across the planning horizon.\\nTo achieve this, we have set minimization of variance of\\nWorkorder hour distribution as the objective function.\\n\\n3. Model Development\\n\\nIn the present article, we have solved a resource allocation\\nproblem inspired from the use case of workorder scheduling\\ndone in e-commerce facilities. We have developed a tool\\naround this model called Automated Planning Tool (or APT).\\nBrieﬂy, the problem is to map the technicians to a set of\\ntasks based on certain requirements modelled as constraints.\\nThe complexity of APT is governed by the nature of its\\nconstraints viz. the time window within which a task is to\\nbe completed, eligibility/availability of technicians to work\\non a given task and the physical construct of the facility\\nfor which the schedule is being created. Last but not the\\nleast, the schedule/plan is to be created at a click of few\\nbuttons within few minutes (5-20 mins per run). Keeping all\\nthese challenges in mind, we divided our research objectives\\ninto multiple categories viz. model creation, model testing,\\nruntime optimization and deployment.\\n\\n3.1. WO scheduling: Resource allocation problem\\n\\nWO scheduling can be modelled as a Resource allocation\\nproblem(RAP). Generally, the complexity of a RAP deter-\\nmines the complexity of a scheduling problem (SP). A difﬁ-\\ncult RAP would result in an equally hard scheduling prob-\\nlem. In mathematical terms, optimally solving a scheduling\\nproblem to determine an optimized network topology is an\\nNP-complete problem. NP-complete problem is a problem\\nwhich is both NP (veriﬁable in non-deterministic polyno-\\nmial time) and NP-hard (any NP-problem can be translated\\ninto this problem) [4]. NP-complete problems is a class\\nof computational problems for which no efﬁcient solution\\nalgorithm has been found. Some examples are Travelling\\nSalesman problem, satisﬁability problems and graph cover-\\ning problems [5]. These problems become more challenging\\nwhen RAP/SP has time bound tasks. One has to explore\\nentire search space to ﬁnd the optimal solution of the SP in\\n\\na reasonable amount of time, therefore, several heuristics\\nare deployed to ﬁnd a near-optimal solution. As a precursor\\nto understand a RAP, ﬁrst problem formulating and mod-\\nelling needs to be done. Modelling is the most critical step\\ntowards solving RAP and is at the core of machine learn-\\ning/Artiﬁcial Intelligence. Robust model creation is the\\nﬁrst step of devising an efﬁcient problem-solving strategy.\\nHowever, modelling NP-complete real world problems is\\nnot straightforward because there are a host of factors at\\nplay consisting of both known unknowns and unknown un-\\nknowns, where the former is still easier to anticipate but\\nthe latter are intractable. Besides, no mathematical guide-\\nlines or algorithms exist which can guide about the factors\\nwhich need to be accounted for during the modelling pro-\\ncess. Through this work, we have carefully balanced the\\nscientiﬁc rigour of innovation with ever evolving require-\\nments and expectations of the customers to develop a model\\nwhich is modular and modiﬁable.\\n\\nThe model requires input data from demand (e.g. pre-\\ndicted workorders) and supply sides (e.g., technician-shift,\\ntechnician-trade, technician-zone mapping and shift-zone\\nmapping for technicians). Besides, there are some other\\ninputs which user needs to enter viz. 1) Planning horizon:\\nno of days for which one wants to pick and schedule worko-\\nrders for, 2) percentage utilization of technician per shift, 3)\\nNo of days beyond planning horizon which user wants to ex-\\npand the scheduling window to. The key modelling output\\nis the name of technician/technicians as well as shift/date\\nand scheduling block to which a WO is mapped.\\n\\n3.2. Workorder scheduling: A non-linear MIP\\n\\nWork order assignment has been formulated as a Mixed\\ninteger non-linear optimization problem with linear con-\\nstraints and non-linear objective function. The objective\\nfunction is to minimize the variance of WO hours allocated\\nacross planning horizon. Mixed Integer-Programming or\\nMIP problems are formulated when the decision variables\\nare to be restricted to either integer or binary values. For\\nexample, in present case one of the decision variable repre-\\nsents allocation of a technician to a workorder which has to\\nbe a binary value and another decision variable represents\\nallocation of a workorder to a shift which again has to be\\na binary value. On the other hand, we have decision vari-\\nables which assume continuous values like how many hours\\nduring a given day will a technician work on the assigned\\nworkorder. Thus, a mix of binary and continuous decision\\nvariables make this problem also a Mixed Integer Program-\\nming (MIP) problem, and in generally MIPs are harder to\\nsolve than LP problems [6, 7, 8]. In the current problem,\\nobjective function selected is minimization of deviation of\\nhours allocated per day from the mean of this distribution\\nof allocated hours. Deviation being a non-linear quantity\\nmakes the objective function also non-linear. Hence, the\\n\\n\\x0cAutomated Planning Tool (APT): A mixed integer non-linear programming problem solver for Workorder scheduling\\n\\nWO scheduling problem is Mixed Integer Non-linear pro-\\ngramming problem or MINLP. This MINLP has been solved\\nusing Branch and Bound technique [9].\\n\\ntechnician a is assigned to trade t on shift s\\nBinary variable zi=1 if work order is NOT allocated\\nSubscript: ”o” for ORT orders and ”d” for DTW orders\\n\\n3.3. Algorithm\\n\\nRoutine 1 consumes the Demand side (all open WOs\\nof PM/PDM/PR type) and supply side: technician-trade,\\ntechnician-shift, technician availability information to match\\nthe WO demand hours to supply of technician hours at a\\ntrade and shift level. In order to accommodate, the zone level\\nassignment of WOs, another constraint is written which\\nmakes sure only those shifts are selected which map to the\\nzone mentioned against the WO.\\n\\n3.3.1. ROUTINE 1: MAP WORKORDER TO A SHIFT\\n\\nWe have introduced slack/surplus variables to convert in-\\nequality constraints into equality constraints. If a constraint\\nis binding, then the corresponding slack variable will as-\\nsume zero value else a non-zero value. Thus, routine 1 does\\nthe mapping of WOs to zone-shift (date and site’s zone-shift\\ncombination on that day) with deviation minimization of\\n“WO allocated hours distribution” as the objective function.\\n\\nIndex\\n\\nI: Set of all work orders\\nO: Set of work orders to be completed in ORT\\nD: Set of work orders to be completed in DTW\\nJ: Set of days in the planning horizon\\nT: Set of all trades (work types)\\nA: Set of all technicians\\nS: Set of all shifts in the planning horizon\\nPH: Set of all days in the planning horizon\\nJ(s): The day which shift s belongs to\\n\\nInput data (after pre-processing)\\n\\nnit: Number of technicians required for WO i of trade t\\nhi: Estimated working hours required for work order i\\nzi: Zone required for work order i\\nLs: Length of shift s in hours\\nLso: Number of ORT hours available in shift s\\nLsd: Number of DTW hours available in shift s\\npi: A penalty cost for NOT completing work order i during\\nthe planning horizon\\nri: It will be zero if no eligible shift is found for the WO i\\nand 1 if there is atleast one shift eligible to complete WO i\\n\\nDecision variables\\n\\nBinary variable xis = 1 if work order i is assigned to shift s\\nContinuous variable yoast ∈ [0,1] indicates fraction of time\\ntechnician a is assigned to trade t on shift s\\nContinuous variable ydast ∈ [0,1] indicates fraction of time\\n\\nConstraints: Routine 1\\n\\nResource allocation constraint: maps WO to a shift based\\non trade, number of technicians and number of hours\\nrequired by WO. This constraint makes sure the workorders\\nare not allocated in a way which exceeds the maximum\\nallowed hours in the shift s across all technicians\\n\\nFor ORT Workorders,\\n\\no\\n(cid:88)\\n\\ni=1\\n\\nmin(Lso, hi)·nit·xis =\\n\\nFor DTW Workorders,\\n\\no\\n(cid:88)\\n\\ni=1\\n\\nmin(Lso, hi)·nit·xis =\\n\\nn\\n(cid:88)\\n\\na=1\\n\\nn\\n(cid:88)\\n\\na=1\\n\\nLs·yoast∀s ∈ S, t ∈ T\\n\\nLs·ydast∀s ∈ S, t ∈ T\\n\\nTech schedule/eligibility constraint:\\n\\nydast = 0 ∀s ∈ S, if technician a is not eligible to do trade t\\nydast = 0 ∀t ∈ T, if technician a does not work on shift s\\nyoast = 0 ∀s ∈ S, if technician a is not eligible to do trade t\\nyoast = 0 ∀t ∈ T, if technician a does not work on shift s\\n\\nThis constraint makes sure the technician ‘a’ does not work\\nbeyond the maximum allowed hours in the shift ‘s’\\n\\n(cid:88)\\n\\nydast ≤ DTW hrs ratio in shift s, ∀a ∈ A, s ∈ S\\n\\nt∈T\\n(cid:88)\\n\\nt∈T\\n\\nyoast ≤ ORT hrs ratio in shift s, ∀a ∈ A, s ∈ S\\n\\nxis=0 if shift s does not have the minimal number of techs\\nwith the required trades for WO i\\n\\nZone-shift combination constraint:\\n\\nxis = 0 if shift s is not mapped to the zone z of WO i\\n\\nShift available hours constraint: Workorder needs to be\\nassigned with sufﬁcient working hours for completion\\n\\n(cid:88)\\n\\ns∈S\\n(cid:88)\\n\\ns∈S\\n\\nmin(Lso, hi)·xis + shorti − overi = hi∀i ∈ O\\n\\nmin(Lsd, hi)·xis + shorti − overi = hi∀i ∈ D\\n\\n\\x0cAutomated Planning Tool (APT): A mixed integer non-linear programming problem solver for Workorder scheduling\\n\\nzi ≥\\n\\n1\\nM\\n\\n∗ shortagei, ∀i ∈ IW orkorders\\n\\nInput data\\n\\nS: shift-day combination\\n\\nxis=0, if WO i is a DTW WO but shift s has no DTW\\nxis=0, if WO i is an ORT WO but shift s has no ORT\\n\\nCompliance window constraint:\\n\\nnit: Number of technicians required for WO i in trade t\\nhi: Working hours assigned to the shift for WO i L: Length\\nof the shift\\n\\nxis = 0 if shift s is NOT within compliance window of WO i\\n\\nDecision variables\\n\\nDeﬁne daily workload deviation:\\n\\ndevj ≥\\n\\n1\\nJ\\n\\ni∈I\\n(cid:88)\\n\\ns∈S\\n\\nmin(Ls, hi)·xis −\\n\\ni∈I\\n(cid:88)\\n\\ns∈S\\n\\nmin(Lso, hi), ∀j ∈ J\\n\\ndevj ≥\\n\\ni∈I\\n(cid:88)\\n\\ns∈S\\n\\nmin(Ls, hi) −\\n\\n1\\nJ\\n\\ni∈I\\n(cid:88)\\n\\ns∈S\\n\\nmin(Lso, hi)·xis, ∀j ∈ J\\n\\nObjective function:\\n(cid:88)\\n\\nMinimize :\\n\\ndevj + pi\\n\\nj∈J\\n\\n(cid:88)\\n\\ni∈I\\n\\nzi + pover\\n\\n(cid:88)\\n\\ni∈I\\n\\noveri+\\n\\n(cid:88)\\n\\ni∈I\\n\\nri·pi·(\\n\\n(cid:88)\\n\\n(1 − xis)) +\\n\\n(cid:88)\\n\\n(psi·zi + poi·overi)\\n\\ni∈I\\n\\ni∈I\\n\\npsi: Penalty cost for short time variable (shorti) for W Oi\\npoi: Penalty cost for surplus time variable (overi) for W Oi\\n\\n3.3.2. ROUTINE 2: MAP WORKORDER TO A TECHNICIAN\\n\\nRoutine 2 performs the next step of ﬁnding technician for a\\ngiven WO by consuming the shift estimated by Routine 1.\\nWhile doing so, the WO would be assigned preferentially\\nto a technician who is mapped to zone estimated for this\\nWO by Routine 1. However, in case estimated zone-shift\\ncombination does not exist, then the WO can be assigned\\nto any technician who is available and eligible for the WO.\\nTo model this, costs in ascending order are deﬁned for A)\\ntechnician who is mapped to estimated zone (lowest cost),\\nB) a technician who is not mapped to any zone and C) high-\\nest cost for a technician who is not mapped to the estimated\\nzone. In case, there is no zone associated with the WO, any\\neligible/available technician in the estimated shift can be\\nassigned to WO. Thus, routine 2 ﬁnds out the most appro-\\npriate technician for a WO by minimizing cost associated\\nwith its allocation( see sample model output in Table 1).\\n\\nIndex\\n\\nI: Set of all work orders assigned to the shift\\nT: Set of all trades (workorder types)\\nA: Set of all technicians who work during a given shift\\n\\nBinary variable xiat = 1 if workorder i is assigned to tech a\\nto perform trade t\\nContinuous variable ciat which denotes the cost of perform-\\ning workorder i by tech a for trade t\\n\\nConstraints\\n\\nFollowing constraint makes sure that WOs (i ∈I) remain\\nunscheduled by assigning a non-zero value to qi which in\\nturn gets penalized for being non-zero using a very high\\npenalty (M) in objective function.\\n\\n(cid:88)\\n\\na∈A\\n\\nxiat + M ·qi ≥ nit∀i ∈ I\\n\\nFollowing constraint makes sure that across all WOs (i ∈I)\\nmapped to a given shift (s), total number of hours allocated\\ndoes not exceed the maximum hours (L) available in that\\nshift for ORT and DTW WOs respectively.\\n\\nFor ORT Work orders, L = Lo\\n\\n(cid:88)\\n\\ni∈I,t∈T\\n\\nmin(hi, L) ≤ L ∀a ∈ A\\n\\nFor DTW Work orders, L = Ld\\n\\n(cid:88)\\n\\ni∈I,t∈T\\n\\nmin(hi, L) ≤ L ∀a ∈ A\\n\\n(cid:88)\\n\\nt∈T\\n\\nxiat ≤ 1 ∀i ∈ I, a ∈ A\\n\\nxiat = 0 if technician a is not eligible to do trade t\\n\\nConstraint which deﬁnes the cost ci of allocating W Oi\\n\\n(cid:88)\\n\\na∈A,t∈T\\n\\nciat·xiat ≤ ci, ∀i ∈ I\\n\\n\\x0cAutomated Planning Tool (APT): A mixed integer non-linear programming problem solver for Workorder scheduling\\n\\nTable 1. Overall model output for a given WO\\n\\nWO\\n\\nSHIFT\\n\\nDATE\\n\\nZONE\\n\\nTECHNICIAN\\n\\n123456 ABC1\\n\\n2020-12-18 ABC1-18\\n\\nXXYZ\\n\\nObjective function:\\n\\nMinimize :\\n\\n(cid:88)\\n\\ni∈I\\n\\nci + M\\n\\n(cid:88)\\n\\ni∈I\\n\\nqi + pi\\n\\n(cid:88)\\n\\ni∈I\\n\\nwi\\n\\nis lower than variance of demand hours distribution across\\nall the days. However, the ratio of variance of allocated\\nhours and demand hours distribution does not essentially go\\ndown by just adding more number of days to the scheduling\\nhorizon. This can be explained by the fact that ability to\\nschedule workorders is limited by various constraints like\\nworkorder trade, zone, compliance window and availability\\nof downtime window for DTW workorders.\\n\\n4. Current state\\n\\nCurrently, APT model has two components viz. the User\\nInterface (UI) and the backend compute capacity. Each\\nmodule of APT is written in Python and utilizes AWS in-\\nfrastructure for data storage/handling. It’s solver module\\nuses Xpress package which is a commercial solver for opti-\\nmization problems [10]. APT users can use the model by\\naccessing it’s Web UI and run the tool themselves, download\\nthe output and verify it. The Web UI requires the user to en-\\nter the site name and the number of days for which they want\\nto plan the WOs. On the output side, the UI provides several\\nmetrics including the number of WOs picked/scheduled,\\nmodel run time, visual plots of demand hours with respect\\nto allocated hours as a function of dates across which WOs\\nare due and a downloadable .csv ﬁle which contains details\\nof scheduled WOs.\\n\\n5. Results\\n\\nThe essence of having solver module is to spread out the WO\\nhour allocation as uniformly as possible across the planning\\nhorizon. We can see that the demand hours as shown in blue\\nhave a lot of ﬂuctuation with respect to time, however, the\\nallocated hours across DTW and ORT are more uniformly\\nspread out as scheduled by APT.\\n\\n5.1. Results for Site 1\\n\\nThe plots here compares how the demand contrasts with the\\nallocation of hours across Downtime window (DTW) and\\nOperational runtime (ORT). In each of the 4 model runs,\\nthe number of workorders picked is the same, however, the\\nnumber of days in planning horizon has been varied. Worko-\\nrders are picked for 7 days (planning horizon: PH) in all 4\\ncases, however, the number of days across which they could\\nbe spread is varied. In Fig.1(a), workorders as spread across\\nonly the 8 days (scheduling horizon: SH) for which they\\nhave picked up, however, in Fig.1 (b, c, d), workorders have\\nbeen spread across 9, 10, 12 days (scheduling horizon). As\\nwe can see here, the variance of allocated hours distribution\\n\\nFigure 1. Results for site 1: Demand vs. Allocated Hours\\n\\n5.2. Results for Site 2\\n\\nSimilar to previous section, the plots here compares how the\\ndemand contrasts with the allocation of hours across Down-\\ntime window (DTW) and Operational runtime (ORT). In\\neach of the 4 model runs, the number of workorders picked\\nis the same, however, the number of days in planning hori-\\nzon has been varied. Workorders are picked for 7 days in\\nall 4 cases, however, the number of days across which they\\ncould be spread is varied. In Fig.2 (a), workorders as spread\\nacross only the 8 days (scheduling horizon) for which they\\nhave picked up, however, in Fig.2 (b, c, d), workorders have\\nbeen spread across 9, 10, 12 days (scheduling horizon). In\\nthis case, we observe an interesting thing here, the variance\\nof allocated hours distribution is higher than the variance\\nof demand hours distribution in Fig. 2(a). This may in-\\ndicate that the model is not able to meet its objective of\\nminimizing variance. However, a more careful investigation\\ntells that the model could not allocate enough workorders\\non days: 3,7 because most workorders falling due on these\\ndays are mapped to zone/shifts which are not available on\\ndays: 3,7. This is the direct outcome of the zone constraint\\nin our model, which is a hard constraint and speciﬁes that a\\nworkorder can only be done in the shift mapped to the zone\\nof that workorder. Similar to the case for site 2, the ratio of\\n\\n\\x0cAutomated Planning Tool (APT): A mixed integer non-linear programming problem solver for Workorder scheduling\\n\\nvariance of allocated hours and demand hours distribution\\ndoes not essentially go down by just adding more number of\\ndays to the scheduling horizon. However, it is very evident\\nthat model does reduce the variance of allocated hours dis-\\ntributed as compared to demand hours. Thus, APT’s model\\nachieves load-balancing as much as possible by comply-\\ning the requirements of each workorder and the associated\\nconstraints with the available resources/technician hours.\\n\\nFigure 2. Results for site 2: Demand vs. Allocated Hours\\n\\n5.3. Results: Model runtime\\n\\nAPT model solves a NP-hard problem and as expected the\\nruntime increases as the number of workorders increases.\\nThe runtime of APT is deﬁned by the time taken in a) Def-\\ninition of decision variables and constraints, b) Addition\\nof constraints, c) Solution of the model. Pre-processing\\nand post-processing consume only 15% of overall runtime.\\nSeveral time studies were carried to evaluate the runtime of\\nthe code developed around the algorithm and our deep dive\\nhelped identify following:\\n\\n• Number of constraints increase as the number of worko-\\nrders and scheduling horizon or number of shifts in-\\ncrease (Resource allocation constraint: Sec 3.3.1)\\n\\n• Number of constraints increase as number of techni-\\ncians increase (Available hour constraint: Sec 3.3.1)\\n\\nAmong the above 2 reasons, the former needs more attention\\nbecause it is directly proportional to number of workorders\\nand deﬁnition of this constraint requires one to iterate over\\nall the possible shift-workorder combinations. Thus, the\\ndeﬁnition time of resource allocation constraint is directly\\nproportional to problem size. On the other hand, deﬁnition\\n\\ntime for shift available hours constraint does not grow as\\nstrongly with increase in problem size because its right hand\\nside is only proportional to number of workorders. Besides,\\nother constraints like workorder compliance window, trade\\neligibility and zone eligibility are also proportional to num-\\nber of shift-workorder combinations. It was observed that\\nsolving for 1000 workorders would require up to 4 hours in\\na 30GB RAM ECS Fargate container. Hence, optimization\\nof code was carried, several iterative loops were combined\\ninto one to deﬁne the constraints together into one by using\\nconditional statements alongwith abstraction of nested iter-\\native loops. These code optimization efforts helped bring\\ndown APT run time with p90 value of 20 minutes.\\n\\n5.4. Results: Percentage of workorder scheduled\\n\\nThe model has 6 important constraints which drastically\\nreduce the possible workorder-shift- technician mappings.\\nHowever, with all these hard constraints in place, APT\\nmodel has been found to schedule 95% of the workorders\\non an average. This number goes up when the scheduling\\nhorizon is increased keeping the planning horizon constant.\\nAlso, this number goes up when the percentage utilization\\nof technician per shift is increased. This is quite obvious\\nbecause increasing shift utilization increases the number\\nof hours available in a given shift, thereby allowing more\\nworkorders to get scheduled. Most of the workorders which\\nwere unscheduled belonged to downtime window which is\\nnot available in every shift and even if available has just\\na 2 hour window per work day. Hence, we have allowed\\nAPT user to conﬁgure the shift utilization percentage and\\nscheduling horizon on their own while using the model.\\nThis degree of freedom provided to APT users empowers\\nthem to utilize the model as per their requirement during\\ntimes where the facilities have more than the usual volume\\nof work.\\n\\n6. Conclusion\\n\\nThis paper reports the development of an optimization\\nmodel for a scheduling problem. Development of the model\\ninvolved understanding the workorder scheduling process\\nof e-commerce facility maintenance teams besides the math-\\nematical formulation of problem. Workorder scheduling has\\nbeen formulated as a Mixed-Integer Non-linear program-\\nming problem with a mix of binary/continuous decision\\nvariables and a non-linear objective function. An algorithm\\nhas been formulated keeping in mind various requirements\\nrelated to workorders modelled as constraints. Some impor-\\ntant facts about the MINLP model are as follows:\\n\\n1. MINLP models uses branch-and-bound method and\\nis able to ﬁnd a feasible solution in a ﬁnite amount of\\ntime (p85 for run time is 15 mins).\\n\\n\\x0cAutomated Planning Tool (APT): A mixed integer non-linear programming problem solver for Workorder scheduling\\n\\nmcasquilho/compute/_linpro/TaylorB_\\nmodule_c.pdf. [Accessed 1 10 2020].\\n\\n8. D. P. Bertsekas, Non-Linear Programming, Athena\\n\\nScientiﬁc, 1995.\\n\\n9. Discrete Optimization,\\n\\nCoursera,\\n\\nAvailable:\\n\\nhttps://www.coursera.org/learn/\\ndiscrete-optimization\\n\\n10. F. Xpress, Branch and Bound, Fico Xpress, [On-\\nAvailable:https://www.fico.com/\\n\\nline].\\nfico-xpress-optimization/docs/\\ndms2019-03/solver/optimizer/HTML/\\nchapter4_sec_section4003.html.\\ncessed 08 2020].\\n\\n[Ac-\\n\\n2. MINLP model has 6 important constraints which dras-\\ntically reduce the possible workorder-shift-technician\\nmappings. However, with all these hard constraints in\\nplace, the model has been found to schedule 90 percent\\nof the workorders on an average. This number goes up\\nwhen the scheduling horizon is increased keeping the\\nplanning horizon constant.\\n\\n3. Strong attention is required on standardizing the inges-\\ntion of data into such complex optimization models.\\nWe created an entire data engineering pipeline using\\nvarious AWS services like S3 buckets, Redshift to get\\naccurate data with zero latency.\\n\\n4. Resource allocation problems like scheduling once de-\\ncomposed into smaller sub-problems makes them more\\ntractable.\\n\\nReferences\\n\\n1. Herrmann, Jeffrey W. ”Information ﬂow and decision-\\nmaking in production scheduling.” IIE Annual Confer-\\nence. Proceedings. Institute of Industrial and Systems\\nEngineers (IISE), 2004. [Accessed 2020].\\n\\n2. Weiwei Chen, Keyu Chen, Jack C.P. Cheng, Qian\\nWang, Vincent J.L. Gan, BIM-based framework for au-\\ntomatic scheduling of facility maintenance work orders,\\nAutomation in Construction,Volume 91,2018, Pages\\n15-30, ISSN 0926-5805 [Accessed 2020].\\n\\n3. Tong Zhou, Dunbing Tang, Haihua Zhu, Zequn\\nZhang, Multi-agent reinforcement learning for online\\nscheduling in smart factories, Robotics and Computer-\\nIntegrated Manufacturing,Volume 72, 2021,102202,\\nISSN 0736-5845 [Accessed 2021].\\n\\n4. E. W. Weisstein,\\n\\n”Wolfram- NP Complete\\nAvailable:\\n\\n[Online].\\n\\nProblem,” Wolfram,\\nhttps://mathworld.wolfram.com/\\nNP-CompleteProblem.html.\\n2021].\\n\\n[Accessed\\n\\n5. T. E. o.\\n\\nE. Britannica, ”Britannica, The Edi-\\n”NP-complete problem”.\\ntors of Encyclopaedia.\\nEncyclopedia Britannica, 16 Jan.\\nAc-\\ncessed 24 January 2021.,” [Online]. Available:\\nhttps://www.britannica.com/science/\\nNP-complete-problem. [Accessed 2021].\\n\\n2018.\\n\\n6. M. F. Tompkins, Optimization Techniques for Task\\nAllocation and Scheduling in Distributed Multi-Agent\\nOperations, Massachusetts: Massachusetts Institute of\\nTechnology, 2003.\\n\\n7. Module C on MIPs,\\n\\n[Online].\\n\\nAvailable:\\n\\nhttp://web.tecnico.ulisboa.pt/\\n\\n\\x0c', metadata={'source': 'source_documents/automated-planning-tool-apt-a-mised-interger-non-linear-programming-problem-solver-for-workorder-scheduling.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 20\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import GPT4All, LlamaCpp\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#embeddings_model_name = #os.environ.get(\"EMBEDDINGS_MODEL_NAME\")\n",
    "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
    "\n",
    "model_type = os.environ.get('MODEL_TYPE')\n",
    "model_path = os.environ.get('MODEL_PATH')\n",
    "model_n_ctx = os.environ.get('MODEL_N_CTX')\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "a=glob.glob(\"source_documents/*.txt\")\n",
    "print(len(a))\n",
    "print(a)\n",
    "for i in range(len(a)):\n",
    "\n",
    "    documents.extend(TextLoader(a[i]).load())\n",
    "\n",
    "a=glob.glob(\"source_documents/*.html\")\n",
    "print(a)\n",
    "print(len(a))\n",
    "\n",
    "for i in range(len(a)):\n",
    "\n",
    "    documents.extend(UnstructuredHTMLLoader(a[i]).load())\n",
    "\n",
    "a=glob.glob(\"source_documents/*.pdf\")\n",
    "print(a)\n",
    "print(len(a))\n",
    "\n",
    "for i in range(len(a)):\n",
    "\n",
    "    documents.extend(PDFMinerLoader(a[i]).load())\n",
    "\n",
    "\n",
    "print(documents)   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1321, which is longer than the specified 1000\n",
      "Created a chunk of size 1375, which is longer than the specified 1000\n",
      "Created a chunk of size 1855, which is longer than the specified 1000\n",
      "Created a chunk of size 2124, which is longer than the specified 1000\n",
      "Created a chunk of size 1528, which is longer than the specified 1000\n",
      "Created a chunk of size 1486, which is longer than the specified 1000\n",
      "Created a chunk of size 1451, which is longer than the specified 1000\n",
      "Created a chunk of size 1553, which is longer than the specified 1000\n",
      "Created a chunk of size 2335, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 2131, which is longer than the specified 1000\n",
      "Created a chunk of size 1037, which is longer than the specified 1000\n",
      "Created a chunk of size 1447, which is longer than the specified 1000\n",
      "Created a chunk of size 1903, which is longer than the specified 1000\n",
      "Created a chunk of size 1205, which is longer than the specified 1000\n",
      "Created a chunk of size 1580, which is longer than the specified 1000\n",
      "Created a chunk of size 1125, which is longer than the specified 1000\n",
      "Created a chunk of size 1151, which is longer than the specified 1000\n",
      "Created a chunk of size 1181, which is longer than the specified 1000\n",
      "Created a chunk of size 1055, which is longer than the specified 1000\n",
      "Created a chunk of size 1555, which is longer than the specified 1000\n",
      "Created a chunk of size 1145, which is longer than the specified 1000\n",
      "Created a chunk of size 1129, which is longer than the specified 1000\n",
      "Created a chunk of size 1041, which is longer than the specified 1000\n",
      "Created a chunk of size 1364, which is longer than the specified 1000\n",
      "Created a chunk of size 1549, which is longer than the specified 1000\n",
      "Created a chunk of size 1500, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1273, which is longer than the specified 1000\n",
      "Created a chunk of size 1360, which is longer than the specified 1000\n",
      "Created a chunk of size 1091, which is longer than the specified 1000\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#model_name = \"sentence-transformers/LaBSE\"\n",
    "#model_name= 'intfloat/e5-large-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "db = Chroma.from_documents(texts, hf)\n",
    "model_id =\"google/flan-t5-base\"\n",
    "#llm =  HuggingFacePipeline.from_model_id(model_id=\"google/flan-t5-base\", task=\"text2text-generation\", model_kwargs={\"temperature\":1e-1, \"max_length\" : 256})\n",
    "#llm =  HuggingFacePipeline.from_model_id(model_id=model_id, task=\"question-answering\", model_kwargs={\"temperature\":1e-1, \"max_length\" : 512}) \n",
    "retriever = db.as_retriever(search_type='similarity', search_kwargs={\"k\": 20} )\n",
    "#callbacks = []  \n",
    "#qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "\n",
    "#llm =  HuggingFacePipeline.from_model_id(model_id=model_id, task=\"question-answering\", model_kwargs={\"temperature\":1e-1, \"max_length\" : 512}) \n",
    "llm =  HuggingFacePipeline.from_model_id(model_id=model_id, task=\"summarization\", model_kwargs={\"temperature\":1e-1, \"max_length\" : 512}) \n",
    "retriever = db.as_retriever(search_type='similarity', search_kwargs={\"k\": 20} )\n",
    "callbacks = []  #if args.mute_stream else [StreamingStdOutCallbackHandler()]\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=QsYGlZkevEg\", add_video_info=True)\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! pip install youtube-transcript-api\n",
    "! pip install einops\n",
    "\n",
    "#! pip3 install youtube-transcript-api\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_112366/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2061646928.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_112366/2061646928.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'db'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_112366/\u001b[0m\u001b[1;33m2061646928.py\u001b[0m:\u001b[94m9\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_112366/2061646928.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'db'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#llm =  HuggingFacePipeline.from_model_id(model_id=\"google/flan-t5-base\", task=\"text2text-generation\", model_kwargs={\"temperature\":1e-1, \"max_length\" : 512})\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "model_id =\"google/flan-t5-base\"\n",
    "#llm =  HuggingFacePipeline.from_model_id(model_id=model_id, task=\"question-answering\", model_kwargs={\"temperature\":1e-1, \"max_length\" : 512}) \n",
    "llm =  HuggingFacePipeline.from_model_id(model_id=model_id, task=\"summarization\", model_kwargs={\"temperature\":1e-1, \"max_length\" : 512}) \n",
    "retriever = db.as_retriever(search_type='similarity', search_kwargs={\"k\": 20} )\n",
    "callbacks = []  #if args.mute_stream else [StreamingStdOutCallbackHandler()]\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1321, which is longer than the specified 1000\n",
      "Created a chunk of size 1375, which is longer than the specified 1000\n",
      "Created a chunk of size 1855, which is longer than the specified 1000\n",
      "Created a chunk of size 2124, which is longer than the specified 1000\n",
      "Created a chunk of size 1528, which is longer than the specified 1000\n",
      "Created a chunk of size 1486, which is longer than the specified 1000\n",
      "Created a chunk of size 1451, which is longer than the specified 1000\n",
      "Created a chunk of size 1553, which is longer than the specified 1000\n",
      "Created a chunk of size 2335, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 2131, which is longer than the specified 1000\n",
      "Created a chunk of size 1037, which is longer than the specified 1000\n",
      "Created a chunk of size 1447, which is longer than the specified 1000\n",
      "Created a chunk of size 1903, which is longer than the specified 1000\n",
      "Created a chunk of size 1205, which is longer than the specified 1000\n",
      "Created a chunk of size 1580, which is longer than the specified 1000\n",
      "Created a chunk of size 1125, which is longer than the specified 1000\n",
      "Created a chunk of size 1151, which is longer than the specified 1000\n",
      "Created a chunk of size 1181, which is longer than the specified 1000\n",
      "Created a chunk of size 1055, which is longer than the specified 1000\n",
      "Created a chunk of size 1555, which is longer than the specified 1000\n",
      "Created a chunk of size 1145, which is longer than the specified 1000\n",
      "Created a chunk of size 1129, which is longer than the specified 1000\n",
      "Created a chunk of size 1041, which is longer than the specified 1000\n",
      "Created a chunk of size 1364, which is longer than the specified 1000\n",
      "Created a chunk of size 1549, which is longer than the specified 1000\n",
      "Created a chunk of size 1500, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1273, which is longer than the specified 1000\n",
      "Created a chunk of size 1360, which is longer than the specified 1000\n",
      "Created a chunk of size 1091, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-mpnet-base-v2' cache_folder=None model_kwargs={} encode_kwargs={}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2226760454.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2226760454.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/langchain/vectorstores/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">chroma.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">422</span> in           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_documents</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">419 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">420 │   │   </span>texts = [doc.page_content <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> doc <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> documents]                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">421 │   │   </span>metadatas = [doc.metadata <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> doc <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> documents]                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>422 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.from_texts(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">423 │   │   │   </span>texts=texts,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">424 │   │   │   </span>embedding=embedding,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">425 │   │   │   </span>metadatas=metadatas,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/langchain/vectorstores/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">chroma.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">383</span> in           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_texts</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">380 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Returns:</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">381 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Chroma: Chroma vectorstore.</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">382 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>383 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>chroma_collection = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">384 │   │   │   </span>collection_name=collection_name,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">385 │   │   │   </span>embedding_function=embedding,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">386 │   │   │   </span>persist_directory=persist_directory,                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/langchain/vectorstores/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">chroma.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">95</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 92 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._persist_directory = persist_directory                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._collection = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._client.get_or_create_collection(                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 │   │   │   </span>name=collection_name,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 95 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>embedding_function=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._embedding_function.embed_documents                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 96 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._embedding_function <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 │   │   │   </span>metadata=collection_metadata,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1614</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getattr__</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1611 │   │   │   </span>modules = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__dict__</span>[<span style=\"color: #808000; text-decoration-color: #808000\">'_modules'</span>]                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1612 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> name <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> modules:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1613 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> modules[name]                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1614 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">AttributeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"'{}' object has no attribute '{}'\"</span>.format(                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1615 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>).<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>, name))                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1616 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1617 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__setattr__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, name: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, value: Union[Tensor, <span style=\"color: #808000; text-decoration-color: #808000\">'Module'</span>]) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'SentenceTransformer'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'embed_documents'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m2226760454.py\u001b[0m:\u001b[94m8\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2226760454.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/langchain/vectorstores/\u001b[0m\u001b[1;33mchroma.py\u001b[0m:\u001b[94m422\u001b[0m in           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mfrom_documents\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m419 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m420 \u001b[0m\u001b[2m│   │   \u001b[0mtexts = [doc.page_content \u001b[94mfor\u001b[0m doc \u001b[95min\u001b[0m documents]                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m421 \u001b[0m\u001b[2m│   │   \u001b[0mmetadatas = [doc.metadata \u001b[94mfor\u001b[0m doc \u001b[95min\u001b[0m documents]                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m422 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mcls\u001b[0m.from_texts(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m423 \u001b[0m\u001b[2m│   │   │   \u001b[0mtexts=texts,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m424 \u001b[0m\u001b[2m│   │   │   \u001b[0membedding=embedding,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m425 \u001b[0m\u001b[2m│   │   │   \u001b[0mmetadatas=metadatas,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/langchain/vectorstores/\u001b[0m\u001b[1;33mchroma.py\u001b[0m:\u001b[94m383\u001b[0m in           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mfrom_texts\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m380 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mReturns:\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m381 \u001b[0m\u001b[2;33m│   │   │   \u001b[0m\u001b[33mChroma: Chroma vectorstore.\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m382 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m383 \u001b[2m│   │   \u001b[0mchroma_collection = \u001b[96mcls\u001b[0m(                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m384 \u001b[0m\u001b[2m│   │   │   \u001b[0mcollection_name=collection_name,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m385 \u001b[0m\u001b[2m│   │   │   \u001b[0membedding_function=embedding,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m386 \u001b[0m\u001b[2m│   │   │   \u001b[0mpersist_directory=persist_directory,                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/langchain/vectorstores/\u001b[0m\u001b[1;33mchroma.py\u001b[0m:\u001b[94m95\u001b[0m in \u001b[92m__init__\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._persist_directory = persist_directory                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._collection = \u001b[96mself\u001b[0m._client.get_or_create_collection(                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   │   \u001b[0mname=collection_name,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 95 \u001b[2m│   │   │   \u001b[0membedding_function=\u001b[96mself\u001b[0m._embedding_function.embed_documents                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._embedding_function \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m \u001b[94mNone\u001b[0m,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   │   │   \u001b[0mmetadata=collection_metadata,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1614\u001b[0m in \u001b[92m__getattr__\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1611 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodules = \u001b[96mself\u001b[0m.\u001b[91m__dict__\u001b[0m[\u001b[33m'\u001b[0m\u001b[33m_modules\u001b[0m\u001b[33m'\u001b[0m]                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1612 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m name \u001b[95min\u001b[0m modules:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1613 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m modules[name]                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1614 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mAttributeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m object has no attribute \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\"\u001b[0m.format(                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1615 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mtype\u001b[0m(\u001b[96mself\u001b[0m).\u001b[91m__name__\u001b[0m, name))                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1616 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1617 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__setattr__\u001b[0m(\u001b[96mself\u001b[0m, name: \u001b[96mstr\u001b[0m, value: Union[Tensor, \u001b[33m'\u001b[0m\u001b[33mModule\u001b[0m\u001b[33m'\u001b[0m]) -> \u001b[94mNone\u001b[0m:             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'SentenceTransformer'\u001b[0m object has no attribute \u001b[32m'embed_documents'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings() #model_name=embeddings_model_name)\n",
    "print(embeddings)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embeddings = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "embeddings = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "print(embeddings)\n",
    "\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings \n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever(search_type='similarity', search_kwargs={\"k\": 20} )\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "llm =  HuggingFacePipeline.from_model_id(model_id=\"google/flan-t5-base\", task=\"text2text-generation\", model_kwargs={\"temperature\":1e-1, \"max_length\" : 256})\n",
    "\n",
    "callbacks = []  #if args.mute_stream else [StreamingStdOutCallbackHandler()]\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3751 > 512). Running this sequence through the model will result in indexing errors\n",
      "/home/user/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Who is Anupam Purwar', 'result': 'Bhagwan Chowdhry is a Professor of Finance at the Indian School of Business and Research Professor at UCLA Anderson where he has held an appointment since 1988', 'source_documents': [Document(page_content='Author\\n\\nAnupam Purwar\\n\\nResearch Scientist', metadata={'source': 'source_documents/Anupam Purwar - Amazon Science.html'}), Document(page_content='Anupam Purwar\\n\\nadmin', metadata={'source': 'source_documents/Anupam Purwar - NSF PG Scholarship Programme.html'}), Document(page_content='Anupam is currently working as a Research Scientist with Amazon developing tech products for Amazon’s global network. He specializes in solving problems related to Natural Language Processing and Optimization. In his previous role at Amazon, he was credited with developing Railways as the third mode of transport which fetched him two awards from Global VP. Prior to this, Anupam worked as a Research Scientist at Indian Institute of Science (IISc). At IISc, he was part of multi-institutional effort which included IITs and ISRO to develop ultra-high-temperature materials and novel structures, which resulted in 20+ peer reviewed publications with Anupam as the lead author. He graduated among top 10% of the class from Indian School of Business and BITS Pilani. He is passionate about contributing towards education of kids from underprivileged backgrounds. He is an environment conservationist and takes pride in cycling every day for work as well as personal commute.', metadata={'source': 'source_documents/anupam.txt'}), Document(page_content='admin\\n\\nAnupam is currently working as a Research Scientist with Amazon \\ndeveloping tech products for Amazon’s global network. He specializes in \\nsolving problems related to Natural Language Processing and \\nOptimization. In his previous role at Amazon, he was credited with \\ndeveloping Railways as the third mode of transport which fetched him two\\n awards from Global VP. Prior to this, Anupam worked as a Research \\nScientist at Indian Institute of Science (IISc). At IISc, he was part of\\n multi-institutional effort which included IITs and ISRO to develop \\nultra-high-temperature materials and novel structures, which resulted in\\n 20+ peer reviewed publications with Anupam as the lead author. He \\ngraduated among top 10% of the class from Indian School of Business and \\nBITS Pilani. He is passionate about contributing towards education of \\nkids from underprivileged backgrounds. He is an environment \\nconservationist and takes pride in cycling every day for work as well as\\n personal commute.', metadata={'source': 'source_documents/Anupam Purwar - NSF PG Scholarship Programme.html'}), Document(page_content='Anupam Purwar is author with 150+ citations | i10 index=5 | h index 7. Wide industrial experience of aerospace, defence, materials, biomedical, project and operations management. At ISB, I am investigating interactions between operational risks and financial risks to drive the valuation of technology backed firms.', metadata={'source': 'source_documents/anupam.txt'}), Document(page_content=\"The Automated Planning Tool\\n (APT) is a powerful and efficient method for solving \\nwork-order-allocation problems. Its ability to handle nonlinear \\nobjectives and its use of a branch-and-bound method make it a valuable \\ntool for scheduling and resource allocation.\\n\\nResearch areas\\n\\nOperations research and optimization\\n\\nTags\\n\\nMultivariate optimization\\n\\nConference\\n\\nEURO 2022\\n\\nRelated publications\\n\\nAutomated planning tool (APT): A mixed integer non linear programming problem solver for workorder scheduling\\n\\nAbout the Author\\n\\nAnupam Purwar\\n\\nAnupam Purwar is a research scientist in Amazon's \\nGlobal Reliability and Maintenance Engineering organization.\", metadata={'source': 'source_documents/Automated Planning Tool makes work order allocation more efficient - Amazon Science.html'}), Document(page_content='Bhagwan Chowdhry is a Professor of Finance at the Indian School of Business and Research Professor at UCLA Anderson where he has held an appointment since 1988. He is the Executive Director, Digital Identity Research Initiative (DIRI), and Faculty Director, I-Venture @ ISB. \\n\\nProfessor Chowdhry has also taught at the University of Chicago, University of Illinois at Chicago, and the Hong Kong University of Science and Technology. He received his Ph.D. from the University of Chicago Booth School of Business . He has an M.B.A. in Finance from the University of Iowa and a B.Tech. in Mechanical Engineering from Indian Institute of Technology, Kanpur.', metadata={'source': 'source_documents/bhagwan.txt'}), Document(page_content='#gsc_rsb-DpkqTwcAAAAJ-img{width:32px;height:30px;}.gs_el_ta #gsc_rsb-DpkqTwcAAAAJ-img,.gs_el_ph #gsc_rsb-DpkqTwcAAAAJ-img{width:56px;height:53px;}Swaroop Anand HangalIndian Institute of Technology BombayVerified email at iitbombay.org\\n\\n#gsc_rsb-VI8PjkEAAAAJ-img{width:32px;height:32px;}.gs_el_ta #gsc_rsb-VI8PjkEAAAAJ-img,.gs_el_ph #gsc_rsb-VI8PjkEAAAAJ-img{width:56px;height:56px;}venkateswaran ThiruvenkatamScientist, Vikram Sarabhai Space Centre/ISRO\\n\\n#gsc_rsb-hr2IZacAAAAJ-img{width:27px;height:32px;}.gs_el_ta #gsc_rsb-hr2IZacAAAAJ-img,.gs_el_ph #gsc_rsb-hr2IZacAAAAJ-img{width:48px;height:56px;}Ariharan S.Scientist/Engineer-SD, Vikram Sarabhai Space Centre, ISRO, Thiruvananthapuram, India\\n\\n#gsc_rsb-k2QxS5UAAAAJ-img{width:31px;height:32px;}.gs_el_ta #gsc_rsb-k2QxS5UAAAAJ-img,.gs_el_ph #gsc_rsb-k2QxS5UAAAAJ-img{width:54px;height:56px;}Vinod ChaubeyEEE Department, BITS PilaniVerified email at pilani.bits-pilani.ac.in', metadata={'source': 'source_documents/_Anupam Purwar_ - _Google Scholar_.html'}), Document(page_content='His research interests, on which he has published several papers in finance and economics journals, are in International Finance, Corporate Finance, Impact Investing, and FinTech. He has also organized and taught Executive Education programs in Los Angeles, Singapore, Hong Kong, Mumbai, and Hyderabad. Professor Chowdhry is an Advisor to several FinTech start-ups. \\n\\nProfessor Chowdhry has proposed Financial Access at Birth (FAB) initiative in which every child born in the world is given an initial deposit of $100 in an online bank account to guarantee that everyone in the world will have access to financial services in a few decades.', metadata={'source': 'source_documents/bhagwan.txt'}), Document(page_content='2017\\n\\nAutomated Planning Tool (APT): A mixed integer non-linear programming problem solver for Workorder schedulingP AnupamAmazon Science, 2022\\n\\n2022\\n\\nEffect of Different Root Canal Filling Materials in Endo-perio Lesions: Design and Computational AnalysisP Anupam, PragyaDesign for Tomorrow, Springer, Print ISBN: 978-981-16-0083-8 3, 39-49, 2021\\n\\n2021\\n\\nA multipurpose sensor for heat flux and temperature measurement: Design and Computational AnalysisA Purwar14th IEEE India Council International Conference (INDICON), 2017\\n\\n2017\\n\\n30th International Symposium on Shock Waves 1: ISSW30-Volume 1G Ben-Dor, O Sadot, O IgraSpringer, 2017\\n\\n2017\\n\\nHuman Detection Based on Explicit Skin Colour Space Thresholding and Minor MotionD Joshi, A Purwar, MS DasguptaIEEE INDICOM, 2014\\n\\n2014\\n\\nARIMAX model for forecasting maintenance work (AMFM): A multi-stage seasonal ARIMAX model for work order time series forecastingP Anupam, M ReimherrConference on Digital Economy, 2023\\n\\n2023', metadata={'source': 'source_documents/_Anupam Purwar_ - _Google Scholar_.html'}), Document(page_content='Automated Planning Tool (APT): A mixed integer non-linear programming\\nproblem solver for Workorder scheduling\\n\\nAnupam Purwar (RME Science, Amazon) * 1', metadata={'source': 'source_documents/automated-planning-tool-apt-a-mised-interger-non-linear-programming-problem-solver-for-workorder-scheduling.pdf'}), Document(page_content='#gsc_rsb-gbBQ4x0AAAAJ-img{width:31px;height:32px;}.gs_el_ta #gsc_rsb-gbBQ4x0AAAAJ-img,.gs_el_ph #gsc_rsb-gbBQ4x0AAAAJ-img{width:54px;height:56px;}Sneh DeepPh D, Indian Institute of ScienceVerified email at iisc.ac.in\\n\\n#gsc_rsb-VvD5STUAAAAJ-img{width:32px;height:21px;}.gs_el_ta #gsc_rsb-VvD5STUAAAAJ-img,.gs_el_ph #gsc_rsb-VvD5STUAAAAJ-img{width:56px;height:37px;}Amaresh ChakrabartiProfessor Indian Institute Science Bangalore IndiaVerified email at iisc.ac.in\\n\\n#gsc_rsb-yqlb20wAAAAJ-img{width:32px;height:24px;}.gs_el_ta #gsc_rsb-yqlb20wAAAAJ-img,.gs_el_ph #gsc_rsb-yqlb20wAAAAJ-img{width:56px;height:42px;}Oren SadotBen Gurion University of the NegevVerified email at bgu.ac.il\\n\\n#gsc_rsb-ZAcWZYEAAAAJ-img{width:32px;height:32px;}.gs_el_ta #gsc_rsb-ZAcWZYEAAAAJ-img,.gs_el_ph #gsc_rsb-ZAcWZYEAAAAJ-img{width:56px;height:56px;}Ben-Dorben gurion universityVerified email at bgu.ac.il', metadata={'source': 'source_documents/_Anupam Purwar_ - _Google Scholar_.html'}), Document(page_content='For news and information about our books and courses, visit http://oreilly.com.\\n\\nFind us on Facebook: http://facebook.com/oreilly\\n\\nFollow us on Twitter: http://twitter.com/oreillymedia\\n\\nWatch us on YouTube: http://youtube.com/oreillymedia\\n\\nAcknowledgments\\nWe want to thank all those who helped to make this book a reality. Special thanks to\\nJeff Bleiel for honest, insightful feedback, and for guiding us through the entire pro‐\\ncess.  We  are  incredibly  grateful  to  Juan  Manuel  Contreras,  Chakri  Cherukuri,  and\\nGregory  Bronner,  who  took  time  out  of  their  busy  lives  to  review  our  book  in  so\\nmuch detail. The book benefited from their valuable feedback and suggestions. Many\\nthanks as well to O’Reilly’s fantastic staff, in particular Michelle Smith for believing in\\nthis project and helping us define its scope.', metadata={'source': 'source_documents/hom.pdf'}), Document(page_content='Natural  language  processing  (NLP)  is  a  subfield  of  artificial  intelligence  used  to  aid\\ncomputers in understanding natural human language. Most NLP techniques rely on\\nmachine  learning  to  derive  meaning  from  human  languages.  When  text  has  been\\nprovided, the computer utilizes algorithms to extract meaning associated with every\\nsentence and collect essential data from them. NLP manifests itself in different forms\\nacross many disciplines under various aliases, including (but not limited to) textual\\nanalysis, text mining, computational linguistics, and content analysis.', metadata={'source': 'source_documents/hom.pdf'}), Document(page_content='In natural language processing, a common technique for extracting features from text\\nis to place all words that occur in the text in a bucket. This approach is called a bag of\\nwords  model.  It’s  referred  to  as  a  bag  of  words  because  any  information  about  the\\nstructure  of  the  sentence  is  lost.  In  this  technique,  we  build  a  single  matrix  from  a\\ncollection of texts, as shown in Figure 10-4, in which each row represents a token and\\neach  column  represents  a  document  or  sentence  in  our  corpus.  The  values  of  the\\nmatrix represent the count of the number of instances of the token appearing.\\n\\n1 A customized deep learning–based feature representation model is built in case study 1 of this chapter.\\n\\n356 \\n\\n| \\n\\nChapter 10: Natural Language Processing\\n\\n\\x0cFigure 10-4. Bag of words', metadata={'source': 'source_documents/hom.pdf'}), Document(page_content='This book is here to help you get your job done. In general, if example code is offered,\\nyou may use it in your programs and documentation. You do not need to contact us\\nfor permission unless you’re reproducing a significant portion of the code. For exam‐\\nple,  writing  a  program  that  uses  several  chunks  of  code  from  this  book  does  not\\n\\nPreface \\n\\n| \\n\\nxiii\\n\\n\\x0crequire permission. Selling or distributing examples from O’Reilly books does require\\npermission. Answering a question by citing this book and quoting example code does\\nnot  require  permission.  Incorporating  a  significant  amount  of  example  code  from\\nthis book into your product’s documentation does require permission.', metadata={'source': 'source_documents/hom.pdf'}), Document(page_content=\"1.5. PoS tagging\\n\\nPart-of-speech (PoS) tagging is the process of assigning a token to its grammatical cat‐\\negory  (e.g.,  verb,  noun,  etc.)  in  order  to  understand  its  role  within  a  sentence.  PoS\\ntags  have  been  used  for  a  variety  of  NLP  tasks  and  are  extremely  useful  since  they\\nprovide a linguistic signal of how a word is being used within the scope of a phrase,\\nsentence, or document.\\n\\nAfter  a  sentence  is  split  into  tokens,  a  tagger,  or  PoS  tagger,  is  used  to  assign  each\\ntoken to a part-of-speech category. Historically, hidden Markov models (HMM) were\\nused to create such taggers. More recently, artificial neural networks have been lever‐\\naged. The code for PoS tagging using the TextBlob library is shown here:\\n\\ntext = 'Google is looking at buying U.K. startup for $1 billion'\\nTextBlob(text).tags\\n\\nOutput\", metadata={'source': 'source_documents/hom.pdf'}), Document(page_content=\"Author details\\n\\n< Back\\n\\nAnupam Purwar\\n\\nPublications\\n\\n Citations \\n\\n19\\n\\nPublications by Year\\n\\n2014\\n\\n2017\\n\\nCo-Authors:\\n\\nVinod Kumar ChaubeyMani Sankar DasguptaSneh DeepDivya Joshi Show All Co-Authors (4)\\n\\nAnupam Purwar\\n\\nAffiliation\\n\\nNational Laboratory of Radar Signal Processing\\n\\nXidian University\\n\\nXi’an, China\\n\\nPublication Topics\\n\\nceramics,\\n\\nfinite element analysis,\\n\\ngas turbines,\\n\\nspace vehicles,\\n\\ntemperature measurement,\\n\\ntemperature sensors,\\n\\nthermocouples,\\n\\nzirconium compounds,\\n\\nAWGN,\\n\\nGlobal Positioning System,\\n\\nadaptive control,\\n\\ncondition monitoring,\\n\\ncontrol system synthesis,\\n\\nelectric resistance,\\n\\nenergy conservation,\\n\\nerror statistics,\\n\\njamming,\\n\\nlamps,\\n\\nlighting control,\\n\\nneurocontrollers,\\n\\npotentiometers,\\n\\nstepping motors,\\n\\nthyristor circuits,\\n\\ntime-varying systems,\\n\\ntriodes\\n\\nShow More\\n\\nPublications\\n\\nCitations\\n\\n19\\n\\nPublications by Year\\n\\n2014\\n\\n2017\\n\\nCo-Authors:\\n\\nVinod Kumar ChaubeyMani Sankar DasguptaSneh DeepDivya JoshiShow All Co-Authors (4)\\n\\nThis Author's Publications\", metadata={'source': 'source_documents/Anupam Purwar - IEEE Xplore Author Profile.html'}), Document(page_content='15 min read·May 17\\n\\n8\\n\\n-', metadata={'source': 'source_documents/Free Open Source Alternative to ChatGPT — GPT4All by Wei-Meng Lee May, 2023 Level Up Coding.html'}), Document(page_content='Export\\n\\nSearch History\\n\\nShowing 1-5 of 5 results\\n\\nSortNewestSortRelevance Newest Oldest Most Cited By Papers Most Cited By Patents Most Popular Publication Title A-Z Publication Title Z-A\\n\\nFilter Results\\n\\nShow\\n\\nAll Results\\n\\nOpen Access Only\\n\\nYearSingle YearRange2014201720142017From ToApply\\n\\nAuthorAnupam Purwar(5)Divya Joshi(3)Mani Sankar Dasgupta(2)Sneh Deep(1)Vinod Kumar Chaubey(1)Apply\\n\\nAffiliationDepartment of Electrical & Electronics Engineering, Birla Institute of Technology and Science, Pilani, India(2)Department of Aerospace Engineering, Indian Institute of Science, Bangalore, India(2)Department of Mechanical Engineering, Birla Institute of Technology and Science, Pilani, India(2)National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China(1)Aerospace Department, Indian Institute of Science, Bangalore, India(1)Show More…Apply', metadata={'source': 'source_documents/Anupam Purwar - IEEE Xplore Author Profile.html'})]}\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"samsum\"\n",
    "print(qa({'query': 'Who is Anupam Purwar'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5758]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install -U sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentences = [\"I'm very happy\", \"I'm full of happiness\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embedding_1= model.encode(sentences[0], convert_to_tensor=True)\n",
    "embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n",
    "\n",
    "util.pytorch_cos_sim(embedding_1, embedding_2)\n",
    "## tensor([[0.6003]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2670 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised learning is an area of machine learning where the chosen algorithm tries to fit a target using the given input. A set of training data that contains labels is sup plied to the algorithm. Based on a massive set of data, the algorithm will learn a rule that it uses to predict the labels for new observations.\n",
      "ANN is a primary algorithm used across all types of machine learning.\n",
      "Recurrent neural networks (RNNs) are called “recurrent” because they perform the same task for every element of a sequence, with the output being dependent on the previous computations. RNNs have a memory, which captures information about what has been calculated so far.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(qa({'query': 'Who is Anupam Purwar'}))\n",
    "\n",
    "#print(qa({'query': 'What is APT'})['result'])\n",
    "\n",
    "print(qa({'query': 'What is Supervised learning'})['result'])\n",
    "print(qa({'query': 'What is ANN'})['result'])\n",
    "print(qa({'query': 'What is RNN'})['result'])\n",
    "#print(qa({'query': 'Who is Anupam Purwar'})['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised learning is an area of machine learning where the chosen algorithm tries to fit a target using the given input. A set of training data that contains labels is sup plied to the algorithm. Based on a massive set of data, the algorithm will learn a rule that it uses to predict the labels for new observations.\n",
      "ANN is a primary algorithm used across all types of machine learning.\n",
      "Recurrent neural networks (RNNs) are called “recurrent” because they perform the same task for every element of a sequence, with the output being dependent on the previous computations. RNN models have a memory, which captures information about what has been calculated so far. As shown in Figure 5-4, a recurrent neural net work can be thought of as multiple copies of the same network, each passing a mes sage to a successor.\n"
     ]
    }
   ],
   "source": [
    "#Token indices sequence length is longer than the specified maximum sequence length for this model (2670 > 512). Running this sequence through the model will result in indexing errors\n",
    "print(qa({'query': 'What is Supervised learning'})['result'])\n",
    "print(qa({'query': 'What is ANN'})['result'])\n",
    "print(qa({'query': 'What is RNN'})['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of artificial intelligence used to aid computers in understanding natural human language.\n",
      "Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing Natural Language Processing\n"
     ]
    }
   ],
   "source": [
    "#print(qa({'query': 'Who is Bhagwan'}))\n",
    "\n",
    "#print(qa({'query': 'Where is Bhagwan'})['result'])\n",
    "#print(qa({'query': 'Who is Bhagwan'})['source_documents'])\n",
    "\n",
    "print(qa({'query': 'What is NLP'})['result'])\n",
    "\n",
    "print(qa({'query': 'Tell about NLP methods'})['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to reduce the dimensionality of a dataset with a large number of variables, while retaining as much variance in the data as possible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(qa({'query': 'What is main idea of Bhagwan Chowdhry'})['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of artificial intelligence used to aid computers in understanding natural human language. Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
      "Natural Language Processing (NLP) is a subfield of artificial intelligence concerned with programming computers to process textual data in order to gain useful insights. Natural Language Processing (NLP) manifests itself in different forms across many disciplines under various aliases, including (but not limited to) textual analysis, text mining, computational linguistics, and content analysis. Natural Language Processing (NLP) manifests itself in different forms across many disciplines under various aliases, including (but not limited to) textual analysis, text mining, computational linguistics, and content analysis. Natural Language Processing (NLP) is a subfield of artificial intelligence concerned with programming computers to process textual data in order to gain useful insights. Natural Language Processing (NLP) manifests itself in different forms across many disciplines under various aliases, including (but not limited to) textual analysis, text mining, computational linguistics, and content analysis. Natural Language Processing (NLP) is a branch of artificial intelligence concerned with programming computers to process textual data in order to gain useful insights. Natural Language Processing (NLP) manifests itself in different forms across many disciplines under various aliases, including (but not limited to\n"
     ]
    }
   ],
   "source": [
    "print(qa({'query': 'Tell about NLP'})['result'])\n",
    "print(qa({'query': 'Tell about NLP techniques'})['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa({'query': 'Tell about NLP'})['result'])\n",
    "print(qa({'query': 'Tell about NLP techniques'})['result'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the `samsum` dataset, we use the `load_dataset()` method from the 🤗 Datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 3.36k/3.36k [00:00<00:00, 25.9MB/s]\n",
      "Downloading metadata: 100%|██████████| 1.58k/1.58k [00:00<00:00, 14.7MB/s]\n",
      "Downloading readme: 100%|██████████| 7.04k/7.04k [00:00<00:00, 36.5MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4275189504.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/4275189504.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">load.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1773</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_dataset</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1770 │   </span>)                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1771 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1772 │   # Create a dataset builder</span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1773 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>builder_instance = load_dataset_builder(                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1774 │   │   </span>path=path,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1775 │   │   </span>name=name,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1776 │   │   </span>data_dir=data_dir,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">load.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1502</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_dataset_builder</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_auth_token <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   </span>download_config = download_config.copy() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> download_config <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> DownloadConfig(  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1501 │   │   </span>download_config.use_auth_token = use_auth_token                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1502 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>dataset_module = dataset_module_factory(                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>path,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>revision=revision,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1505 │   │   </span>download_config=download_config,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">load.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1219</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">dataset_module_factory</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1216 │   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Couldn't find a dataset script at {</span>relative_to_absolute_path(c  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1217 │   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Couldn't find '{</span>path<span style=\"color: #808000; text-decoration-color: #808000\">}' on the Hugging Face Hub either: {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(e  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1218 │   │   │   │   │   </span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1219 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> e1 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1220 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1221 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">FileNotFoundError</span>(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1222 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Couldn't find a dataset script at {</span>relative_to_absolute_path(combined_path  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">load.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">dataset_module_factory</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   │   │   │   </span>download_config=download_config,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   │   │   │   </span>download_mode=download_mode,                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   │   </span>dynamic_modules_path=dynamic_modules_path,                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>).get_module()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> HubDatasetModuleFactoryWithoutScript(                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   │   │   │   </span>path,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">load.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">909</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_module</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 906 │   │   </span>dataset_infos_path = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.download_dataset_infos_file()                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 907 │   │   </span>dataset_readme_path = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.download_dataset_readme_file()                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 908 │   │   </span>imports = get_imports(local_path)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 909 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>local_imports = _download_additional_modules(                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 910 │   │   │   </span>name=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.name,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 911 │   │   │   </span>base_path=hf_hub_url(repo_id=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.name, path=<span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>, revision=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.revision),     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 912 │   │   │   </span>imports=imports,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/datasets/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">load.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">222</span> in                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_download_additional_modules</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 219 │   │   </span>_them_str = <span style=\"color: #808000; text-decoration-color: #808000\">\"them\"</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(needs_to_be_installed) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"it\"</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 220 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"sklearn\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> needs_to_be_installed.keys():                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 221 │   │   │   </span>needs_to_be_installed[<span style=\"color: #808000; text-decoration-color: #808000\">\"sklearn\"</span>] = <span style=\"color: #808000; text-decoration-color: #808000\">\"scikit-learn\"</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 222 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ImportError</span>(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 223 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"To be able to use {</span>name<span style=\"color: #808000; text-decoration-color: #808000\">}, you need to install the following {</span>_dependencies  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 224 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{', '</span>.join(needs_to_be_installed)<span style=\"color: #808000; text-decoration-color: #808000\">}.\\nPlease install {</span>_them_str<span style=\"color: #808000; text-decoration-color: #808000\">} using 'pip</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 225 │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{' '</span>.join(needs_to_be_installed.values())<span style=\"color: #808000; text-decoration-color: #808000\">}' for instance.\"</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ImportError: </span>To be able to use samsum, you need to install the following dependency: py7zr.\n",
       "Please install it using <span style=\"color: #008000; text-decoration-color: #008000\">'pip install py7zr'</span> for instance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m4275189504.py\u001b[0m:\u001b[94m4\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/4275189504.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/datasets/\u001b[0m\u001b[1;33mload.py\u001b[0m:\u001b[94m1773\u001b[0m in \u001b[92mload_dataset\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1770 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1771 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1772 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Create a dataset builder\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1773 \u001b[2m│   \u001b[0mbuilder_instance = load_dataset_builder(                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1774 \u001b[0m\u001b[2m│   │   \u001b[0mpath=path,                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1775 \u001b[0m\u001b[2m│   │   \u001b[0mname=name,                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1776 \u001b[0m\u001b[2m│   │   \u001b[0mdata_dir=data_dir,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/datasets/\u001b[0m\u001b[1;33mload.py\u001b[0m:\u001b[94m1502\u001b[0m in \u001b[92mload_dataset_builder\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m use_auth_token \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   \u001b[0mdownload_config = download_config.copy() \u001b[94mif\u001b[0m download_config \u001b[94melse\u001b[0m DownloadConfig(  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1501 \u001b[0m\u001b[2m│   │   \u001b[0mdownload_config.use_auth_token = use_auth_token                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1502 \u001b[2m│   \u001b[0mdataset_module = dataset_module_factory(                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mpath,                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mrevision=revision,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1505 \u001b[0m\u001b[2m│   │   \u001b[0mdownload_config=download_config,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/datasets/\u001b[0m\u001b[1;33mload.py\u001b[0m:\u001b[94m1219\u001b[0m in \u001b[92mdataset_module_factory\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1216 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt find a dataset script at \u001b[0m\u001b[33m{\u001b[0mrelative_to_absolute_path(c  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1217 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt find \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0mpath\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m on the Hugging Face Hub either: \u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(e  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1218 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m) \u001b[94mfrom\u001b[0m \u001b[94mNone\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1219 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m e1 \u001b[94mfrom\u001b[0m \u001b[94mNone\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1220 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1221 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mFileNotFoundError\u001b[0m(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1222 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCouldn\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt find a dataset script at \u001b[0m\u001b[33m{\u001b[0mrelative_to_absolute_path(combined_path  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/datasets/\u001b[0m\u001b[1;33mload.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92mdataset_module_factory\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mdownload_config=download_config,                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mdownload_mode=download_mode,                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mdynamic_modules_path=dynamic_modules_path,                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   │   \u001b[0m).get_module()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m HubDatasetModuleFactoryWithoutScript(                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpath,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/datasets/\u001b[0m\u001b[1;33mload.py\u001b[0m:\u001b[94m909\u001b[0m in \u001b[92mget_module\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 906 \u001b[0m\u001b[2m│   │   \u001b[0mdataset_infos_path = \u001b[96mself\u001b[0m.download_dataset_infos_file()                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 907 \u001b[0m\u001b[2m│   │   \u001b[0mdataset_readme_path = \u001b[96mself\u001b[0m.download_dataset_readme_file()                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 908 \u001b[0m\u001b[2m│   │   \u001b[0mimports = get_imports(local_path)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 909 \u001b[2m│   │   \u001b[0mlocal_imports = _download_additional_modules(                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 910 \u001b[0m\u001b[2m│   │   │   \u001b[0mname=\u001b[96mself\u001b[0m.name,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 911 \u001b[0m\u001b[2m│   │   │   \u001b[0mbase_path=hf_hub_url(repo_id=\u001b[96mself\u001b[0m.name, path=\u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m, revision=\u001b[96mself\u001b[0m.revision),     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 912 \u001b[0m\u001b[2m│   │   │   \u001b[0mimports=imports,                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/datasets/\u001b[0m\u001b[1;33mload.py\u001b[0m:\u001b[94m222\u001b[0m in                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_download_additional_modules\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 219 \u001b[0m\u001b[2m│   │   \u001b[0m_them_str = \u001b[33m\"\u001b[0m\u001b[33mthem\u001b[0m\u001b[33m\"\u001b[0m \u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(needs_to_be_installed) > \u001b[94m1\u001b[0m \u001b[94melse\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mit\u001b[0m\u001b[33m\"\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 220 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33msklearn\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m needs_to_be_installed.keys():                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 221 \u001b[0m\u001b[2m│   │   │   \u001b[0mneeds_to_be_installed[\u001b[33m\"\u001b[0m\u001b[33msklearn\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[33m\"\u001b[0m\u001b[33mscikit-learn\u001b[0m\u001b[33m\"\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 222 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mImportError\u001b[0m(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 223 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mTo be able to use \u001b[0m\u001b[33m{\u001b[0mname\u001b[33m}\u001b[0m\u001b[33m, you need to install the following \u001b[0m\u001b[33m{\u001b[0m_dependencies  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 224 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m'\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'\u001b[0m.join(needs_to_be_installed)\u001b[33m}\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mPlease install \u001b[0m\u001b[33m{\u001b[0m_them_str\u001b[33m}\u001b[0m\u001b[33m using \u001b[0m\u001b[33m'\u001b[0m\u001b[33mpip\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 225 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m'\u001b[0m\u001b[33m \u001b[0m\u001b[33m'\u001b[0m.join(needs_to_be_installed.values())\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m for instance.\u001b[0m\u001b[33m\"\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mImportError: \u001b[0mTo be able to use samsum, you need to install the following dependency: py7zr.\n",
       "Please install it using \u001b[32m'pip install py7zr'\u001b[0m for instance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Train dataset size: 14732\n",
    "# Test dataset size: 819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets checkout an example of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">827305702.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/827305702.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m827305702.py\u001b[0m:\u001b[94m4\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/827305702.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'dataset'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randrange        \n",
    "\n",
    "\n",
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "print(f\"summary: \\n{sample['summary']}\\n---------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need to convert our inputs (text) to token IDs. This is done by a 🤗 Transformers Tokenizer. If you are not sure what this means check out [chapter 6](https://huggingface.co/course/chapter6/1?fw=tf) of the Hugging Face Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we can start training we need to preprocess our data. Abstractive Summarization is a text2text-generation task. This means our model will take a text as input and generate a summary as output. For this we want to understand how long our input and output will be to be able to efficiently batch our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2755960781.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2755960781.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m2755960781.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2755960781.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'dataset'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1363501781.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">21</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/1363501781.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m1363501781.py\u001b[0m:\u001b[94m21\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/1363501781.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'dataset'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune and evaluate FLAN-T5\n",
    "\n",
    "After we have processed our dataset, we can start training our model. Therefore we first need to load our [FLAN-T5](https://huggingface.co/models?search=flan-t5) from the Hugging Face Hub. In the example we are using a instance with a NVIDIA V100 meaning that we will fine-tune the `base` version of the model. \n",
    "_I plan to do a follow-up post on how to fine-tune the `xxl` version of the model using Deepspeed._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate our model during training. The `Trainer` supports evaluation during training by providing a `compute_metrics`.  \n",
    "The most commonly used metrics to evaluate summarization task is [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric)) short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries\n",
    "\n",
    "We are going to use `evaluate` library to evaluate the `rogue` score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3734341054.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/3734341054.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ModuleNotFoundError: </span>No module named <span style=\"color: #008000; text-decoration-color: #008000\">'evaluate'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m3734341054.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/3734341054.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mModuleNotFoundError: \u001b[0mNo module named \u001b[32m'evaluate'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training is to create a `DataCollator` that will take care of padding our inputs and labels. We will use the `DataCollatorForSeq2Seq` from the 🤗 Transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define the hyperparameters (`TrainingArguments`) we want to use for our training. We are leveraging the [Hugging Face Hub](https://huggingface.co/models) integration of the `Trainer` to automatically push our checkpoints, logs and metrics during training into a repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2330578515.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">38</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2330578515.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'tokenized_dataset'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m2330578515.py\u001b[0m:\u001b[94m38\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2330578515.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'tokenized_dataset'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our training by using the `train` method of the `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2336409129.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2336409129.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'trainer'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m2336409129.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2336409129.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'trainer'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![flan-t5-tensorboard](../assets/flan-t5-tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we have trained our model. 🎉 Lets run evaluate the best model again on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2732109216.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2732109216.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'trainer'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m2732109216.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2732109216.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'trainer'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we achieved is an `rouge1` score of `47.23`. \n",
    "\n",
    "Lets save our results and tokenizer to the Hugging Face Hub and create a model card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2738712472.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2738712472.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'trainer'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m2738712472.py\u001b[0m:\u001b[94m3\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/2738712472.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'trainer'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference\n",
    "\n",
    "Now we have a trained model, we can use it to run inference. We will use the `pipeline` API from transformers and a `test` example from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.53k/1.53k [00:00<00:00, 9.29MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 990M/990M [00:48<00:00, 20.6MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 13.7MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:03<00:00, 222kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:01<00:00, 2.00MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 24.0MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_116435/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">186302488.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_116435/186302488.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/transformers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">988</span> in         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pipeline</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">985 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> device <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">986 │   │   </span>kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"device\"</span>] = device                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">987 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>988 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> pipeline_class(model=model, framework=framework, task=task, **kwargs)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">989 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/transformers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">text2text_generation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">65</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 62 │   </span>return_name = <span style=\"color: #808000; text-decoration-color: #808000\">\"generated\"</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 63 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 64 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 65 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(*args, **kwargs)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 66 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 67 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.check_model_type(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 68 │   │   │   </span>TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/transformers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">781</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 778 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.framework = framework                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 779 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 780 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.framework == <span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> device <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(device, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 781 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.to(device)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 782 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 783 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> device <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 784 │   │   │   # `accelerate` device map</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1878</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1875 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" model has already been set to the correct devices and casted to the co</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1876 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1877 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1878 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().to(*args, **kwargs)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1879 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1880 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">half</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args):                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1881 │   │   # Checks if the model has been loaded in 8-bit</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1145</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1142 │   │   │   │   │   │   │   </span>non_blocking, memory_format=convert_to_format)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1143 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> t.to(device, dtype <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> t.is_floating_point() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> t.is_complex() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">No</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1144 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1145 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._apply(convert)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1146 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1147 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_full_backward_pre_hook</span>(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1148 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">797</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_apply</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 794 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 795 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_apply</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, fn):                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 796 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> module <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.children():                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 797 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>module._apply(fn)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 798 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 799 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_should_use_set_data</span>(tensor, tensor_applied):                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 800 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._has_compatible_shallow_copy_type(tensor, tensor_applied):           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">820</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_apply</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 817 │   │   │   # track autograd history of `param_applied`, so we have to use</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 818 │   │   │   # `with torch.no_grad():`</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 819 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 820 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>param_applied = fn(param)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 821 │   │   │   </span>should_use_set_data = compute_should_use_set_data(param, param_applied)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 822 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> should_use_set_data:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 823 │   │   │   │   </span>param.data = param_applied                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1143</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1140 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> convert_to_format <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> t.dim() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>):                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1141 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> t.to(device, dtype <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> t.is_floating_point() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> t.is_complex() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">els</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1142 │   │   │   │   │   │   │   </span>non_blocking, memory_format=convert_to_format)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1143 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> t.to(device, dtype <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> t.is_floating_point() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> t.is_complex() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">No</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1144 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1145 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._apply(convert)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1146 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user/.local/lib/python3.10/site-packages/torch/cuda/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">247</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_lazy_init</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 244 │   │   # are found or any other error occurs</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 245 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">'CUDA_MODULE_LOADING'</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> os.environ:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 246 │   │   │   </span>os.environ[<span style=\"color: #808000; text-decoration-color: #808000\">'CUDA_MODULE_LOADING'</span>] = <span style=\"color: #808000; text-decoration-color: #808000\">'LAZY'</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 247 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch._C._cuda_init()                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 248 │   │   # Some of the queued calls may reentrantly call _lazy_init();</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 249 │   │   # we need to just return without initializing in that case.</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 250 │   │   # However, we must not let any *other* threads in!</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a \n",
       "driver from <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://www.nvidia.com/Download/index.aspx</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_116435/\u001b[0m\u001b[1;33m186302488.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_116435/186302488.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/transformers/pipelines/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m988\u001b[0m in         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mpipeline\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m985 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m device \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m986 \u001b[0m\u001b[2m│   │   \u001b[0mkwargs[\u001b[33m\"\u001b[0m\u001b[33mdevice\u001b[0m\u001b[33m\"\u001b[0m] = device                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m987 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m988 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m pipeline_class(model=model, framework=framework, task=task, **kwargs)           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m989 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/transformers/pipelines/\u001b[0m\u001b[1;33mtext2text_generation.py\u001b[0m:\u001b[94m65\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__init__\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 62 \u001b[0m\u001b[2m│   \u001b[0mreturn_name = \u001b[33m\"\u001b[0m\u001b[33mgenerated\u001b[0m\u001b[33m\"\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 63 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 64 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__init__\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 65 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(*args, **kwargs)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 66 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 67 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.check_model_type(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 68 \u001b[0m\u001b[2m│   │   │   \u001b[0mTF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/transformers/pipelines/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m781\u001b[0m in \u001b[92m__init__\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 778 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.framework = framework                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 779 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 780 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.framework == \u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mand\u001b[0m device \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96misinstance\u001b[0m(device, \u001b[96mint\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 781 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.model.to(device)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 782 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 783 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m device \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 784 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# `accelerate` device map\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m1878\u001b[0m in \u001b[92mto\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1875 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m model has already been set to the correct devices and casted to the co\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1876 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1877 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1878 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().to(*args, **kwargs)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1879 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1880 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mhalf\u001b[0m(\u001b[96mself\u001b[0m, *args):                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1881 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Checks if the model has been loaded in 8-bit\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1145\u001b[0m in \u001b[92mto\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1142 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mnon_blocking, memory_format=convert_to_format)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1143 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m t.is_complex() \u001b[94melse\u001b[0m \u001b[94mNo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1144 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1145 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._apply(convert)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1146 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1147 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_full_backward_pre_hook\u001b[0m(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m797\u001b[0m in \u001b[92m_apply\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 794 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 795 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_apply\u001b[0m(\u001b[96mself\u001b[0m, fn):                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 796 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 797 \u001b[2m│   │   │   \u001b[0mmodule._apply(fn)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 798 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 799 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcompute_should_use_set_data\u001b[0m(tensor, tensor_applied):                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 800 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m820\u001b[0m in \u001b[92m_apply\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# track autograd history of `param_applied`, so we have to use\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 818 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# `with torch.no_grad():`\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 819 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 820 \u001b[2m│   │   │   │   \u001b[0mparam_applied = fn(param)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 821 \u001b[0m\u001b[2m│   │   │   \u001b[0mshould_use_set_data = compute_should_use_set_data(param, param_applied)       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 822 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m should_use_set_data:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 823 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mparam.data = param_applied                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1143\u001b[0m in \u001b[92mconvert\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m convert_to_format \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m t.dim() \u001b[95min\u001b[0m (\u001b[94m4\u001b[0m, \u001b[94m5\u001b[0m):                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m t.is_complex() \u001b[94mels\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1142 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mnon_blocking, memory_format=convert_to_format)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1143 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m t.is_complex() \u001b[94melse\u001b[0m \u001b[94mNo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1144 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1145 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._apply(convert)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1146 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user/.local/lib/python3.10/site-packages/torch/cuda/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m247\u001b[0m in \u001b[92m_lazy_init\u001b[0m          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 244 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# are found or any other error occurs\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 245 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m'\u001b[0m\u001b[33mCUDA_MODULE_LOADING\u001b[0m\u001b[33m'\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m os.environ:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 246 \u001b[0m\u001b[2m│   │   │   \u001b[0mos.environ[\u001b[33m'\u001b[0m\u001b[33mCUDA_MODULE_LOADING\u001b[0m\u001b[33m'\u001b[0m] = \u001b[33m'\u001b[0m\u001b[33mLAZY\u001b[0m\u001b[33m'\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 247 \u001b[2m│   │   \u001b[0mtorch._C._cuda_init()                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 248 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 249 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# we need to just return without initializing in that case.\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 250 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# However, we must not let any *other* threads in!\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mFound no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a \n",
       "driver from \u001b[4;94mhttp://www.nvidia.com/Download/index.aspx\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from random import randrange        \n",
    "\n",
    "# load model and tokenizer from huggingface hub with pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\", device=0)\n",
    "\n",
    "# select a random test sample\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "# summarize dialogue\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
